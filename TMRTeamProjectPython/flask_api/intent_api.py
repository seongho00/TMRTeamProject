from collections import deque
from contextlib import nullcontext

from flask import Flask, request, jsonify, Response
import json
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import pickle
import mecab_ko
import pymysql
import io, os, uuid, tempfile
import cv2, numpy as np, pytesseract, re
from PIL import Image
from werkzeug.utils import secure_filename
from pytesseract import Output
import re, time, random, hashlib
from collections import deque
from playwright.sync_api import sync_playwright, Page


# HEIC ì§€ì› (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ìë™ ë“±ë¡)
try:
    import pillow_heif
    pillow_heif.register_heif_opener()
except Exception:
    pass

# í¬ë¡¤ë§ ê´€ë ¨ import
from typing import Any, Dict, List, Optional, Tuple
from playwright.sync_api import sync_playwright, TimeoutError as PwTimeout



# ì‚¬ì§„ ì—…ë¡œë“œ ì €ì¥ ë””ë ‰í† ë¦¬ (ìŠ¤í”„ë§ê³¼ ë™ì¼/ê³µìœ  ê²½ë¡œë©´ ë” ì¢‹ìŒ)
UPLOAD_DIR = os.path.join(tempfile.gettempdir(), "registry-uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)

# tesseract ì—”ì§„ ê²½ë¡œ (Dokerë¡œ íŒ€í”Œë¡œ ì „í™˜ ì˜ˆì •)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
os.environ["TESSDATA_PREFIX"] = r"C:\Program Files\Tesseract-OCR\tessdata"

# ê³µí†µ OCR ì„¤ì •
LANG_MAIN = "kor"

# í—ˆìš© MIME (í•„ìš”ì‹œ application/pdf ì¶”ê°€)
ALLOWED = {"image/jpeg", "image/png", "image/webp", "image/heic"}


print(pytesseract.get_tesseract_version())
print(pytesseract.get_languages(config=f"-l {LANG_MAIN}"))


# âœ… ì˜ˆì¸¡ í•¨ìˆ˜
def predict_intent(text, threshold=0.1):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=32)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=1)[0]
        confidence, predicted_class_id = torch.max(probs, dim=0)

    confidence = confidence.item()
    class_idx = predicted_class_id.item()

    if confidence < threshold:
        return "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.", confidence

    predicted_label = label_encoder.inverse_transform([class_idx])[0]
    return predicted_label, confidence


def extract_location(text):
    valid_city_map = {
        'ëŒ€ì „': ['ì„œêµ¬', 'ìœ ì„±êµ¬', 'ëŒ€ë•êµ¬', 'ë™êµ¬', 'ì¤‘êµ¬']
    }
    # ì‹œë„ + ì‹œêµ°êµ¬ + ìë©´ë™ê¹Œì§€ ì¶”ì¶œ
    pattern = r"(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)[\s]*(?:íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|ë„)?[\s]*" \
              r"([ê°€-í£]+[ì‹œêµ°êµ¬])?[\s]*([ê°€-í£]+[ë™ë©´ì])?"

    match = re.search(pattern, text)
    if match:
        sido, sigungu, eupmyeondong = match.groups()
        if sigungu and sigungu not in valid_city_map.get(sido, []):
            return "âŒ í–‰ì •êµ¬ì—­ í˜•ì‹ì— ë§ì§€ ì•ŠìŒ"  # âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì¡°í•©
        parts = [sido, sigungu, eupmyeondong]
        return " ".join(p for p in parts if p)

    return None


# âœ… ì˜ë„ + ì§€ì—­ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
def generate_response(user_input):
    intent, confidence = predict_intent(user_input)
    location = extract_location(user_input)

    if intent == 0:
        if location:
            return f"âœ… '{location}'ì˜ ë§¤ì¶œ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
        else:
            return "âš ï¸ ë§¤ì¶œ ì •ë³´ë¥¼ ì¡°íšŒí•˜ë ¤ë©´ ì§€ì—­ëª…ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    elif intent == 1:
        if location:
            return f"ğŸ“Š '{location}'ì˜ ì¸êµ¬ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
        else:
            return "âš ï¸ ì¸êµ¬ ì •ë³´ë¥¼ ì¡°íšŒí•˜ë ¤ë©´ ì§€ì—­ëª…ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    elif intent == 2:
        return "ğŸš¨ íì—… ìœ„í—˜ë„ ë†’ì€ ìƒê¶Œì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."

    else:
        return "â“ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."


# âœ… ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_nouns(text):
    parsed = tagger.parse(text)
    lines = parsed.split('\n')
    nouns = []

    for line in lines:
        if line == 'EOS' or line == '':
            continue
        word, feature = line.split('\t')
        pos = feature.split(',')[0]

        if pos in ['NNG', 'NNP']:  # ì¼ë°˜ëª…ì‚¬ or ê³ ìœ ëª…ì‚¬
            nouns.append(word)

    return nouns


# âœ… ì˜ë¯¸ ë¶„ì„ í•¨ìˆ˜
def analyze_input(user_input, valid_emd_list):
    # ì‹œë„ ëª©ë¡ (ë‹¨ì¼ ë‹¨ì–´ ê¸°ì¤€)
    valid_sido = {'ëŒ€ì „'}

    # ì‹œêµ°êµ¬ ëª©ë¡
    valid_sigungu = {'ì„œêµ¬', 'ìœ ì„±êµ¬', 'ëŒ€ë•êµ¬', 'ë™êµ¬', 'ì¤‘êµ¬'}

    gender_keywords = {
        "ë‚¨ì": "male", "ë‚¨ì„±": "male",
        "ì—¬ì": "female", "ì—¬ì„±": "female"
    }

    age_keywords = {
        "10ëŒ€": "age_10",
        "20ëŒ€": "age_20",
        "30ëŒ€": "age_30",
        "40ëŒ€": "age_40",
        "50ëŒ€": "age_50",
        "60ëŒ€": "age_60"
    }

    nouns = extract_nouns_with_age_merge(user_input)
    print("ì¶”ì¶œëœ ëª…ì‚¬:", nouns)
    gender = None
    age_group = None
    sido = None
    sigungu = None
    emd_nm = None

    for token in nouns:
        # ì‹œë„ ì„¤ì •
        if token in valid_sido:
            sido = token

        # ì‹œêµ°êµ¬ ì„¤ì •
        if token in valid_sigungu:
            sigungu = token

        # ì„±ë³„ ì„¤ì •
        if token in gender_keywords:
            gender = gender_keywords[token]

        # ë‚˜ì´ëŒ€ ì„¤ì •
        if token in age_keywords:
            age_group = age_keywords[token]

        # í–‰ì •ë™ ì„¤ì •
        if token in valid_emd_list:
            emd_nm = token  # âœ… í–‰ì •ë™ ì´ë¦„ ì €ì¥

    return gender, age_group, sido, sigungu, emd_nm


# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ (DBì—ì„œ í–‰ì •ë™ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
def extract_emd_nm_list():
    conn = pymysql.connect(
        host='localhost',
        user='root',
        password='',
        db='TMRTeamProject',
        charset='utf8'
    )
    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT DISTINCT emd_nm FROM admin_dong")
            result = cursor.fetchall()
            if result:
                return [row[0] for row in result]
            else:
                return []  # âœ… ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë„ ë¦¬í„´
    except Exception as e:
        print("âŒ emd_nm ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨:", e)
        return []  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸
    finally:
        conn.close()


# ìˆ«ì í¬í•¨ ë¶„ì„
def extract_nouns_with_age_merge(text):
    parsed = tagger.parse(text)
    lines = parsed.split('\n')

    result = []
    i = 0
    while i < len(lines) - 1:  # ë§ˆì§€ë§‰ 'EOS' ì œì™¸
        line = lines[i]
        if line == '' or line == 'EOS':
            i += 1
            continue

        word, feature = line.split('\t')
        features = feature.split(',')

        # í˜„ì¬ í’ˆì‚¬
        current_tag = features[0]

        # ìˆ«ì + ëŒ€ ì¡°í•©ì´ë©´ ë³‘í•©
        if current_tag == 'SN' and i + 1 < len(lines):
            next_line = lines[i + 1]
            if '\t' not in next_line:
                i += 1
                continue
            next_word, next_feature = next_line.split('\t')
            next_tag = next_feature.split(',')[0]
            if next_tag == 'NNBC' and next_word == 'ëŒ€':
                result.append(word + next_word)  # ì˜ˆ: "20ëŒ€"
                i += 2
                continue

        # ëª…ì‚¬ë¥˜ë§Œ í•„í„°ë§ (ì§€ëª…/ì¼ë°˜ëª…ì‚¬ ë“±)
        if current_tag in ['NNG', 'NNP']:
            result.append(word)

        i += 1

    return result


# ì„œë²„ ì‹œì‘
tagger = mecab_ko.Tagger()

app = Flask(__name__)

# âœ… intent label ë³µì› (ë¼ë²¨ ì¸ì½”ë”ë¡œ)
with open("label_encoder.pickle", "rb") as f:
    label_encoder = pickle.load(f)
intent_labels = list(label_encoder.classes_)

# âœ… HuggingFace BERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_PATH = "./intent_bert_model"
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
model.eval()

# ì•± ì „ì²´ì—ì„œ ì‚¬ìš©í•  ì „ì—­ ë¦¬ìŠ¤íŠ¸
valid_emd_list = extract_emd_nm_list()


# âœ… API ë¼ìš°íŒ…
@app.route("/predict", methods=["GET"])
def predict():
    question = request.args.get("text", "").strip()

    if not question:
        return jsonify({"error": "text íŒŒë¼ë¯¸í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400

    # ì˜ˆì¸¡
    intent, confidence = predict_intent(question)

    # ê¸°ë³¸ ì‘ë‹µ
    response_data = {
        "intent": str(intent),
        "confidence": float(round(confidence, 4)),
        "message": generate_response(question)
    }

    # intentê°€ 1(ìœ ë™ì¸êµ¬ ì¡°íšŒ)ì¼ ë•Œë§Œ ì˜ë¯¸ ë¶„ì„ ì‹¤í–‰
    if intent == 1:
        gender, age_group, sido, sigungu, emd_nm = analyze_input(question, valid_emd_list)
        response_data.update({
            "sido": str(sido),
            "sigungu": str(sigungu),
            "emd_nm": str(emd_nm),
            "gender": str(gender),
            "age_group": str(age_group)
        })

    return Response(
        json.dumps(response_data, ensure_ascii=False),
        content_type="application/json; charset=utf-8"
    )


# ì‚¬ì§„ ì „ì²˜ë¦¬ ë° ë¶„ì„ ì½”ë“œ
def _sha1(path, limit=1024 * 128):
    """íŒŒì¼ ì•ë¶€ë¶„ë§Œ ì½ì–´ ë¹ ë¥¸ ì²´í¬ì„¬(ì„ íƒ)"""
    h = hashlib.sha1()
    with open(path, "rb") as f:
        chunk = f.read(limit)
        h.update(chunk)
    return h.hexdigest()


def preprocess_for_ocr(img_bgr: np.ndarray) -> np.ndarray:
    """ê¸°ë³¸ ì „ì²˜ë¦¬: ë¦¬ì‚¬ì´ì¦ˆ â†’ ê·¸ë ˆì´ â†’ ì¡ìŒì œê±° â†’ ëŒ€ë¹„ë³´ì • â†’ ì´ì§„í™”"""
    h, w = img_bgr.shape[:2]
    scale = 1400 / max(h, w)
    if scale < 1:
        img_bgr = cv2.resize(img_bgr, (int(w * scale), int(h * scale)))
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    gray = cv2.bilateralFilter(gray, 9, 75, 75)  # ì¡ìŒ ì¤„ì´ë©´ì„œ ì—£ì§€ ì‚´ë¦¬ê¸°
    gray = cv2.equalizeHist(gray)  # ëŒ€ë¹„ í–¥ìƒ
    bw = cv2.adaptiveThreshold(gray, 255,
                               cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                               cv2.THRESH_BINARY, 31, 15)
    return bw


def _cleanup_text(text: str) -> str:
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


# ì„¹ì…˜ ë‚˜ëˆ„ê¸°(í‘œì œë¶€/ê°‘êµ¬/ì„êµ¬ í‚¤ì›Œë“œ ê¸°ë°˜ì˜ ë§¤ìš° ë‹¨ìˆœí•œ ë¶„ë¦¬)
SEC_RE = {
    "í‘œì œë¶€": re.compile(r"(í‘œì œë¶€.*?)(?=ê°‘êµ¬|ì„êµ¬|$)", re.S),
    "ê°‘êµ¬": re.compile(r"(ê°‘êµ¬.*?)(?=í‘œì œë¶€|ì„êµ¬|$)", re.S),
    "ì„êµ¬": re.compile(r"(ì„êµ¬.*?)(?=í‘œì œë¶€|ê°‘êµ¬|$)", re.S),
}


def split_sections(text: str) -> dict:
    secs = {}
    for k, rx in SEC_RE.items():
        m = rx.search(text)
        secs[k] = m.group(1) if m else ""
    return secs


# ì„êµ¬ì—ì„œ ê¶Œë¦¬/ê¸ˆì•¡/ë‚ ì§œ ì•„ì£¼ ê¸°ì´ˆ ì¶”ì¶œ(ì •ê·œì‹ ê¸°ë°˜ â€“ ì„œì‹ ì°¨ì´ì— ì•½í•¨)
DATE_RE = r"(\d{4}[./-]\d{1,2}[./-]\d{1,2})"
MONEY_RE = r"([0-9,]+)\s*ì›"


def parse_eulgu(eul_text: str) -> list:
    """
    ì˜ˆ: ìˆœìœ„ë²ˆí˜¸ 1 ê·¼ì €ë‹¹ê¶Œ ... ì ‘ìˆ˜ì¼ 2023-01-01 ì±„ê¶Œìµœê³ ì•¡ 100,000,000ì›
    ì„œì‹ì´ ì œê°ê°ì´ë¼ 100% ì •í™•í•˜ì§„ ì•Šì§€ë§Œ, ê¸°ë³¸ì ì¸ íŒ¨í„´ë§Œ ë½‘ìŒ
    """
    entries = []
    pat = re.compile(
        rf"ìˆœìœ„ë²ˆí˜¸\s*(\d+).*?"
        rf"(ê·¼ì €ë‹¹ê¶Œ|ì €ë‹¹ê¶Œ|ì „ì„¸ê¶Œ|ì„ì°¨ê¶Œ|ê°€ë“±ê¸°|ì‹ íƒë“±ê¸°).*?"
        rf"(ì ‘ìˆ˜|ì ‘ìˆ˜ì¼|ì›ì¸ì¼ì).*?{DATE_RE}.*?"
        rf"(ì±„ê¶Œìµœê³ ì•¡|ì „ì„¸ê¸ˆ|ì±„ê¶Œì•¡)?\s*{MONEY_RE}?",
        re.S
    )
    for m in pat.finditer(eul_text):
        entries.append({
            "ìˆœìœ„": int(m.group(1)),
            "ê¶Œë¦¬": m.group(2),
            "ì ‘ìˆ˜ì¼": re.sub(r"[./]", "-", m.group(4)),
            "ê¸ˆì•¡ëª…": m.group(5) or "",
            "ê¸ˆì•¡": int((m.group(6) or "0").replace(",", "")) if m.group(6) else None
        })
    return entries


# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
def build_conf(psm=6, lang=LANG_MAIN):
    # ë¬¸ì„œí˜• ë ˆì´ì•„ì›ƒ ê¸°ë³¸ê°’
    return f'-l {lang} --oem 3 --psm {psm} --dpi 300 -c preserve_interword_spaces=1'

# --------- (1) ìë™ ë°©í–¥ ë³´ì • ---------
def auto_orient(pil: Image.Image) -> Image.Image:
    try:
        osd = pytesseract.image_to_osd(pil, config=f'--oem 3 --psm 0 -l {LANG_MAIN}')
        m = re.search(r"Rotate:\s+(\d+)", osd)
        deg = int(m.group(1)) if m else 0
        if deg in (90, 180, 270):
            pil = pil.rotate(-deg, expand=True)
    except Exception:
        pass
    return pil

# --------- (2) ë°ìŠ¤í + CLAHE(ë¶€ë“œëŸ¬ìš´ ê·¸ë ˆì´ í›„ë³´) ---------
def deskew(gray: np.ndarray) -> np.ndarray:
    bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]
    coords = np.column_stack(np.where(bw > 0))
    if coords.size == 0:
        return gray
    angle = cv2.minAreaRect(coords)[-1]
    angle = -(90 + angle) if angle < -45 else -angle
    h, w = gray.shape[:2]
    M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)
    return cv2.warpAffine(gray, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)

def enhance_gray(pil: Image.Image, target_long=2000) -> np.ndarray:
    rgb = np.array(pil.convert("RGB"))
    bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
    h, w = bgr.shape[:2]
    scale = target_long / max(h, w)
    if scale > 1.0:  # ì €í•´ìƒë„ë§Œ ì—…ìŠ¤ì¼€ì¼
        bgr = cv2.resize(bgr, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_CUBIC)
    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
    gray = clahe.apply(gray)
    gray = deskew(gray)
    return gray

# --------- (3) ë„ˆê°€ ê°€ì§„ í¬ë¡­/ê°•ì´ì§„ ê·¸ëŒ€ë¡œ ì‚¬ìš© ---------
def crop_document(bgr: np.ndarray) -> np.ndarray:
    h, w = bgr.shape[:2]
    y1, y2 = int(0.05*h), int(0.95*h)
    x1, x2 = int(0.06*w), int(0.94*w)
    bgr = bgr[y1:y2, x1:x2].copy()
    try:
        gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 60, 180)
        cnts, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if cnts:
            c = max(cnts, key=cv2.contourArea)
            peri = cv2.arcLength(c, True)
            approx = cv2.approxPolyDP(c, 0.02*peri, True)
            if len(approx) == 4:
                pts = approx.reshape(4,2).astype(np.float32)
                s = pts.sum(axis=1); diff = np.diff(pts, axis=1)
                rect = np.array([pts[np.argmin(s)], pts[np.argmin(diff)],
                                 pts[np.argmax(s)], pts[np.argmax(diff)]], dtype=np.float32)
                (tl,tr,br,bl) = rect
                W = int(max(np.linalg.norm(br-bl), np.linalg.norm(tr-tl)))
                H = int(max(np.linalg.norm(tr-br), np.linalg.norm(tl-bl)))
                M = cv2.getPerspectiveTransform(rect, np.array([[0,0],[W,0],[W,H],[0,H]], np.float32))
                bgr = cv2.warpPerspective(bgr, M, (W,H))
    except Exception:
        pass
    return bgr

def preprocess_doc(bgr: np.ndarray, upscale_to=2000) -> np.ndarray:
    h, w = bgr.shape[:2]
    scale = max(1.0, upscale_to / max(h, w))
    bgr = cv2.resize(bgr, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_CUBIC)
    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)
    bg = cv2.medianBlur(gray, 31)
    norm = cv2.divide(gray, bg, scale=255)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    norm = clahe.apply(norm)
    _, bw = cv2.threshold(norm, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    bw = cv2.medianBlur(bw, 3)
    bw = cv2.morphologyEx(bw, cv2.MORPH_CLOSE, np.ones((2,2), np.uint8), iterations=1)
    return bw

# --------- (4) Tesseract ì‹¤í–‰ + ì ìˆ˜í™” ---------
def _run_tess(pil_img: Image.Image, psm: int):
    conf = build_conf(psm=psm)
    data = pytesseract.image_to_data(pil_img, config=conf, output_type=Output.DICT)
    words = [w for w in data["text"] if w.strip()]
    confs = [int(c) for c in data["conf"] if c not in ("-1", "-0.0")]
    avg = sum(confs)/len(confs) if confs else 0
    return " ".join(words), avg

def run_ocr(gray: np.ndarray, extra_candidates=None) -> str:
    cands = [gray]
    # ì•½ì´ì§„ í›„ë³´ ì¶”ê°€
    thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                cv2.THRESH_BINARY, 31, 15)
    cands.append(thr)
    # ê°•ì´ì§„(ìˆìœ¼ë©´)ë„ í›„ë³´ë¡œ
    if extra_candidates:
        cands.extend(extra_candidates)

    best_txt, best_score = "", -1.0
    for img in cands:
        pil = Image.fromarray(img)
        for psm in (6, 4, 11, 3):  # ë¬¸ì„œ(6)â†’ì»¬ëŸ¼(4)â†’ìŠ¤íŒŒìŠ¤(11)â†’ìë™(3)
            try:
                txt, avg = _run_tess(pil, psm)
            except Exception:
                continue
            # í•œê¸€/ì „ì²´ ê¸¸ì´ë¡œ ê°€ì¤‘ì¹˜ ì£¼ê¸°
            score = (avg or 0) + 0.2 * len(re.findall(r"[ê°€-í£]", txt))
            if score > best_score:
                best_score, best_txt = score, txt
    return _cleanup_text(best_txt)

def ocr_image_bytes(data: bytes) -> str:
    # 1) ë¡œë“œ + ìë™ íšŒì „
    pil = Image.open(io.BytesIO(data)); pil.load()
    pil = auto_orient(pil)

    # 2) ë¬¸ì„œ í¬ë¡­(ê°€ëŠ¥í•˜ë©´)
    try:
        bgr = cv2.cvtColor(np.array(pil.convert("RGB")), cv2.COLOR_RGB2BGR)
        bgr = crop_document(bgr)
        pil = Image.fromarray(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))
    except Exception:
        pass

    # 3) ë¶€ë“œëŸ¬ìš´ ê·¸ë ˆì´ í›„ë³´
    gray = enhance_gray(pil, target_long=2000)

    # 4) ê°•ì´ì§„ í›„ë³´ (preprocess_doc)
    extra = []
    try:
        bgr_for_bw = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)
        bw = preprocess_doc(bgr_for_bw)
        extra.append(bw)
    except Exception:
        pass

    # 5) ì—¬ëŸ¬ PSM/í›„ë³´ ì¡°í•© â†’ best pick
    return run_ocr(gray, extra_candidates=extra)



@app.route("/analyze", methods=["POST"])
def analyze():
    if "files" not in request.files:
        return jsonify(ok=False, message="files í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤."), 400

    files = request.files.getlist("files")
    if not files:
        return jsonify(ok=False, message="ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 400

    results_meta = []
    texts = []
    for f in files:
        if not f or f.filename == "":
            continue

        ctype = (f.mimetype or "").lower()
        if ctype not in ALLOWED:
            return jsonify(ok=False, message=f"í—ˆìš©ë˜ì§€ ì•Šì€ í˜•ì‹: {ctype}"), 415

        # ë©”íƒ€ë°ì´í„°ë§Œ ë³´ì¡´(ì €ì¥ì€ ì•ˆ í•¨)
        base, ext = os.path.splitext(secure_filename(f.filename))
        ext = ext or ".bin"
        fname = f"{uuid.uuid4()}{ext}"


        # ë°”ì´íŠ¸ë¥¼ ë©”ëª¨ë¦¬ë¡œ ì½ìŒ
        data = f.read()
        if not data:
            continue

        try:
            txt = ocr_image_bytes(data)
        except Exception as e:
            txt = f"[OCR ì‹¤íŒ¨: {e}]"

        results_meta.append({
            "originalName": f.filename,
            "contentType": ctype,
            "fileName": fname,  # ì‹¤ì œ ì €ì¥ì€ ì•ˆ í•˜ì§€ë§Œ ì¶”ì ìš©ìœ¼ë¡œ ì‘ë‹µ
            "size": len(data),
        })
        texts.append(txt)

    if not results_meta:
        return jsonify(ok=False, message="ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 400

    full_text = "\n\n---PAGEBREAK---\n\n".join(texts)
    secs = split_sections(full_text)
    # eul_entries = parse_eulgu(secs.get("ì„êµ¬", ""))

    return jsonify(
        ok=True,
        count=len(results_meta),
        files=results_meta,  # storedPath ì œê±°ë¨ (ë””ìŠ¤í¬ ì €ì¥ ì•ˆ í•¨)
        sections_detected=[k for k, v in secs.items() if v],
        textPreview=full_text[:2000],
        textFull=full_text,  # â¬…ï¸ ì „ì²´ OCR í…ìŠ¤íŠ¸ ì¶”ê°€
        # parsed={"ì„êµ¬_entries": eul_entries}
    ), 200


# ì¢Œí‘œë¥¼ í†µí•œ ë„¤ì´ë²„ ë¶€ë™ì‚° ì‹¤ì‹œê°„ í¬ë¡¤ë§
# --- Playwright ì„¤ì • ---
USER_DATA_DIR = "./.chrome-profile"   # ì¿ í‚¤/ì§€ë¬¸ ìœ ì§€
HEADLESS = False                      # ë¨¼ì € ì°½ ë„ì›Œì„œ í™•ì¸ í›„ Trueë¡œ ì „í™˜ ê°€ëŠ¥
LIST_SEL_CANDS = [
    "div.item_list.item_list--article",
    "div.item_list--article",
    "div.article_list",  # ì˜ˆë¹„
]
API_SOFT_WAIT_MS = 1200          # 800~1500 ê¶Œì¥ (ë„ˆë¬´ ëŠë¦¬ë©´ 900ë¡œ)
MAX_CLICK = 20                   # í´ë¦­í•  ì¹´ë“œ ìˆ˜ ìƒí•œ

def _pick_scroll_box(page) -> str:
    # scrollHeight > clientHeight ì¸ ì²« ìš”ì†Œ ì„ íƒ
    page.wait_for_selector(", ".join(LIST_SEL_CANDS), timeout=8000)
    for sel in LIST_SEL_CANDS:
        try:
            ok = page.evaluate("""(s) => {
                const el = document.querySelector(s);
                if (!el) return false;
                return el.scrollHeight > el.clientHeight;
            }""", sel)
            if ok:
                return sel
        except:
            pass
    return LIST_SEL_CANDS[0]

def scroll_list_to_bottom(page, pause_ms=700, max_stall=2):
    sel = _pick_scroll_box(page)
    stall = 0
    last_h = -1

    # ì»¨í…Œì´ë„ˆì— í¬ì»¤ìŠ¤(í‚¤ë³´ë“œ ìŠ¤í¬ë¡¤ ë°±ì—…ìš©)
    try:
        page.locator(sel).click(position={"x":10,"y":10}, timeout=1000)
    except:
        pass

    for _ in range(60):  # ì¶©ë¶„í•œ íšŸìˆ˜ ì‹œë„
        try:
            h = page.evaluate("""(s) => {
                const el = document.querySelector(s);
                if (!el) return 0;
                el.scrollTop = el.scrollHeight;
                return el.scrollHeight;
            }""", sel)
        except:
            # ë¦¬ë Œë” ëì„ ë•Œ ë‹¤ì‹œ ê³ ë¥´ê¸°
            sel = _pick_scroll_box(page)
            h = page.evaluate("(s) => document.querySelector(s)?.scrollHeight || 0", sel)

        page.wait_for_timeout(pause_ms)

        if h == last_h:
            stall += 1
            if stall >= max_stall:
                break
        else:
            stall = 0
            last_h = h

    # ìŠ¤í¬ë¡¤ í›„ ì¹´ë“œ ìˆ˜ì§‘
    return page.query_selector_all("div.item, div.item_list--article div.item")

def get_env_proxy():
    for k in ("HTTPS_PROXY","https_proxy","HTTP_PROXY","http_proxy"):
        v = os.environ.get(k)
        if v: return {"server": v}
    return None

def _launch_ctx(p):
    proxy_cfg = get_env_proxy()
    ctx = p.chromium.launch_persistent_context(
        user_data_dir=USER_DATA_DIR,
        channel="chrome",
        headless=HEADLESS,
        locale="ko-KR",
        viewport={"width": 1280, "height": 860},
        proxy=proxy_cfg,
        ignore_https_errors=True,
        args=[
            "--disable-blink-features=AutomationControlled",
            "--no-first-run","--no-default-browser-check",
            "--disable-dev-shm-use","--no-sandbox",
        ],
    )
    # ì•½ì‹ ìŠ¤í…”ìŠ¤
    ctx.add_init_script("""
      Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
      Object.defineProperty(navigator, 'language', {get: () => 'ko-KR'});
      Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR','ko','en-US','en']});
      window.chrome = window.chrome || { runtime: {} };
    """)
    ctx.set_extra_http_headers({"Accept-Language":"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"})
    return ctx

def _goto_offices(page, lat: float, lng: float):
    root = "https://new.land.naver.com"
    path = f"/offices?ms={lat},{lng},17&a=SG&e=RETAIL"
    page.goto(root, wait_until="domcontentloaded", timeout=30000)
    try:
        page.evaluate("href => { location.href = href }", path)
        page.wait_for_url("**/offices**", timeout=20000)
        return
    except PwTimeout:
        pass
    page.goto(root + path, referer=root, wait_until="domcontentloaded", timeout=30000)
    page.wait_for_url("**/offices**", timeout=20000)


DETAIL_PATTERNS = [
    re.compile(r"https://new\.land\.naver\.com/api/.*/articles/\d+"),
    re.compile(r"https://new\.land\.naver\.com/api/.*/article/\d+"),
    re.compile(r"https://new\.land\.naver\.com/api/.*/detail"),
]

def _soft_wait_detail(page: Page, q: deque, prev_len: int, wait_ms: int) -> Optional[Dict[str, Any]]:
    deadline = time.time() + wait_ms / 1000.0
    while time.time() < deadline:
        if len(q) > prev_len:
            return q[-1]  # ê°€ì¥ ìµœê·¼ ë„ì°©ë¶„
        page.wait_for_timeout(40)  # ì§§ê²Œ ì–‘ë³´
    return None

def _is_api(url: str) -> bool:
    return "https://new.land.naver.com/api/" in url

def _is_detail(url: str) -> bool:
    return _is_api(url) and any(p.search(url) for p in DETAIL_PATTERNS)

def crawl_viewport(lat: float, lng: float,
                   radius_m: int = 800,
                   category: str = "offices",
                   filters: Optional[Dict[str, Any]] = None,
                   limit_detail_fetch: Optional[int] = 60) -> Dict[str, Any]:

    with sync_playwright() as p:
        ctx = _launch_ctx(p)
        page = ctx.new_page()

        detail_hits: List[Dict[str, Any]] = []

        # ë””í…Œì¼ ì‘ë‹µ í (on_respê°€ ì—¬ê¸°ë¡œ push)
        api_detail_queue: deque = deque()

        def on_resp(resp):
            url = resp.url
            if _is_detail(url):
                try:
                    data = resp.json()
                except Exception:
                    data = None
                api_detail_queue.append({"url": url, "status": resp.status, "data": data})

        page.on("response", on_resp)

        # 1) /offices ì§„ì… & ëª©ë¡ ë¡œë“œ
        _goto_offices(page, lat, lng)
        page.wait_for_timeout(800)
        scroll_list_to_bottom(page, pause_ms=700, max_stall=2)

        # 2) ì¹´ë“œ ëª©ë¡ í™•ë³´
        page.wait_for_selector(
            "div.item_list--article div.item, div.item_list.item_list--article div.item",
            timeout=6000
        )
        cards = page.query_selector_all(
            "div.item_list--article div.item, div.item_list.item_list--article div.item"
        )

        # 3) í´ë¦­ â†’ (ì§§ê²Œ) API ì†Œí”„íŠ¸ ëŒ€ê¸° â†’ DOM íŒŒì‹±(í•­ìƒ)
        for c in cards[len(cards)]:
            if limit_detail_fetch and len(detail_hits) >= int(limit_detail_fetch):
                break

            try:
                before_len = len(api_detail_queue)
                c.scroll_into_view_if_needed()
                c.click()
            except Exception:
                continue

            # APIëŠ” ìµœëŒ€ 1.2ì´ˆë§Œ ëŒ€ê¸° (ì—†ì–´ë„ í†µê³¼)
            hit = _soft_wait_detail(page, api_detail_queue, before_len, API_SOFT_WAIT_MS)

            # ë””í…Œì¼ íŒ¨ë„ DOM íŒŒì‹± (í•­ìƒ ì‹œë„)
            page.wait_for_timeout(200)  # íŒ¨ë„ ë Œë” ì—¬ìœ 
            try:
                dom_data = scrape_detail_panel(page)   # ë˜ëŠ” scrape_detail_panel_raw(page)
            except Exception:
                dom_data = {}

            detail_hits.append({
                "url":    (hit["url"] if hit else None),
                "status": (hit["status"] if hit else None),
                "json":   (hit["data"] if hit else None),
                "dom":    dom_data,
            })

            # ë‚´ë¶€ ì¶”ê°€ XHR ì‹œê°„ì„ ì•„ì£¼ ì‚´ì§ ë¶€ì—¬
            page.wait_for_timeout(120 + int(random.random() * 120))

        # ì •ë¦¬
        page.off("response", on_resp)
        ctx.close()

        limit = int(limit_detail_fetch or len(detail_hits))
        return {
            "ok": True,
            "meta": {
                "lat": lat, "lng": lng,
                "category": category,
                "limit_detail_fetch": limit,
                "count_detail": len(detail_hits),
                "source": "playwright_softwait(dom+api)",
            },
            "items": detail_hits[:limit],   # ê° item = {"json":..., "dom":...}
        }

from playwright.sync_api import Page, Locator, TimeoutError as PWTimeout
import re

def wait_for_element(page: Page, selector: str, timeout: int = 6000) -> Locator:
    loc = page.locator(selector)
    loc.wait_for(state="visible", timeout=timeout)
    return loc

def wait_for_elements(page: Page, selector: str, min_count: int = 1, timeout: int = 6000):
    page.wait_for_selector(selector, state="attached", timeout=timeout)
    loc = page.locator(selector)
    # ê°œìˆ˜ ë§Œì¡±ê¹Œì§€ í´ë§
    with page.expect_event("load", timeout=200) if False else nullcontext():  # no-op
        end = page.context._impl_obj._loop.time() + (timeout / 1000)
        while True:
            if loc.count() >= min_count:
                break
            if page.context._impl_obj._loop.time() > end:
                raise PWTimeout(f"Timeout waiting for {min_count}+ elements: {selector}")
    return [loc.nth(i) for i in range(loc.count())]

def wait_for_child_element(parent: Locator, selector: str, timeout: int = 6000) -> Locator:
    child = parent.locator(selector)
    child.wait_for(state="visible", timeout=timeout)
    return child

def text_or_none(loc: Locator, timeout: int = 3000) -> Optional[str]:
    try:
        return loc.inner_text(timeout=timeout).strip()
    except Exception:
        return None

def get_by_header(page: Page, header_text: str) -> Optional[str]:
    """
    <tr class="info_table_item">
      <th>ê´€ë¦¬ë¹„</th><td>â€¦</td>
    </tr>
    ê°™ì€ êµ¬ì¡°ì—ì„œ í—¤ë” í…ìŠ¤íŠ¸ë¡œ ê°’ì„ ë½‘ì•„ì˜´
    """
    row = page.locator(
        "tr.info_table_item",
        has=page.locator("th", has_text=header_text)
    ).first

    if row.count() == 0:
        return None

    return text_or_none(row.locator("td").first)  # text_or_none: Optional[str] ë°˜í™˜

def parse_deposit_monthly(price_value: Optional[str]) -> Tuple[Optional[int], Optional[int]]:
    if not price_value or "/" not in price_value:
        return None, None
    a, b = price_value.split("/", 1)
    clean = lambda s: int("".join(ch for ch in s if ch.isdigit())) if any(ch.isdigit() for ch in s) else None
    return clean(a), clean(b)

def scrape_detail_panel(page: Page) -> dict:
    detail = {}

    # ê°€ê²© ë°•ìŠ¤
    price_box = wait_for_element(page, "div.info_article_price")
    detail["price_type"]  = text_or_none(price_box.locator(".type"))   # ì˜ˆ: "ì›”ì„¸"
    detail["price_value"] = text_or_none(price_box.locator(".price"))  # ì˜ˆ: "4,000/230"

    dep, mon = parse_deposit_monthly(detail["price_value"])
    detail["deposit"] = dep
    detail["monthly"] = mon

    # ìƒì„¸ í…Œì´ë¸”(í—¤ë” ê¸°ë°˜ ì¶”ì¶œ)
    # í˜ì´ì§€ë§ˆë‹¤ ë¼ë²¨ì´ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆì–´, í•„ìš”í•œ ê²ƒë§Œ ê³¨ë¼ í˜¸ì¶œí•˜ë©´ ë¨
    candidates = {
        "ê´€ë¦¬ë¹„": ["ê´€ë¦¬ë¹„"],
        "ì „ìš©ë©´ì ": ["ì „ìš©ë©´ì ", "ì „ìš©"],
        "ê³µê¸‰ë©´ì ": ["ê³µê¸‰ë©´ì ", "ê³µê¸‰"],
        "ì¸µ": ["ì¸µ", "ì¸µìˆ˜"],
        "ì…ì£¼ê°€ëŠ¥ì¼": ["ì…ì£¼ê°€ëŠ¥ì¼", "ì…ì£¼ì¼"],
        "ì£¼ì°¨": ["ì£¼ì°¨"],
        "ë‚œë°©": ["ë‚œë°©"],
        "ìš©ë„": ["ìš©ë„", "ì£¼ìš©ë„"],
        "ì‚¬ìš©ìŠ¹ì¸ì¼": ["ì‚¬ìš©ìŠ¹ì¸ì¼", "ì¤€ê³µì¼", "ì‚¬ìš©ìŠ¹ì¸"],
        "í™”ì¥ì‹¤ ìˆ˜": ["í™”ì¥ì‹¤ ìˆ˜"],
    }


    for key, labels in candidates.items():
        val = None
        for lbl in labels:
            val = get_by_header(page, lbl)
            if val: break
        detail[key] = val

    # ì¤‘ê°œì—…ì†Œ(ìˆì„ ê²½ìš°)
    broker_box = page.locator("div.broker_info, div.article_broker_info").first
    if broker_box.count() > 0:
        detail["broker_name"]    = text_or_none(broker_box.locator(".name, .broker_name"))
        detail["broker_ceo"]     = text_or_none(broker_box.locator(".ceo, .broker_ceo"))
        detail["broker_address"] = text_or_none(broker_box.locator(".addr, .broker_address"))
        detail["broker_phone"]   = text_or_none(broker_box.locator(".tel, .broker_phone"))

    print(detail)


    return detail

# --- Flask ë¼ìš°íŠ¸ ---
@app.route("/crawl", methods=["POST"])
def api_crawl():
    payload = request.get_json(silent=True) or {}
    lat = float(payload.get("lat"))
    lng = float(payload.get("lng"))
    radius_m = payload.get("radius_m") or 800
    category  = payload.get("category") or "offices"
    limit_detail_fetch = int(payload.get("limit_detail_fetch") or 60)

    print(f"[CRAWL IN] lat={lat}, lng={lng}, r={radius_m}, cat={category}, limit={limit_detail_fetch}")
    try:
        # â˜… í‚¤ì›Œë“œ ì¸ìë¡œ í˜¸ì¶œ (í¬ì§€ì…”ë„ë¡œ ë„˜ê¸°ë©´ filters ìë¦¬ì— ë°•í˜€ì„œ ì—‰ë§ë¨)
        res = crawl_viewport(
            lat, lng,
            radius_m=radius_m,
            category=category,
            limit_detail_fetch=limit_detail_fetch
        )
        print(f"[CRAWL OUT] count={res['meta']['count']}")
        return jsonify(res), 200
    except Exception as e:
        print(f"[CRAWL ERR] {e}")
        return jsonify(ok=False, error="crawl_failed", message=str(e)), 500

if __name__ == "__main__":
    # ë¦¬ë¡œë”/ë©€í‹°ìŠ¤ë ˆë“œ ì¶©ëŒ ë°©ì§€(ë¨¼ì € ì´ë ‡ê²Œ í™•ì¸ í›„ ì ì§„ í™•ì¥)
    app.run(host="0.0.0.0", port=5000, debug=False, use_reloader=False, threaded=False)