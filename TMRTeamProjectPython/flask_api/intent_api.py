from flask import Flask, request, jsonify, Response
import json
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import pickle
import re
import mecab_ko
import pymysql
import pymysql.cursors


# âœ… ì˜ˆì¸¡ í•¨ìˆ˜
def predict_intent(text, threshold=0.1):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=32)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=1)[0]
        confidence, predicted_class_id = torch.max(probs, dim=0)

    confidence = confidence.item()
    class_idx = int(predicted_class_id.item())

    if confidence < threshold:
        return "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ìž…ë‹ˆë‹¤.", confidence

    return class_idx, confidence


def extract_location(text):
    valid_city_map = {
        'ëŒ€ì „': ['ì„œêµ¬', 'ìœ ì„±êµ¬', 'ëŒ€ë•êµ¬', 'ë™êµ¬', 'ì¤‘êµ¬']
    }
    # ì‹œë„ + ì‹œêµ°êµ¬ + ìë©´ë™ê¹Œì§€ ì¶”ì¶œ
    pattern = r"(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)[\s]*(?:íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|ë„)?[\s]*" \
              r"([ê°€-íž£]+[ì‹œêµ°êµ¬])?[\s]*([ê°€-íž£]+[ë™ë©´ì])?"

    match = re.search(pattern, text)
    if match:
        sido, sigungu, eupmyeondong = match.groups()
        if sigungu and sigungu not in valid_city_map.get(sido, []):
            return "âŒ í–‰ì •êµ¬ì—­ í˜•ì‹ì— ë§žì§€ ì•ŠìŒ"  # âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì¡°í•©
        parts = [sido, sigungu, eupmyeondong]
        return " ".join(p for p in parts if p)

    return None


# âœ… ì˜ë„ + ì§€ì—­ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
def generate_response(user_input):
    intent, confidence = predict_intent(user_input)
    location = extract_location(user_input)

    if intent == 0:
        if location:
            return f"âœ… '{location}'ì˜ ë§¤ì¶œ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
        else:
            return "âš ï¸ ë§¤ì¶œ ì •ë³´ë¥¼ ì¡°íšŒí•˜ë ¤ë©´ ì§€ì—­ëª…ì„ ìž…ë ¥í•´ ì£¼ì„¸ìš”."

    elif intent == 1:
        if location:
            return f"ðŸ“Š '{location}'ì˜ ì¸êµ¬ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
        else:
            return "âš ï¸ ì¸êµ¬ ì •ë³´ë¥¼ ì¡°íšŒí•˜ë ¤ë©´ ì§€ì—­ëª…ì„ ìž…ë ¥í•´ ì£¼ì„¸ìš”."

    elif intent == 2:
        return "ðŸš¨ íì—… ìœ„í—˜ë„ ë†’ì€ ìƒê¶Œì„ ë¶„ì„ ì¤‘ìž…ë‹ˆë‹¤."

    else:
        return "â“ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ìž…ë‹ˆë‹¤."


# âœ… ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_nouns(text):
    parsed = tagger.parse(text)
    lines = parsed.split('\n')
    nouns = []

    for line in lines:
        if line == 'EOS' or line == '':
            continue
        word, feature = line.split('\t')
        pos = feature.split(',')[0]

        if pos in ['NNG', 'NNP']:  # ì¼ë°˜ëª…ì‚¬ or ê³ ìœ ëª…ì‚¬
            nouns.append(word)

    return nouns


# ì•± ì‹œìž‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ (DBì—ì„œ í–‰ì •ë™ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
def extract_emd_nm_list():
    conn = pymysql.connect(
        host='localhost',
        user='root',
        password='',
        db='TMRTeamProject',
        charset='utf8'
    )
    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT DISTINCT emd_nm FROM admin_dong")
            result = cursor.fetchall()
            if result:
                return [row[0] for row in result]
            else:
                return []  # âœ… ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë„ ë¦¬í„´
    except Exception as e:
        print("âŒ emd_nm ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨:", e)
        return []  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸
    finally:
        conn.close()


# ì•± ì‹œìž‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ (DBì—ì„œ ì—…ì¢… ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
def extract_upjong_code_map():
    conn = pymysql.connect(
        host='localhost',
        user='root',
        password='',
        db='TMRTeamProject',
        charset='utf8mb4',
        cursorclass=pymysql.cursors.DictCursor
    )
    try:
        with conn.cursor() as cursor:
            sql = """
                SELECT DISTINCT upjong_cd, upjong_nm
                FROM upjong_code
                WHERE upjong_cd IS NOT NULL AND upjong_nm IS NOT NULL
                ORDER BY upjong_nm
            """
            cursor.execute(sql)
            rows = cursor.fetchall()
            name_to_code = {r['upjong_nm']: r['upjong_cd'] for r in rows}
            return name_to_code
    except Exception as e:
        print("âŒ ì—…ì¢… ì½”ë“œ ë¡œë”© ì‹¤íŒ¨:", e)
        return {}
    finally:
        conn.close()


# âœ… ì˜ë¯¸ ë¶„ì„ í•¨ìˆ˜
def analyze_input(user_input, intent, valid_emd_list):
    tokens = extract_nouns_with_age_merge(user_input)
    print("ì¶”ì¶œëœ ëª…ì‚¬:", tokens)

    entities = {
        "sido": None,
        "sigungu": None,
        "emd_nm": None,
        "gender": None,
        "age_group": None,
        "upjong_cd": None,
        "raw_upjong": None,  # ë§¤ì¹­ ì•ˆ ë˜ë©´ ì›ë¬¸ ë³´ê´€
    }

    for t in tokens:
        # ì‹œ/ë„
        if t in valid_sido:
            entities["sido"] = t
        # ì‹œ/êµ°/êµ¬
        if t in valid_sigungu:
            entities["sigungu"] = t
        # í–‰ì •ë™
        if t in valid_emd_list:
            entities["emd_nm"] = t
        # ì„±ë³„
        if t in gender_keywords:
            entities["gender"] = gender_keywords[t]
        # ì—°ë ¹
        if t in age_keywords:
            entities["age_group"] = age_keywords[t]
        # ì—…ì¢…
        if t in upjong_keywords:
            entities["upjong_cd"] = upjong_keywords[t]
            entities["raw_upjong"] = t
        else:
            # ë§¤ì¹­ ì•ˆ ë˜ì–´ë„ ì—…ì¢… ë‹¨ì–´ ê°™ìœ¼ë©´ ì›ë¬¸ë§Œ ê¸°ë¡(ê°„ë‹¨ ì˜ˆì‹œ)
            if t in ("ì¹´íŽ˜","íŽ¸ì˜ì ","ë¶„ì‹","íŒ¨ìŠ¤íŠ¸í‘¸ë“œ","ì˜ë¥˜"):
                entities["raw_upjong"] = t

    # ì§€ì—­ ìµœì†Œ ë‹¨ìœ„ íŒì •(í•˜ë‚˜ë¼ë„ ìžˆìœ¼ë©´ OK)
    has_region = bool(entities["emd_nm"] or entities["sigungu"] or entities["sido"])

    # ëˆ„ë½ í•­ëª© ê³„ì‚°
    required = INTENT_REQUIRED.get(intent, {"need": [], "optional": []})
    missing = []

    if "sido_or_sigungu_or_emd" in required["need"] and not has_region:
        missing.append("region")  # ì§€ì—­(ì‹œ/êµ¬/ë™ ì¤‘ í•˜ë‚˜)

    # ë‹¤ë¥¸ ì˜ë¬´ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ì¡°ê±´ ì¶”ê°€(ì˜ˆ: intentë³„ í•„ìˆ˜ ì„±ë³„ ë“±)

    return entities, missing


# ìˆ«ìž í¬í•¨ ë¶„ì„
def extract_nouns_with_age_merge(text):
    parsed = tagger.parse(text)
    lines = parsed.split('\n')

    result = []
    i = 0
    while i < len(lines) - 1:  # ë§ˆì§€ë§‰ 'EOS' ì œì™¸
        line = lines[i]
        if line == '' or line == 'EOS':
            i += 1
            continue

        word, feature = line.split('\t')
        features = feature.split(',')

        # í˜„ìž¬ í’ˆì‚¬
        current_tag = features[0]

        # ìˆ«ìž + ëŒ€ ì¡°í•©ì´ë©´ ë³‘í•©
        if current_tag == 'SN' and i + 1 < len(lines):
            next_line = lines[i + 1]
            if '\t' not in next_line:
                i += 1
                continue
            next_word, next_feature = next_line.split('\t')
            next_tag = next_feature.split(',')[0]
            if next_tag == 'NNBC' and next_word == 'ëŒ€':
                result.append(word + next_word)  # ì˜ˆ: "20ëŒ€"
                i += 2
                continue

        # ëª…ì‚¬ë¥˜ë§Œ í•„í„°ë§ (ì§€ëª…/ì¼ë°˜ëª…ì‚¬ ë“±)
        if current_tag in ['NNG', 'NNP']:
            result.append(word)

        i += 1

    return result


# ì„œë²„ ì‹œìž‘
tagger = mecab_ko.Tagger()

app = Flask(__name__)

# âœ… intent label ë³µì› (ë¼ë²¨ ì¸ì½”ë”ë¡œ)
with open("label_encoder.pickle", "rb") as f:
    label_encoder = pickle.load(f)
intent_labels = list(label_encoder.classes_)

# âœ… HuggingFace BERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_PATH = "./intent_bert_model"
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
model.eval()

# ì•± ì „ì²´ì—ì„œ ì‚¬ìš©í•  ì „ì—­ ë¦¬ìŠ¤íŠ¸
valid_sido = {'ì„œìš¸'}

# ì‹œêµ°êµ¬ ëª©ë¡
valid_sigungu = {
    'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ìš©ì‚°êµ¬', 'ì„±ë™êµ¬', 'ê´‘ì§„êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ì¤‘ëž‘êµ¬',
    'ì„±ë¶êµ¬', 'ê°•ë¶êµ¬', 'ë„ë´‰êµ¬', 'ë…¸ì›êµ¬', 'ì€í‰êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ë§ˆí¬êµ¬',
    'ì–‘ì²œêµ¬', 'ê°•ì„œêµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'ë™ìž‘êµ¬', 'ê´€ì•…êµ¬',
    'ì„œì´ˆêµ¬', 'ê°•ë‚¨êµ¬', 'ì†¡íŒŒêµ¬', 'ê°•ë™êµ¬'
}

gender_keywords = {
    "ë‚¨ìž": "male", "ë‚¨ì„±": "male",
    "ì—¬ìž": "female", "ì—¬ì„±": "female"
}

age_keywords = {
    "10ëŒ€": "age_10",
    "20ëŒ€": "age_20",
    "30ëŒ€": "age_30",
    "40ëŒ€": "age_40",
    "50ëŒ€": "age_50",
    "60ëŒ€": "age_60"
}

valid_emd_list = extract_emd_nm_list()
upjong_keywords = extract_upjong_code_map()

# ì˜ë„ë³„ ìš”êµ¬ íŒŒë¼ë¯¸í„° ì •ì˜
# intent: 0=ë§¤ì¶œ, 1=ìœ ë™ì¸êµ¬, 2=ìœ„í—˜ë„, 3=ì²­ì•½(ì˜ˆì‹œ)
INTENT_REQUIRED = {
    0: {"need": ["sido_or_sigungu_or_emd", "upjong_cd"], "optional": []},  # ë§¤ì¶œ ì¡°íšŒ: ì§€ì—­ì€ ìµœì†Œ 1ë‹¨ê³„, ì—…ì¢…ì€ ì„ íƒ
    1: {"need": ["sido_or_sigungu_or_emd"], "optional": ["gender", "age_group"]},  # ìœ ë™ì¸êµ¬: ì§€ì—­ í•„ìˆ˜, ì„±ë³„/ì—°ë ¹ ì„ íƒ
    2: {"need": ["sido_or_sigungu_or_emd"], "optional": ["upjong_cd"]},  # ìœ„í—˜ë„: ì§€ì—­ í•„ìˆ˜
    3: {"need": ["sido_or_sigungu_or_emd"], "optional": []},  # ì²­ì•½: ì§€ì—­ í•„ìˆ˜(í•„ìš”ì— ë§žê²Œ ìˆ˜ì •)
}


# âœ… API ë¼ìš°íŒ…
@app.route("/predict", methods=["GET"])
def predict():
    question = request.args.get("text", "").strip()

    if not question:
        return jsonify({"error": "text íŒŒë¼ë¯¸í„°ê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤."}), 400

    # ì˜ˆì¸¡
    intent_id, confidence = predict_intent(question)

    # 2) íŒŒë¼ë¯¸í„° ë¶„ì„
    entities, missing = analyze_input(question, intent_id, valid_emd_list)

    # 3) ëˆ„ë½ ì•ˆë‚´
    if missing:
        parts = []
        if "region" in missing: parts.append("ì§€ì—­(ì‹œ/êµ¬/ë™)")
        if "upjong_cd" in missing:
            hint = f" (ì¸ì‹: {entities['raw_upjong']})" if entities.get("raw_upjong") else " (ì˜ˆ: ì¹´íŽ˜, íŽ¸ì˜ì )"
            parts.append("ì—…ì¢…" + hint)
        return jsonify({
            "intent": intent_id,
            "confidence": round(confidence, 4),
            "need_more": missing,
            "entities": entities,
            "message": " / ".join(parts) + " ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
        }), 200

    if intent_id == 0:  # ë§¤ì¶œ
        return jsonify({
            "intent": 0,
            "confidence": round(confidence, 4),
            "entities": entities,   # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200

    elif intent_id == 1:  # ìœ ë™ì¸êµ¬
        return jsonify({
            "intent": 1,
            "confidence": round(confidence, 4),
            "entities": entities,   # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200

    elif intent_id == 2:  # ìœ„í—˜ë„
        return jsonify({
            "intent": 2,
            "confidence": round(confidence, 4),
            "entities": entities,   # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200

    else:  # intent_id == 3 ë“±
        return jsonify({
            "intent": int(intent_id),
            "confidence": round(confidence, 4),
            "entities": entities,   # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200


if __name__ == "__main__":
    app.run(port=5000, debug=True)
