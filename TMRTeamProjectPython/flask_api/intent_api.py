import io
import os
import pickle
import random
import time
from collections import deque
from contextlib import nullcontext
from collections import defaultdict
from flask import Flask, request, jsonify, Response
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import pickle
import mecab_ko
import pdfplumber
import pymysql
import pymysql.cursors
import io, os, uuid, tempfile
import re, time, random, hashlib
from collections import deque
from decimal import Decimal, InvalidOperation

import io
import pdfplumber
import fitz  # â† PyMuPDF
from playwright.sync_api import Page, Locator, TimeoutError as PWTimeout

# HEIC ì§€ì› (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ìë™ ë“±ë¡)
try:
    import pillow_heif

    pillow_heif.register_heif_opener()
except Exception:
    pass

# í¬ë¡¤ë§ ê´€ë ¨ import
from typing import Optional, List, Dict, Tuple, Any, Set
from playwright.sync_api import sync_playwright, TimeoutError as PwTimeout

# ì—…ì¢… ê²€ìƒ‰ ê´€ë ¨ import
import re
import unicodedata

try:
    from rapidfuzz import process, fuzz

    HAVE_RAPIDFUZZ = True
except Exception:
    HAVE_RAPIDFUZZ = False


# âœ… ì˜ˆì¸¡ í•¨ìˆ˜
def predict_intent(text, threshold=0.1):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=32)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=1)[0]
        confidence, predicted_class_id = torch.max(probs, dim=0)

    confidence = confidence.item()
    class_idx = int(predicted_class_id.item())

    if confidence < threshold:
        return "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.", confidence

    return class_idx, confidence


def extract_location(text):
    valid_city_map = {
        'ëŒ€ì „': ['ì„œêµ¬', 'ìœ ì„±êµ¬', 'ëŒ€ë•êµ¬', 'ë™êµ¬', 'ì¤‘êµ¬']
    }
    # ì‹œë„ + ì‹œêµ°êµ¬ + ìë©´ë™ê¹Œì§€ ì¶”ì¶œ
    pattern = r"(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)[\s]*(?:íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|ë„)?[\s]*" \
              r"([ê°€-í£]+[ì‹œêµ°êµ¬])?[\s]*([ê°€-í£]+[ë™ë©´ì])?"

    match = re.search(pattern, text)
    if match:
        sido, sigungu, eupmyeondong = match.groups()
        if sigungu and sigungu not in valid_city_map.get(sido, []):
            return "âŒ í–‰ì •êµ¬ì—­ í˜•ì‹ì— ë§ì§€ ì•ŠìŒ"  # âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì¡°í•©
        parts = [sido, sigungu, eupmyeondong]
        return " ".join(p for p in parts if p)

    return None


# âœ… ì˜ë„ + ì§€ì—­ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
def generate_response(user_input):
    intent, confidence = predict_intent(user_input)
    location = extract_location(user_input)

    if intent == 0:
        if location:
            return f"âœ… '{location}'ì˜ ë§¤ì¶œ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
        else:
            return "âš ï¸ ë§¤ì¶œ ì •ë³´ë¥¼ ì¡°íšŒí•˜ë ¤ë©´ ì§€ì—­ëª…ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    elif intent == 1:
        if location:
            return f"ğŸ“Š '{location}'ì˜ ì¸êµ¬ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
        else:
            return "âš ï¸ ì¸êµ¬ ì •ë³´ë¥¼ ì¡°íšŒí•˜ë ¤ë©´ ì§€ì—­ëª…ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    elif intent == 2:
        return "ğŸš¨ íì—… ìœ„í—˜ë„ ë†’ì€ ìƒê¶Œì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."

    else:
        return "â“ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."


# âœ… ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_nouns(text):
    parsed = tagger.parse(text)
    lines = parsed.split('\n')
    nouns = []

    for line in lines:
        if line == 'EOS' or line == '':
            continue
        word, feature = line.split('\t')
        pos = feature.split(',')[0]

        if pos in ['NNG', 'NNP']:  # ì¼ë°˜ëª…ì‚¬ or ê³ ìœ ëª…ì‚¬
            nouns.append(word)

    return nouns


# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ (DBì—ì„œ í–‰ì •ë™ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
def extract_emd_nm_list():
    conn = pymysql.connect(
        host='localhost',
        user='root',
        password='',
        db='TMRTeamProject',
        charset='utf8'
    )
    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT DISTINCT emd_nm FROM admin_dong")
            result = cursor.fetchall()
            if result:
                return [row[0] for row in result]
            else:
                return []  # âœ… ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë„ ë¦¬í„´
    except Exception as e:
        print("âŒ emd_nm ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨:", e)
        return []  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸
    finally:
        conn.close()


# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ (DBì—ì„œ ì—…ì¢… ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
def extract_upjong_code_map():
    conn = pymysql.connect(
        host='localhost',
        user='root',
        password='',
        db='TMRTeamProject',
        charset='utf8mb4',
        cursorclass=pymysql.cursors.DictCursor
    )
    try:
        with conn.cursor() as cursor:
            sql = """
                SELECT DISTINCT upjong_cd, upjong_nm
                FROM upjong_code
                WHERE upjong_cd IS NOT NULL AND upjong_nm IS NOT NULL
                ORDER BY upjong_nm
            """
            cursor.execute(sql)
            rows = cursor.fetchall()
            name_to_code = {r['upjong_nm']: r['upjong_cd'] for r in rows}

            print(name_to_code)

            return name_to_code
    except Exception as e:
        print("âŒ ì—…ì¢… ì½”ë“œ ë¡œë”© ì‹¤íŒ¨:", e)
        return {}
    finally:
        conn.close()


def _normalize(s: str) -> str:
    s = unicodedata.normalize("NFC", s)
    # í™˜ê²½ì— ë”°ë¼ ì•„ë˜ ë‘˜ ì¤‘ í•˜ë‚˜ ì‚¬ìš©
    s = re.sub(r'[^0-9A-Za-z\u3131-\u318E\uAC00-\uD7A3]+', '', s)  # ê¶Œì¥ ëŒ€ì•ˆ
    # s = re.sub(r'[\s\p{P}\p{S}]+', '', s)
    return s


def find_upjong_pre_morph_from_map(
        user_input: str,
        name_to_code: dict[str, str],  # {"ì¤‘êµ­ìŒì‹ì ":"CS1001", ...}
        fuzzy_threshold: int = 60,
        max_window_len: int = 12
):
    """
    ë°˜í™˜: (raw_phrase, mapped_name, mapped_code, score, method)
    ì‹¤íŒ¨: (None, None, None, None, None)
    """
    text_norm = _normalize(user_input)
    if not text_norm:
        return (None, None, None, None, None)

    # ì •ê·œí™” ì¸ë±ìŠ¤
    idx = {_normalize(nm): (nm, code) for nm, code in name_to_code.items()}

    # 1) ê³µë°±ë¬´ì‹œ ë¶€ë¶„ì¼ì¹˜(ê°€ì¥ ê¸´ ë§¤ì¹­ ìš°ì„ )
    best = None
    for nm_norm, (nm, cd) in idx.items():
        if nm_norm in text_norm:
            cand = (nm, nm, cd, 100, "substring")
            if not best or len(nm) > len(best[0]):
                best = cand
    if best:
        return best

    # 2) í¼ì§€ ë§¤ì¹­(ì˜µì…˜)
    if HAVE_RAPIDFUZZ:
        keys = list(idx.keys())
        BEST = (None, None, None, -1, "fuzzy")
        Lmax = min(max_window_len, max((len(k) for k in keys), default=0))
        for L in range(2, Lmax + 1):
            for i in range(0, max(0, len(text_norm) - L + 1)):
                sub = text_norm[i:i + L]
                m = process.extractOne(sub, keys, scorer=fuzz.ratio)
                if m and m[1] > BEST[3]:
                    nm, cd = idx[m[0]]
                    BEST = (sub, nm, cd, m[1], "fuzzy")
        if BEST[3] >= fuzzy_threshold:
            return BEST

    return (None, None, None, None, None)


# âœ… ì˜ë¯¸ ë¶„ì„ í•¨ìˆ˜
def analyze_input(user_input, intent, valid_emd_list):
    entities = {
        "sido": None,
        "sigungu": None,
        "emd_nm": None,
        "gender": None,
        "age_group": None,
        "upjong_cd": None,
        "raw_upjong": None,  # ë§¤ì¹­ ì•ˆ ë˜ë©´ ì›ë¬¸ ë³´ê´€
    }

    # 1) (ê°€ì¥ ë¨¼ì €) ë¬¸ìì—´ ê¸°ë°˜ í–‰ì •ë™ ì¸ì‹
    emd_index = build_emd_index(valid_emd_list)  # ì‹¤ì„œë¹„ìŠ¤ëŠ” ëª¨ë“ˆ ë¡œë“œì‹œ 1íšŒë§Œ
    emd_hits = find_emd_in_text(user_input, emd_index)
    if emd_hits:
        entities["emd_nm"] = emd_hits[0]  # ì—¬ëŸ¬ ê°œë©´ ì²« ë²ˆì§¸ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ

    # ì—…ì¢… ì„ ë§¤í•‘ (ë™ì˜ì–´ ì—†ì´)
    raw, nm, cd, score, method = find_upjong_pre_morph_from_map(
        user_input, upjong_keywords, fuzzy_threshold=80
    )
    if cd:
        entities["raw_upjong"] = raw or nm
        entities["upjong_nm"] = nm
        entities["upjong_cd"] = cd

    tokens = extract_nouns_with_age_merge(user_input)
    print("ì¶”ì¶œëœ ëª…ì‚¬:", tokens)

    for t in tokens:
        # ì‹œ/ë„
        if t in valid_sido:
            entities["sido"] = t
        # ì‹œ/êµ°/êµ¬
        if t in valid_sigungu:
            entities["sigungu"] = t
        # í–‰ì •ë™
        if t in valid_emd_list:
            entities["emd_nm"] = t
        # ì„±ë³„
        if t in gender_keywords:
            entities["gender"] = gender_keywords[t]
        # ì—°ë ¹
        if t in age_keywords:
            entities["age_group"] = age_keywords[t]
        # ì—…ì¢…
        if entities["upjong_cd"] is None:
            if t in upjong_keywords:  # ê¸°ì¡´ í‚¤ì›Œë“œ ì‚¬ì „ (ê°„ë‹¨ ë§¤í•‘ìš©)
                entities["upjong_cd"] = upjong_keywords[t]
                entities["upjong_nm"] = t
                entities["raw_upjong"] = t
            else:
                # ì¼ìƒì–´ íŒíŠ¸ë§Œ ê¸°ë¡
                if t in ("ì¹´í˜", "í¸ì˜ì ", "ë¶„ì‹", "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ", "ì˜ë¥˜", "ì¤‘êµ­ì§‘", "ì¹˜í‚¨ì§‘"):
                    entities["raw_upjong"] = t

    # ì§€ì—­ ìµœì†Œ ë‹¨ìœ„ íŒì •(í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ OK)
    has_region = bool(entities["emd_nm"] or entities["sigungu"] or entities["sido"])

    # ëˆ„ë½ í•­ëª© ê³„ì‚°
    required = INTENT_REQUIRED.get(intent, {"need": [], "optional": []})
    missing = []

    if "sido_or_sigungu_or_emd" in required["need"] and not has_region:
        missing.append("region")  # ì§€ì—­(ì‹œ/êµ¬/ë™ ì¤‘ í•˜ë‚˜)

    # ë‹¤ë¥¸ ì˜ë¬´ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ì¡°ê±´ ì¶”ê°€(ì˜ˆ: intentë³„ í•„ìˆ˜ ì„±ë³„ ë“±)
    print(entities)
    return entities, missing


# ìˆ«ì í¬í•¨ ë¶„ì„
def extract_nouns_with_age_merge(text):
    parsed = tagger.parse(text)
    lines = parsed.split('\n')

    result = []
    i = 0
    while i < len(lines) - 1:  # ë§ˆì§€ë§‰ 'EOS' ì œì™¸
        line = lines[i]
        if line == '' or line == 'EOS':
            i += 1
            continue

        word, feature = line.split('\t')
        features = feature.split(',')

        # í˜„ì¬ í’ˆì‚¬
        current_tag = features[0]

        # ìˆ«ì + ëŒ€ ì¡°í•©ì´ë©´ ë³‘í•©
        if current_tag == 'SN' and i + 1 < len(lines):
            next_line = lines[i + 1]
            if '\t' not in next_line:
                i += 1
                continue
            next_word, next_feature = next_line.split('\t')
            next_tag = next_feature.split(',')[0]
            if next_tag == 'NNBC' and next_word == 'ëŒ€':
                result.append(word + next_word)  # ì˜ˆ: "20ëŒ€"
                i += 2
                continue

        # ëª…ì‚¬ë¥˜ë§Œ í•„í„°ë§ (ì§€ëª…/ì¼ë°˜ëª…ì‚¬ ë“±)
        if current_tag in ['NNG', 'NNP']:
            result.append(word)

        i += 1

    return result


# ì„œë²„ ì‹œì‘
tagger = mecab_ko.Tagger()

app = Flask(__name__)

# âœ… intent label ë³µì› (ë¼ë²¨ ì¸ì½”ë”ë¡œ)
with open("label_encoder.pickle", "rb") as f:
    label_encoder = pickle.load(f)
intent_labels = list(label_encoder.classes_)

# âœ… HuggingFace BERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_PATH = "./intent_bert_model"
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
model.eval()

# ì•± ì „ì²´ì—ì„œ ì‚¬ìš©í•  ì „ì—­ ë¦¬ìŠ¤íŠ¸
valid_sido = {'ì„œìš¸'}

# ì‹œêµ°êµ¬ ëª©ë¡
valid_sigungu = {
    'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ìš©ì‚°êµ¬', 'ì„±ë™êµ¬', 'ê´‘ì§„êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ì¤‘ë‘êµ¬',
    'ì„±ë¶êµ¬', 'ê°•ë¶êµ¬', 'ë„ë´‰êµ¬', 'ë…¸ì›êµ¬', 'ì€í‰êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ë§ˆí¬êµ¬',
    'ì–‘ì²œêµ¬', 'ê°•ì„œêµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'ë™ì‘êµ¬', 'ê´€ì•…êµ¬',
    'ì„œì´ˆêµ¬', 'ê°•ë‚¨êµ¬', 'ì†¡íŒŒêµ¬', 'ê°•ë™êµ¬'
}

gender_keywords = {
    "ë‚¨ì": "male", "ë‚¨ì„±": "male",
    "ì—¬ì": "female", "ì—¬ì„±": "female"
}

age_keywords = {
    "10ëŒ€": "age_10",
    "20ëŒ€": "age_20",
    "30ëŒ€": "age_30",
    "40ëŒ€": "age_40",
    "50ëŒ€": "age_50",
    "60ëŒ€": "age_60"
}

valid_emd_list = extract_emd_nm_list()
upjong_keywords = extract_upjong_code_map()
# ì˜ë„ë³„ ìš”êµ¬ íŒŒë¼ë¯¸í„° ì •ì˜
# intent: 0=ë§¤ì¶œ, 1=ìœ ë™ì¸êµ¬, 2=ìœ„í—˜ë„, 3=ì²­ì•½(ì˜ˆì‹œ)
INTENT_REQUIRED = {
    0: {"need": ["sido_or_sigungu_or_emd", "upjong_cd"], "optional": []},  # ë§¤ì¶œ ì¡°íšŒ: ì§€ì—­ì€ ìµœì†Œ 1ë‹¨ê³„, ì—…ì¢…ì€ ì„ íƒ
    1: {"need": ["sido_or_sigungu_or_emd"], "optional": ["gender", "age_group"]},  # ìœ ë™ì¸êµ¬: ì§€ì—­ í•„ìˆ˜, ì„±ë³„/ì—°ë ¹ ì„ íƒ
    2: {"need": ["sido_or_sigungu_or_emd"], "optional": ["upjong_cd"]},  # ìœ„í—˜ë„: ì§€ì—­ í•„ìˆ˜
    3: {"need": ["sido_or_sigungu_or_emd"], "optional": []},  # ì²­ì•½: ì§€ì—­ í•„ìˆ˜(í•„ìš”ì— ë§ê²Œ ìˆ˜ì •)
}

# 1) í•œê¸€+ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°
_NORM_RE = re.compile(r'[^ê°€-í£0-9]')

def normalize_kr(s: str) -> str:
    return _NORM_RE.sub('', s or '')

_ENUM_SEP_RE = re.compile(r'[Â·ã†\.\,\/]')

# ì˜ˆ: "ì¢…ë¡œ1Â·2Â·3Â·4ê°€ë™"  /  "ëª…ë¥œ1Â·2ê°€ë™"
#  - prefix: "ì¢…ë¡œ" / "ëª…ë¥œ"
#  - numseq: "1Â·2Â·3Â·4" / "1Â·2"
#  - suf   : "ê°€ë™" (ë˜ëŠ” "ë™", "ê°€")
_ENUM_PATTERN = re.compile(
    r'^(?P<prefix>[ê°€-í£]+)\s*(?P<numseq>\d+(?:[Â·ã†\.\,\/]\d+)+)\s*(?P<suf>ê°€ë™|ë™|ê°€)$'
)

def gen_emd_variants(name: str) -> Set[str]:
    """
    ê³µì‹ í–‰ì •ë™ëª… í•˜ë‚˜ë¡œë¶€í„° ë§¤ì¹­ì— ì‚¬ìš©í•  'ì •ê·œí™” ë³€í˜•ë¬¸ìì—´' ì§‘í•© ìƒì„±.
    - ë¬¶ìŒí˜•(ì¢…ë¡œ1Â·2Â·3Â·4ê°€ë™) â†’ ê° ë²ˆí˜¸ë³„ íŒŒìƒ(ì¢…ë¡œ1ê°€ë™, ì¢…ë¡œ2ê°€ë™, ...)
    - 'ê°€ë™'ì¸ ê²½ìš°, ì‚¬ëŒë“¤ì´ ë§ì´ ì¹˜ëŠ” 'ì¢…ë¡œ3ê°€'ë„ í—ˆìš© (ê·œì¹™ ê¸°ë°˜, ë³„ì¹­ í…Œì´ë¸” ì•„ë‹˜)
    - 'ì œìˆ«ì' â†” 'ìˆ«ì' ë³€í˜•ë„ ì–‘ë°©í–¥ ìƒì„±
    """
    variants: Set[str] = set()
    raw = (name or '').strip()

    # ê¸°ë³¸í˜•(ê³µì‹ëª… ìì²´)
    base_norm = normalize_kr(raw)
    if base_norm:
        variants.add(base_norm)

    # 1) ë²ˆí˜¸ ë‚˜ì—´(Â·/ã†/./,/ /)ì´ ë“¤ì–´ê°„ ë¬¶ìŒí˜•ì¼ ê²½ìš° â†’ ìë™ ë¶„í•´
    m = _ENUM_PATTERN.match(raw)
    if m:
        prefix = m.group('prefix')
        numseq = m.group('numseq')
        suf    = m.group('suf')  # 'ê°€ë™' | 'ë™' | 'ê°€'
        nums = [n for n in _ENUM_SEP_RE.split(numseq) if n]

        for num in nums:
            # ex) ì¢…ë¡œ3ê°€ë™
            cand = f"{prefix}{num}{suf}"
            variants.add(normalize_kr(cand))
            # ì‚¬ëŒë“¤ì´ ìì£¼ ì¹˜ëŠ” í˜•íƒœ: 'ê°€ë™'ì´ë©´ '...ê°€'ë„ í—ˆìš© (ex: ì¢…ë¡œ3ê°€)
            if suf == 'ê°€ë™':
                variants.add(normalize_kr(f"{prefix}{num}ê°€"))

            # 'ì œìˆ«ì' í‘œê¸° í—ˆìš© (ì œ3ê°€ë™ / ì œ3ê°€)
            variants.add(normalize_kr(f"{prefix}ì œ{num}{suf}"))
            if suf == 'ê°€ë™':
                variants.add(normalize_kr(f"{prefix}ì œ{num}ê°€"))

        return variants  # ë¬¶ìŒí˜•ì´ë©´ ì—¬ê¸°ì„œ ë

    # 2) ì¼ë°˜í˜•: 'ì œìˆ«ìì ‘ë¯¸' â†” 'ìˆ«ìì ‘ë¯¸' ì–‘ë°©í–¥
    #   ì˜ˆ) 'ì°½ì‹ ì œ1ë™' â†’ 'ì°½ì‹ 1ë™',  'ì°½ì‹ 1ë™' â†’ 'ì°½ì‹ ì œ1ë™'
    m2 = re.search(r'ì œ(\d+)(ë™|ê°€ë™|ì|ë©´|ë¦¬|ê°€)$', raw)
    if m2:
        num, suf = m2.groups()
        variants.add(normalize_kr(re.sub(r'ì œ(\d+)(ë™|ê°€ë™|ì|ë©´|ë¦¬|ê°€)$', f'{num}{suf}', raw)))

    m3 = re.search(r'(\d+)(ë™|ê°€ë™|ì|ë©´|ë¦¬|ê°€)$', raw)
    if m3:
        num, suf = m3.groups()
        variants.add(normalize_kr(re.sub(r'(\d+)(ë™|ê°€ë™|ì|ë©´|ë¦¬|ê°€)$', f'ì œ{num}{suf}', raw)))

    return variants

# 3) (ì´ˆê¸°í™” ì‹œ 1íšŒ) í–‰ì •ë™ ì¸ë±ìŠ¤ êµ¬ì¶•: ë³€í˜•ë“¤ -> ì›ë³¸ëª… ë§¤í•‘
def build_emd_index(valid_emd_list: List[str]) -> Dict[str, str]:
    index = {}
    for emd in valid_emd_list:
        for v in gen_emd_variants(emd):
            # ê°™ì€ í‚¤ê°€ ì—¬ëŸ¬ ì›ë³¸ì— ë§¤í•‘ë˜ë©´ ë” ê¸´(íŠ¹ì´ì„±ì´ ë†’ì€) ì›ë³¸ì„ ìš°ì„ 
            if v not in index or len(emd) > len(index[v]):
                index[v] = emd
    return index

# 4) ì›ë¬¸ì—ì„œ ìµœì¥ì¼ì¹˜ íƒìƒ‰ (í† í° ë¬´ì‹œ, ë¬¸ìì—´ ê¸°ë°˜)
def find_emd_in_text(user_input: str, emd_index: Dict[str, str]) -> List[str]:
    """
    ì •ê·œí™”ëœ ì…ë ¥ì—ì„œ ì‚¬ì „ í‚¤(ì •ê·œí™”ëœ ë³€í˜•)ë“¤ì„ ê¸¸ì´ìˆœìœ¼ë¡œ ìŠ¤ìº”í•´ ìµœì¥ì¼ì¹˜ë¡œ ë½‘ëŠ”ë‹¤.
    ì…ë ¥ ê¸¸ì´ê°€ ì§§ìœ¼ë‹ˆ O(N*M) ë‹¨ìˆœ ìŠ¤ìº”ìœ¼ë¡œë„ ì¶©ë¶„íˆ ë¹ ë¦„.
    """
    text = normalize_kr(user_input)
    if not text:
        return []

    # í‚¤ë“¤ì„ ê¸¸ì´ ê¸´ ìˆœìœ¼ë¡œ ì •ë ¬ â†’ ìµœì¥ì¼ì¹˜ ìš°ì„ 
    keys = sorted(emd_index.keys(), key=len, reverse=True)

    hits = []
    used = [False] * len(text)  # ê²¹ì¹¨ ë°©ì§€ìš© ë§ˆìŠ¤í¬

    def can_place(start: int, end: int) -> bool:
        return all(not used[i] for i in range(start, end))

    def place(start: int, end: int):
        for i in range(start, end):
            used[i] = True

    for k in keys:
        start = 0
        while True:
            idx = text.find(k, start)
            if idx == -1:
                break
            j = idx + len(k)
            if can_place(idx, j):
                place(idx, j)
                hits.append(emd_index[k])  # ì›ë³¸ëª… ì €ì¥
            start = idx + 1

    # ì¤‘ë³µ ì œê±°, ì…ë ¥ ë‚´ ë“±ì¥ ìˆœì„œ ìœ ì§€
    seen = set()
    ordered = []
    for h in hits:
        if h not in seen:
            seen.add(h)
            ordered.append(h)
    return ordered

# âœ… API ë¼ìš°íŒ…
@app.route("/predict", methods=["GET"])
def predict():
    question = request.args.get("text", "").strip()

    if not question:
        return jsonify({"error": "text íŒŒë¼ë¯¸í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400

    # ì˜ˆì¸¡
    intent_id, confidence = predict_intent(question)

    # 2) íŒŒë¼ë¯¸í„° ë¶„ì„
    entities, missing = analyze_input(question, intent_id, valid_emd_list)

    # 3) ëˆ„ë½ ì•ˆë‚´
    if missing:
        parts = []
        if "region" in missing: parts.append("ì§€ì—­(ì‹œ/êµ¬/ë™)")
        if "upjong_cd" in missing:
            hint = f" (ì¸ì‹: {entities['raw_upjong']})" if entities.get("raw_upjong") else " (ì˜ˆ: ì¹´í˜, í¸ì˜ì )"
            parts.append("ì—…ì¢…" + hint)
        return jsonify({
            "intent": intent_id,
            "confidence": round(confidence, 4),
            "need_more": missing,
            "entities": entities,
            "message": " / ".join(parts) + " ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
        }), 200

    if intent_id == 0:  # ë§¤ì¶œ
        return jsonify({
            "intent": 0,
            "confidence": round(confidence, 4),
            "entities": entities,  # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200

    elif intent_id == 1:  # ìœ ë™ì¸êµ¬
        return jsonify({
            "intent": 1,
            "confidence": round(confidence, 4),
            "entities": entities,  # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200


    elif intent_id == 2:  # ìœ„í—˜ë„
        return jsonify({
            "intent": 2,
            "confidence": round(confidence, 4),
            "entities": entities,  # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200

    else:  # intent_id == 3 ë“±
        return jsonify({
            "intent": int(intent_id),
            "confidence": round(confidence, 4),
            "entities": entities,  # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200


_BRACKETS = re.compile(r"\[[^\]]+\]")
_CANCEL_RX = re.compile(r"(ê¸°?ë§ì†Œ|í•´ì§€)")
_HANGUL_OR_NUM = re.compile(r"[ê°€-í£0-9]")


def _norm_ws(s: str) -> str:
    return " ".join((s or "").replace("\u200b", "").split()).strip()


def _stable_unique(seq):
    seen, out = set(), []
    for x in seq:
        if x not in seen:
            seen.add(x);
            out.append(x)
    return out


_WS = re.compile(r"\s+")
_MONEY = re.compile(r"ê¸ˆ?\s*([0-9,]+)\s*ì›")

# 'ë§ì†Œ/í•´ì§€' í”Œë˜ê·¸ë§Œ í™•ì¸
_CANCEL_FLAG_RX = re.compile(r"(ë§ì†Œ|í•´ì§€)")
# ë³´ì¡°: purpose í…ìŠ¤íŠ¸ì—ì„œ "1ë²ˆ ê·¼ì €ë‹¹ê¶Œ ì„¤ì •" ê°™ì€ í‘œê¸°ë¥¼ íŒŒì‹± (ìˆœìœ„ë²ˆí˜¸ ì¹¸ì´ ë¹„ì—ˆì„ ë•Œë§Œ ì‚¬ìš©)
_CANCEL_TARGET_RX = re.compile(r"(\d+)\s*ë²ˆ")
# ì·¨ì†Œ ëŒ€ìƒ ë²ˆí˜¸ë“¤: "ì œ1ë²ˆ", "1ë²ˆ", "1,2ë²ˆ", "1ë²ˆÂ·2ë²ˆ" ë“±ì—ì„œ ëª¨ë‘ ì¶”ì¶œ
_RANK_LIST_RX = re.compile(r"(?:ì œ?\s*)(\d+)\s*ë²ˆ")

_RISK_FLAG_RX = re.compile(r"(ê°€ì••ë¥˜|ê°€ì²˜ë¶„|ì••ë¥˜)")


def _one_line(s: str) -> str:
    return _WS.sub(" ", (s or "").strip())


def _parse_amount_num(text: str):
    m = _MONEY.search(text or "")
    if not m:
        return None
    try:
        return int(m.group(1).replace(",", ""))
    except:
        return None


EUL_RX = re.compile(r"ã€\s*ì„\s*êµ¬\s*ã€‘")
ANY_SECTION_RX = re.compile(r"ã€\s*(?:í‘œ\s*ì œ\s*ë¶€|ê°‘\s*êµ¬|ì„\s*êµ¬)\s*ã€‘")


def extract_mortgage_info(pdf_bytes: bytes):
    """
    ã€ì„êµ¬ã€‘ ì „ì²´(ì—¬ëŸ¬ í˜ì´ì§€ ì´ì–´ì§ í¬í•¨)ë¥¼ ìŠ¤ìº”:
      - 'ì„¤ì •'ë§Œ ìˆ˜ì§‘
      - 'ë§ì†Œ/í•´ì§€' í–‰ì€ í•´ë‹¹ ëª©ì  í…ìŠ¤íŠ¸ì— í¬í•¨ëœ 'â€¦ë²ˆ' ìˆœìœ„ë“¤ì„ ì·¨ì†Œ ëŒ€ìƒìœ¼ë¡œ ë§ˆí‚¹
    ë°˜í™˜: {"mortgages":[...], "riskFlags":[...]}
    """
    mortgages_by_rank = {}
    cancel_ranks = set()
    risks = []

    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        in_eul = False  # ì„êµ¬ ì„¹ì…˜ ì•ˆì— ìˆëŠ”ì§€ ìƒíƒœ í”Œë˜ê·¸

        for page in pdf.pages:
            txt = page.extract_text() or ""

            # ì„¹ì…˜ í—¤ë” ìŠ¤ìœ„ì¹­
            if EUL_RX.search(txt):
                in_eul = True
            elif ANY_SECTION_RX.search(txt):
                # ë‹¤ë¥¸ ì„¹ì…˜ í—¤ë”(ê°‘êµ¬/í‘œì œë¶€ ë“±)ë¥¼ ë§Œë‚˜ë©´ ì„êµ¬ ì¢…ë£Œ
                if in_eul:
                    in_eul = False
                # ì„êµ¬ê°€ ì•„ë‹ˆë¯€ë¡œ ë„˜ì–´ê°
            # ì„¹ì…˜ ì™¸ í˜ì´ì§€ëŠ” ìŠ¤í‚µ
            if not in_eul:
                continue

            # ---- ì—¬ê¸°ë¶€í„°ëŠ” ì„êµ¬ ì„¹ì…˜ ì•ˆì˜ í˜ì´ì§€ ì „ë¶€ ì²˜ë¦¬ ----
            tables = page.extract_tables() or []
            for tb in tables:
                if not tb:
                    continue

                # í—¤ë” ìœ„ì¹˜ ì¶”ì • (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                col_rank = col_purpose = col_amount = None
                for row in tb[:5]:
                    cells = [(_one_line(c) if c else "") for c in (row or [])]
                    for i, c in enumerate(cells):
                        if col_rank is None and (c in ("ìˆœìœ„ë²ˆí˜¸", "ìˆœìœ„")):    col_rank = i
                        if col_purpose is None and ("ë“±ê¸°" in c and "ëª©ì " in c):  col_purpose = i
                        if col_amount is None and ("ì±„ê¶Œìµœê³ ì•¡" in c):            col_amount = i
                if col_rank is None: col_rank = 0
                if col_purpose is None: col_purpose = 1
                if col_amount is None:  col_amount = 4

                for row in tb:
                    if not row or len(row) <= max(col_rank, col_purpose, col_amount):
                        continue

                    rank_raw = _one_line(row[col_rank])
                    purpose = _one_line(row[col_purpose])
                    amount_txt = _one_line(row[col_amount])

                    # 1) ë§ì†Œ/í•´ì§€ -> í…ìŠ¤íŠ¸ ë‚´ 'â€¦ë²ˆ'ë“¤ë§Œ ì·¨ì†Œ ëŒ€ìƒìœ¼ë¡œ ìˆ˜ì§‘
                    if _CANCEL_FLAG_RX.search(purpose):
                        for n in _RANK_LIST_RX.findall(purpose):  # ["1","2",...]
                            cancel_ranks.add(int(n))
                        continue

                    # 2) ê·¼ì €ë‹¹ê¶Œ 'ì„¤ì •'ë§Œ ìˆ˜ì§‘
                    if ("ê·¼ì €ë‹¹ê¶Œ" in purpose) and ("ì„¤ì •" in purpose):
                        if rank_raw.isdigit():
                            row_rank = int(rank_raw)
                            mortgages_by_rank[row_rank] = {
                                "rankNo": row_rank,
                                "amountKRW": _parse_amount_num(amount_txt),
                                "status": "normal",
                            }

                    # ìœ„í—˜ ìŠ¤ìº”(ê¸°ì¡´)
                    if _RISK_FLAG_RX.search(purpose):
                        risks.append({
                            "rankNo": int(rank_raw) if rank_raw.isdigit() else None,
                            "purpose": purpose
                        })

    # ë§ì†Œ ëŒ€ìƒ ìˆœìœ„ë¥¼ ì·¨ì†Œ ì²˜ë¦¬
    for r in cancel_ranks:
        if r in mortgages_by_rank:
            mortgages_by_rank[r]["status"] = "cancelled"

    return {
        "mortgages": sorted(mortgages_by_rank.values(), key=lambda x: x["rankNo"]),
        "riskFlags": risks
    }


def extract_joint_collateral_addresses_follow(
        pdf_bytes: bytes,
        must_have_bracket: bool = True,
        max_follow: int = 6,
):
    """
    ã€ê³µë™ë‹´ë³´ëª©ë¡ã€‘ì—ì„œ 'ë¶€ë™ì‚°ì— ê´€í•œ ê¶Œë¦¬ì˜ í‘œì‹œ' ì£¼ì†Œë§Œ ì¶”ì¶œ. + [ì§‘í•©ê±´ë¬¼] í˜„ì¬ì£¼ì†Œ ì¶”ê°€
    - ì²« í˜ì´ì§€: í—¤ë”(ë¶€ë™ì‚°/í‘œì‹œ) ë°˜ë“œì‹œ ì°¾ê³  ê·¸ ì—´ë§Œ ì‚¬ìš©
    - ì´ì–´ì§€ëŠ” í˜ì´ì§€: í—¤ë”ê°€ ì—†ì–´ë„ ì£¼ì†Œì—´ì„ 'í…ìŠ¤íŠ¸ëŸ‰/í•œê¸€+ìˆ«ìëŸ‰' ìŠ¤ì½”ì–´ë¡œ ì¶”ì •
    - ê°™ì€ í–‰ì— 'í•´ì§€/ë§ì†Œ/ê¸°ë§ì†Œ' í¬í•¨ë˜ë©´ ì œì™¸
    """

    # [ì§‘í•©ê±´ë¬¼] ë’¤ ì£¼ì†Œ ì¶”ì¶œ
    def _extract_current_address(pdf):
        for page in pdf.pages:
            txt = page.extract_text() or ""
            m = re.search(r"\[ì§‘í•©ê±´ë¬¼\]\s*(.+)", txt)
            if m:
                return _norm_ws(m.group(1))
        return None

    def _find_header_top(page):
        txt = page.extract_text() or ""
        if not re.search(r"ã€\s*ê³µë™ë‹´ë³´ëª©ë¡\s*ã€‘", txt):
            return False, 0.0
        words = page.extract_words(extra_attrs=["top", "text"]) or []
        tops = [w["top"] for w in words if "ê³µë™ë‹´ë³´ëª©ë¡" in (w.get("text") or "")]
        return True, min(tops) if tops else 0.0

    def _extract_tables_on(page, crop_bbox=None):
        tgt = page.within_bbox(crop_bbox) if crop_bbox else page
        tables = []
        try:
            tables = tgt.extract_tables(table_settings={
                "vertical_strategy": "lines",
                "horizontal_strategy": "lines",
                "intersection_x_tolerance": 3,
                "intersection_y_tolerance": 3,
            }) or []
        except Exception:
            pass
        if not tables:
            try:
                tables = tgt.extract_tables() or []
            except Exception:
                tables = []
        return tables

    def _guess_addr_col(tb):
        """í—¤ë”ê°€ ì—†ì„ ë•Œ ì£¼ì†Œì—´ ì¶”ì •: (í•œê¸€+ìˆ«ì ê°œìˆ˜ ìŠ¤ì½”ì–´ + ì „ì²´ ê¸¸ì´ ìŠ¤ì½”ì–´) ìµœëŒ€"""
        if not tb or not tb[0]:
            return None
        cols = max(len(r or []) for r in tb)

        def col_score(i):
            kor_num = 0
            total = 0
            for r in tb[1:]:
                cell = (r[i] if i < len(r) else "") or ""
                t = _norm_ws(str(cell))
                kor_num += len(_HANGUL_OR_NUM.findall(t))
                total += len(t)
            # í•œê¸€/ìˆ«ì ë¹„ì¤‘ì„ ë” ê°€ì¤‘
            return kor_num * 2 + total

        scores = [col_score(i) for i in range(cols)]
        return max(range(cols), key=lambda i: scores[i])

    def _pick_addresses_from_tables(tables, require_header: bool, prev_last=None):
        hits = []

        for tb in tables:
            if not tb or not tb[0]:
                continue

            addr_idx = None
            header_row_idx = 0

            if require_header:
                for r_idx, row in enumerate(tb[:5]):
                    header = [(_ or "").strip() for _ in (row or [])]
                    header_txt = " ".join(header)
                    if "ë¶€ë™ì‚°" in header_txt and "í‘œì‹œ" in header_txt:
                        header_row_idx = r_idx
                        try:
                            addr_idx = next(i for i, h in enumerate(header) if ("ë¶€ë™ì‚°" in h and "í‘œì‹œ" in h))
                        except StopIteration:
                            addr_idx = None
                        break
                if addr_idx is None:
                    continue
            else:
                addr_idx = _guess_addr_col(tb)
                if addr_idx is None:
                    continue

            # í–‰ ìŠ¤ìº” (hitsë§Œ ì‚¬ìš©)
            for r in tb[header_row_idx + 1:]:
                serial = (r[0] or "").strip() if r else ""
                row_text = " ".join((c or "") for c in r)

                cell = r[addr_idx] if addr_idx < len(r) else None
                if not cell or not str(cell).strip():
                    continue
                cleaned = _norm_ws(_BRACKETS.sub(" ", str(cell)))
                if not cleaned:
                    continue

                # ë¹ˆ ì¼ë ¨ë²ˆí˜¸: ì§ì „ í•­ëª©ì— merge
                if serial == "":
                    target = hits[-1] if hits else prev_last
                    if target is not None:
                        target["address"] += " " + cleaned
                        if _CANCEL_RX.search(row_text):
                            target["status"] = "cancelled"
                    # ë¶™ì¼ ëŒ€ìƒì´ ì „í˜€ ì—†ìœ¼ë©´ ê³ ì•„ ë¼ì¸ â†’ ìŠ¤í‚µ
                    continue

                if not serial.isdigit():
                    continue

                status = "cancelled" if _CANCEL_RX.search(row_text) else "normal"
                hits.append({"serial": serial, "address": cleaned, "status": status})

        return hits

    pages_hit, addresses = [], []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:

        # í˜„ì¬ì£¼ì†Œ ë¨¼ì € ë½‘ê¸°
        current_addr = _extract_current_address(pdf)

        i = 0
        while i < len(pdf.pages):
            page = pdf.pages[i]

            found, header_top = _find_header_top(page)
            if must_have_bracket and not found:
                i += 1
                continue

            crop = (0, header_top, page.width, page.height)

            # ì‹œì‘ í˜ì´ì§€
            prev_last = addresses[-1] if addresses else None
            addrs = _pick_addresses_from_tables(_extract_tables_on(page, crop), require_header=True,
                                                prev_last=prev_last)
            if addrs:
                pages_hit.append(i + 1)
                addresses.extend(addrs)

            # ì´ì–´ì§€ëŠ” í˜ì´ì§€
            follow, j = 0, i + 1
            while j < len(pdf.pages) and follow < max_follow:
                tables_j = _extract_tables_on(pdf.pages[j])
                if not tables_j:
                    break

                prev_last = addresses[-1] if addresses else None
                more = _pick_addresses_from_tables(tables_j, require_header=False, prev_last=prev_last)
                if more:
                    pages_hit.append(j + 1)
                    addresses.extend(more)

                follow += 1
                j += 1

            i += 1

    # ì—¬ê¸°ë¶€í„°ëŠ” addressesê°€ [ {serial, address, status}, ... ]
    return {"pages": sorted(set(pages_hit)), "addresses": addresses, "currentAddress": current_addr}


_OWNER_CHANGE_RX = re.compile(r"ì†Œìœ ê¶Œ.*ì´ì „")
_CO_OWNER_RX = re.compile(r"ê³µìœ ")


# ê°‘êµ¬ ë¶„ì„
def extract_gabu_info(pdf_bytes: bytes):
    """
    ã€ê°‘êµ¬ã€‘ì—ì„œ ì†Œìœ ê¶Œ ë³€ê²½ ì´ë ¥, ê³µë™ì†Œìœ  ì—¬ë¶€ ì¶”ì¶œ
    ë°˜í™˜:
    {
        "ownerChangeCount": int,
        "ownerChangeDetails": [ ... ],
        "coOwners": bool
    }
    """
    title_rx = re.compile(r"ã€\s*ê°‘\s*êµ¬\s*ã€‘")
    owner_changes = []
    co_owners = False

    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for page in pdf.pages:
            txt = page.extract_text() or ""
            if not title_rx.search(txt):
                continue

            tables = page.extract_tables() or []
            for tb in tables:
                if not tb:
                    continue

                for row in tb:
                    cells = [(_one_line(c) if c else "") for c in (row or [])]
                    if not cells:
                        continue
                    purpose = cells[1] if len(cells) > 1 else ""  # ë“±ê¸°ëª©ì 
                    right_holder = cells[2] if len(cells) > 2 else ""  # ê¶Œë¦¬ì/ê¸°íƒ€ì‚¬í•­

                    # ì†Œìœ ê¶Œ ì´ì „ íƒì§€
                    if _OWNER_CHANGE_RX.search(purpose):
                        owner_changes.append(purpose)

                    # ê³µìœ ì ì—¬ë¶€ íƒì§€
                    if _CO_OWNER_RX.search(right_holder):
                        co_owners = True

    return {
        "ownerChangeCount": len(owner_changes),
        "ownerChangeDetails": owner_changes,
        "coOwners": co_owners
    }


import io, re, pdfplumber


def _find_y_bottom_of(words, pattern):
    """pattern(ì •ê·œì‹)ì— ë§¤ì¹­ë˜ëŠ” ë‹¨ì–´ë“¤ì˜ 'bottom' ì¤‘ ìµœëŒ“ê°’ ë°˜í™˜"""
    cands = [w for w in words if re.search(pattern, w["text"])]
    return max((w["bottom"] for w in cands), default=None)


def _find_y_top_of_section_linewise(words, patterns):
    """
    words: page.extract_words() ê²°ê³¼
    patterns: ê³µë°± ì—†ëŠ” ì •ê·œì‹ íŒ¨í„´ ëª©ë¡ (ì˜ˆ: 'ê°‘êµ¬', 'ì„êµ¬', 'í‘œì œë¶€', 'ì „ìœ ë¶€ë¶„ì˜ê±´ë¬¼ì˜í‘œì‹œ', 'ëŒ€ì§€ê¶Œì˜í‘œì‹œ')
    ë°˜í™˜: í•´ë‹¹ ì„¹ì…˜ ë¼ì¸ì˜ top (ì—†ìœ¼ë©´ None)
    """
    # 1) ê°™ì€ ì¤„(ìœ ì‚¬ y)ì— ìˆëŠ” wordë“¤ì„ ë¬¶ì–´ì„œ í•œ ì¤„ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
    lines = defaultdict(list)
    for w in words:
        # ê°™ì€ ì¤„ íŒì •: top ê°’ì„ ë°˜ì˜¬ë¦¼í•´ì„œ keyë¡œ ì‚¬ìš© (í—ˆìš© ì˜¤ì°¨ 2~3px ì •ë„)
        key = round(w["top"] / 2)  # í•„ìš”í•˜ë©´ 3,5 ë“±ìœ¼ë¡œ ì¡°ì ˆ
        lines[key].append(w)

    y_candidates = []
    for key, ws in lines.items():
        # í•œ ì¤„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì—°ê²°
        ws_sorted = sorted(ws, key=lambda x: x["x0"])
        line_text = "".join(w["text"] for w in ws_sorted)

        # 2) ê³µë°±/ê´„í˜¸/íŠ¹ìˆ˜ê¸°í˜¸ ì œê±° í›„ ë¹„êµ
        norm = re.sub(r"[\sã€ã€‘\[\]\(\)<>]", "", line_text)

        # 3) íŒ¨í„´ ë§¤ì¹­ (ê³µë°± ì—†ì´ ë§Œë“  íŒ¨í„´ê³¼ ë¹„êµ)
        if any(re.search(p, norm) for p in patterns):
            # ì´ ë¼ì¸ì˜ ì‹¤ì œ topì€ ë¼ì¸ ë‚´ ë‹¨ì–´ë“¤ì˜ ìµœì†Œ top ì‚¬ìš©
            y_top = min(w["top"] for w in ws_sorted)
            y_candidates.append(y_top)

    return min(y_candidates) if y_candidates else None


def _combine_address_lines(lines):
    """
    ì†Œì¬ì§€ë²ˆ ì…€ ë‚´ ì¤„ë“¤ì´
      [ '1. ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬ ì„œì´ˆë™', '1317-16', '2. ì„œìš¸íŠ¹ë³„ì‹œ ì„œì´ˆêµ¬ ì„œì´ˆë™', '1317-17', ... ]
    í˜•íƒœì¼ ë•Œ, (í–‰ì •êµ¬ì—­ + ì§€ë²ˆ) í˜ì–´ë¡œ í•©ì³ì„œ í•œ ì¤„ì”© ë°˜í™˜
    """
    combined = []
    buf = None
    for s in lines:
        s = s.strip()
        if not s:
            continue
        if buf is None:
            buf = s
        else:
            combined.append((buf + " " + s).strip())
            buf = None
    if buf is not None:  # í™€ìˆ˜ ê°œë¡œ ëë‚˜ë©´ ë‚¨ì€ ê²ƒ ê·¸ëŒ€ë¡œ
        combined.append(buf)
    return combined


def extract_land_share_table(pdf_bytes: bytes):
    """
    (ëŒ€ì§€ê¶Œì˜ ëª©ì ì¸ í† ì§€ì˜ í‘œì‹œ) ì´í›„ ì˜ì—­ë§Œ ì˜ë¼ í…Œì´ë¸” ì¶”ì¶œ
    - í˜ì´ì§€ ë‚´ í—¤ë” yì¢Œí‘œ ì•„ë˜ë¡œ crop
    - ê°™ì€ í˜ì´ì§€ì— 'ê°‘êµ¬/ì„êµ¬'ê°€ ë‚˜ì˜¤ë©´ ê·¸ ìœ„ê¹Œì§€ë§Œ crop
    - ì†Œì¬ì§€ë²ˆ ì¤„ë°”ê¿ˆ(í–‰ì •êµ¬ì—­/ì§€ë²ˆ) í˜ì–´ë¥¼ í•©ì³ í•œ ì¤„ ì£¼ì†Œë¡œ ì •ê·œí™”
    """
    results = []
    in_section = False

    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for page in pdf.pages:
            words = page.extract_words(use_text_flow=True, keep_blank_chars=False)

            # 1) ì„¹ì…˜ ì‹œì‘ y ì°¾ê¸° (í˜„ì¬ í˜ì´ì§€ì—ì„œ í—¤ë” ë°œê²¬ ì‹œ ê·¸ ì•„ë«ë¶€ë¶„ë§Œ)
            y0 = None
            if not in_section:
                # 'ëŒ€ì§€ê¶Œì˜' ë§Œìœ¼ë¡œë„ ì‹œì‘ í¬ì°© (ê³µë°± ë¬´ì‹œëŠ” ë‹¨ì–´ë‹¨ìœ„ë¼ '\s*'ë³´ë‹¨ í‚¤ì›Œë“œë¡œ)
                y0 = _find_y_bottom_of(words, r"ëŒ€ì§€ê¶Œ")
                if y0 is not None:
                    in_section = True
                else:
                    continue  # ì•„ì§ ì„¹ì…˜ ì‹œì‘ ì•ˆí•¨ â†’ ë‹¤ìŒ í˜ì´ì§€ë¡œ

            # 2) ì„¹ì…˜ ì¢…ë£Œ y ì°¾ê¸° (ê°™ì€ í˜ì´ì§€ì— ê°‘êµ¬/ì„êµ¬ê°€ ìˆìœ¼ë©´ ê·¸ ìœ„ê¹Œì§€)
            # ì¢…ë£Œ í‚¤ì›Œë“œë“¤: ê³µë°±/ê´„í˜¸ ì œê±°í•œ í˜•íƒœë¡œ ëŒ€ë¹„
            end_patterns = [
                r"ê°‘êµ¬",
                r"ì„êµ¬",
                r"ì „ìœ ë¶€ë¶„ì˜ê±´ë¬¼ì˜í‘œì‹œ",
                r"ëŒ€ì§€ê¶Œì˜í‘œì‹œ"
            ]
            y1 = _find_y_top_of_section_linewise(words, end_patterns)

            # 3) í¬ë¡­ ë°•ìŠ¤ ì„¤ì • (í—¤ë”ê°€ ì´ í˜ì´ì§€ì—ì„œ ì‹œì‘í–ˆìœ¼ë©´ y0 ì•„ë˜ë¶€í„°, ì´ì–´ì§€ëŠ” í˜ì´ì§€ë©´ ì „ì²´ ìƒë‹¨ë¶€í„°)
            top = (y0 + 2) if y0 is not None else 0
            bottom = (y1 - 2) if y1 is not None else page.height
            bbox = (0, top, page.width, bottom)
            sub = page.crop(bbox)

            # 4) í…Œì´ë¸”ë§Œ ì¶”ì¶œ
            tables = sub.extract_tables()

            for table in tables:
                for row in table:
                    if not row:
                        continue
                    # None â†’ "" ë° ì¢Œìš° ê³µë°± ì œê±°
                    row = [str(c).strip() if c else "" for c in row]
                    joined = "".join(row)

                    # í—¤ë”í–‰ ìŠ¤í‚µ (ê³µë°±/ê°œí–‰ ë¶ˆê·œì¹™ ëŒ€ì‘)
                    if re.search(r"í‘œ\s*ì‹œ\s*ë²ˆ\s*í˜¸", joined) or re.search(r"ì†Œ\s*ì¬\s*ì§€\s*ë²ˆ", joined):
                        continue

                    # ì—´ ìˆ˜ ë³´ì • (ìµœì†Œ 4ì—´ ê°€ì •: í‘œì‹œë²ˆí˜¸, ì†Œì¬ì§€ë²ˆ, ì§€ëª©, ë©´ì , (ë¹„ê³ ))
                    while len(row) < 5:
                        row.append("")

                    í‘œì‹œë²ˆí˜¸, ì†Œì¬ì§€ë²ˆ, ì§€ëª©, ë©´ì , ë¹„ê³  = row

                    # ì†Œì¬ì§€ë²ˆ/ì§€ëª©/ë©´ì ì— ì¤„ë°”ê¿ˆì´ ë¶™ì–´ ì—¬ëŸ¬ ê±´ì´ í•œ ì…€ì— ìˆì„ ìˆ˜ ìˆìŒ â†’ ë¶„í•´
                    addr_lines = ì†Œì¬ì§€ë²ˆ.split("\n") if ì†Œì¬ì§€ë²ˆ else []
                    jimo_lines = ì§€ëª©.split("\n") if ì§€ëª© else []
                    area_lines = ë©´ì .split("\n") if ë©´ì  else []

                    # ì†Œì¬ì§€ë²ˆì€ (í–‰ì •êµ¬ì—­ + ì§€ë²ˆ) í˜ì–´ë¡œ í•©ì¹˜ê¸°
                    if len(addr_lines) >= 2:
                        addr_lines = _combine_address_lines(addr_lines)

                    # ê°€ì¥ ê¸´ ê¸¸ì´ì— ë§ì¶° í–‰ ë¶„í•´
                    max_len = max(len(addr_lines), len(jimo_lines), len(area_lines), 1)
                    for i in range(max_len):
                        results.append({
                            "í‘œì‹œë²ˆí˜¸": í‘œì‹œë²ˆí˜¸ if i == 0 else f"{í‘œì‹œë²ˆí˜¸}-{i + 1}" if í‘œì‹œë²ˆí˜¸ else "",
                            "ì†Œì¬ì§€ë²ˆ": (addr_lines[i].strip() if i < len(addr_lines) else "").strip() or None,
                            "ì§€ëª©": (jimo_lines[i].strip() if i < len(jimo_lines) else "").strip() or None,
                            "ë©´ì ": (area_lines[i].strip() if i < len(area_lines) else "").strip() or None,
                            "ë¹„ê³ ": ë¹„ê³  or None
                        })

            # 5) ì´ í˜ì´ì§€ì—ì„œ ì„¹ì…˜ì´ ëë‚¬ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
            if y1 is not None:
                in_section = False
                break

    # í›„ì²˜ë¦¬: ì™„ì „ ë¹ˆ í–‰ë“¤ ì œê±°
    cleaned = [
        r for r in results
        if any([r.get("ì†Œì¬ì§€ë²ˆ"), r.get("ì§€ëª©"), r.get("ë©´ì ")])
    ]
    return cleaned


_num = r'[0-9][0-9\.,]*'  # ìˆ«ì(ì½¤ë§ˆ/ì†Œìˆ˜ì  í¬í•¨)


def _parse_ratio_pairs(cell_text: str):
    """
    ì…ë ¥: "102446.2ë¶„ì˜\n1539.9454\n525165ë¶„ì˜\n7893.912\n..."
    ì¶œë ¥: [{"numerator":"102446.2","denominator":"1539.9454","text":"102446.2ë¶„ì˜ 1539.9454"}, ...]
    """
    if not cell_text:
        return []

    lines = [ln.strip() for ln in str(cell_text).split("\n") if ln.strip()]
    pairs = []
    pending_numer = None

    for ln in lines:
        # 1) ê°™ì€ ì¤„ì— ë‘˜ ë‹¤ ìˆëŠ” ê²½ìš°
        m_both = re.search(rf'({_num})\s*ë¶„ì˜\s*({_num})', ln)
        if m_both:
            num, den = m_both.group(1), m_both.group(2)
            pairs.append({
                "numerator": num,
                "denominator": den,
                "text": f"{num}ë¶„ì˜ {den}"
            })
            pending_numer = None
            continue

        # 2) ë¶„ìë§Œ ìˆëŠ” ì¤„ (â€¦ë¶„ì˜)
        m_num_only = re.search(rf'({_num})\s*ë¶„ì˜$', ln)
        if m_num_only:
            pending_numer = m_num_only.group(1)
            continue

        # 3) ìˆ«ìë§Œ ìˆëŠ” ì¤„ (ì´ì „ ì¤„ì˜ 'â€¦ë¶„ì˜'ì™€ ê²°í•© â†’ ë¶„ëª¨)
        m_num = re.fullmatch(rf'{_num}', ln)
        if m_num and pending_numer is not None:
            num, den = pending_numer, m_num.group(0)
            pairs.append({
                "numerator": num,
                "denominator": den,
                "text": f"{num}ë¶„ì˜ {den}"
            })
            pending_numer = None
            continue

        # 4) ì˜ˆì™¸(íŒ¨í„´ ë¶ˆì¼ì¹˜) â†’ ê·¸ëŒ€ë¡œ í…ìŠ¤íŠ¸ë¡œ ë‚¨ê¹€
        pairs.append({"numerator": None, "denominator": None, "text": ln})

    # ëì— ë¶„ìë§Œ ë‚¨ì•„ìˆìœ¼ë©´ ë³´ì¡´
    if pending_numer is not None:
        pairs.append({"numerator": pending_numer, "denominator": None, "text": f"{pending_numer}ë¶„ì˜"})

    # ìˆ˜ì¹˜í˜• ë¹„ìœ¨(den/num) ê³„ì‚° ë¶€ê°€ í•„ë“œ
    for p in pairs:
        try:
            if p["numerator"] and p["denominator"]:
                p["share"] = (Decimal(p["denominator"].replace(",", "")) /
                              Decimal(p["numerator"].replace(",", "")))
            else:
                p["share"] = None
        except (InvalidOperation, ZeroDivisionError):
            p["share"] = None

    return pairs


def _num_prefix(s: str):
    """ë¬¸ìì—´ ì•ì˜ ë²ˆí˜¸ ì¶”ì¶œ: '2 ì†Œìœ ê¶ŒëŒ€ì§€ê¶Œ' â†’ '2'"""
    if not s: return None
    m = re.match(r'^\s*(\d+)[\.\)]?\s*', s)
    return m.group(1) if m else None


# -------------------------------------------------
# ë©”ì¸: (ëŒ€ì§€ê¶Œì˜ í‘œì‹œ) ì„¹ì…˜ì—ì„œ í…Œì´ë¸” ì¶”ì¶œ
# columns: í‘œì‹œë²ˆí˜¸ | ëŒ€ì§€ê¶Œì¢…ë¥˜ | ëŒ€ì§€ê¶Œë¹„ìœ¨ | ë“±ê¸°ì›ì¸ ë° ê¸°íƒ€ì‚¬í•­
# -------------------------------------------------
def extract_land_right_ratios(pdf_bytes: bytes):
    results = []
    in_section = False
    last_group_no = ""

    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for page in pdf.pages:
            words = page.extract_words(use_text_flow=True, keep_blank_chars=False) or []

            # ì‹œì‘ ì§€ì  ì°¾ê¸°
            y0 = None
            if not in_section:
                y0 = _find_y_top_of_section_linewise(words, [r"ëŒ€ì§€ê¶Œì˜í‘œì‹œ"])
                if y0 is not None:
                    in_section = True
                else:
                    continue

            # ì¢…ë£Œ ì§€ì (ê°™ì€ í˜ì´ì§€ì— ë‹¤ìŒ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ê·¸ ìœ„ê¹Œì§€ë§Œ)
            end_patterns = [r"ê°‘êµ¬", r"ì„êµ¬"]
            y1 = _find_y_top_of_section_linewise(words, end_patterns)

            top = (y0 + 2) if y0 is not None else 0
            bottom = (y1 - 2) if y1 is not None else page.height
            sub = page.crop((0, top, page.width, bottom))

            tables = sub.extract_tables()
            for table in tables:
                for row in table:
                    if not row:
                        continue
                    # None -> "" + strip
                    row = [str(c).strip() if c else "" for c in row]
                    joined = "".join(row)

                    # í—¤ë” ë¼ì¸ ìŠ¤í‚µ (ê³µë°± ë¬´ì‹œ)
                    if re.search(r"í‘œ\s*ì‹œ\s*ë²ˆ\s*í˜¸", joined) or \
                            re.search(r"ëŒ€\s*ì§€\s*ê¶Œ\s*ì¢…\s*ë¥˜", joined) or \
                            re.search(r"ëŒ€\s*ì§€\s*ê¶Œ\s*ë¹„\s*ìœ¨", joined):
                        continue

                    # ì—´ ìˆ˜ ë³´ì •: [í‘œì‹œë²ˆí˜¸, ëŒ€ì§€ê¶Œì¢…ë¥˜, ëŒ€ì§€ê¶Œë¹„ìœ¨, ë“±ê¸°ì›ì¸ë°ê¸°íƒ€ì‚¬í•­]
                    while len(row) < 4:
                        row.append("")

                    group_no, kind_cell, ratio_cell, note_cell = row[:4]

                    # í‘œì‹œë²ˆí˜¸ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ì´ì „ ê·¸ë£¹ ë²ˆí˜¸ ìœ ì§€
                    if group_no:
                        last_group_no = group_no
                    else:
                        group_no = last_group_no

                    # ì¹¸ë³„ ì¤„ ë¶„í•´
                    kinds = [s.strip() for s in kind_cell.split("\n") if s.strip()] if kind_cell else []
                    ratios = _parse_ratio_pairs(ratio_cell)
                    notes = [s.strip() for s in note_cell.split("\n") if s.strip()] if note_cell else []

                    # ìµœëŒ“ê¸¸ì´ì— ë§ì¶° í™•ì¥
                    n = max(len(kinds), len(ratios), len(notes), 1)

                    for i in range(n):
                        raw_kind = kinds[i] if i < len(kinds) else None

                        # 1) ê·¸ë£¹ë²ˆí˜¸: ëŒ€ì§€ê¶Œì¢…ë¥˜ì˜ ì• ìˆ«ì â†’ í‘œì‹œë²ˆí˜¸ ì…€ ìˆ«ì â†’ ì´ì „ ê·¸ë£¹ ë²ˆí˜¸
                        grp_from_kind = _num_prefix(raw_kind)
                        grp_from_cell = _num_prefix(group_no)
                        group_no_final = grp_from_kind or grp_from_cell or last_group_no
                        if group_no_final:
                            last_group_no = group_no_final  # ë‹¤ìŒ í–‰ ëŒ€ë¹„ ê°±ì‹ 

                        # 2) ë¹„ìœ¨
                        r = ratios[i] if i < len(ratios) else {
                            "numerator": None, "denominator": None, "text": None, "share": None
                        }

                        results.append({
                            "í‘œì‹œë²ˆí˜¸ê·¸ë£¹": group_no_final,  # â† ì˜ˆ: "2"
                            "ëŒ€ì§€ê¶Œì¢…ë¥˜": raw_kind,  # â† ì˜ˆ: "2 ì†Œìœ ê¶ŒëŒ€ì§€ê¶Œ" (ìˆ«ì ë³´ì¡´)
                            "ëŒ€ì§€ê¶Œë¹„ìœ¨": {
                                "numerator": r["numerator"],
                                "denominator": r["denominator"],
                                "text": r["text"],
                                "share": (str(r["share"]) if r["share"] is not None else None)
                            },
                            "ë“±ê¸°ì›ì¸ë°ê¸°íƒ€ì‚¬í•­": notes[i] if i < len(notes) else None
                        })
            if y1 is not None:  # ì´ í˜ì´ì§€ì—ì„œ ì„¹ì…˜ ì¢…ë£Œ
                in_section = False
                break

    # ë¶ˆí•„ìš”í•œ ë¹ˆ ë ˆì½”ë“œ ì œê±°
    cleaned = [x for x in results if any([x.get("ëŒ€ì§€ê¶Œì¢…ë¥˜"), x.get("ëŒ€ì§€ê¶Œë¹„ìœ¨", {}).get("text")])]
    return cleaned


def parse_float_m2(s):
    # "229.7ã¡" -> 229.7
    return float(re.sub(r"[^\d\.]", "", s))


def lot_no_from_addr(addr):
    # "1. ì„œìš¸íŠ¹ë³„ì‹œ ..." -> 1
    m = re.match(r"\s*(\d+)\.", addr or "")
    return int(m.group(1)) if m else None


def compute_land_share_area(land_rows, ratio_rows):
    # land_rows: [{"ì†Œì¬ì§€ë²ˆ": "1. ...", "ë©´ì ": "229.7ã¡"}, ...]
    # ratio_rows: [{"í‘œì‹œë²ˆí˜¸ê·¸ë£¹":"1","ëŒ€ì§€ê¶Œë¹„ìœ¨":{"share":"0.01503..."}} ...]
    area_by_lot = {}
    for r in land_rows:
        lot = lot_no_from_addr(r.get("ì†Œì¬ì§€ë²ˆ"))
        if lot is None:
            continue
        area = parse_float_m2(r.get("ë©´ì ", "0"))
        area_by_lot[lot] = area

    total = 0.0
    parts = []
    for rr in ratio_rows:
        lot = None
        # í‘œì‹œë²ˆí˜¸ê·¸ë£¹ì´ ìˆ«ì ë¬¸ìì—´ë¡œ ë“¤ì–´ìˆë‹¤ê³  ê°€ì •
        g = rr.get("í‘œì‹œë²ˆí˜¸ê·¸ë£¹")
        if g and str(g).isdigit():
            lot = int(g)
        share_str = rr.get("ëŒ€ì§€ê¶Œë¹„ìœ¨", {}).get("share")
        if lot is None or not share_str:
            continue
        share = float(share_str)
        area = area_by_lot.get(lot)
        if area is None:
            continue
        part = area * share
        parts.append(part)
        total += part
    return total, parts


# ============ Flask ë¼ìš°íŠ¸ ============
@app.route("/analyze", methods=["POST"])
def analyze():
    # íŒŒì¼ ì²˜ë¦¬
    uploads = []
    if "files" in request.files:
        uploads = request.files.getlist("files")
    elif "file" in request.files:
        uploads = [request.files["file"]]
    if not uploads:
        return jsonify(ok=False, message="PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 400
    if len(uploads) > 1:
        return jsonify(ok=False, message="PDFëŠ” 1ê°œë§Œ ì—…ë¡œë“œí•˜ì„¸ìš”."), 400

    up = uploads[0]
    data = up.read() or b""
    if b"%PDF-" not in data[:1024] and b"%PDF-" not in data:
        return jsonify(ok=False, message="PDFë§Œ í—ˆìš©í•©ë‹ˆë‹¤."), 415

    try:
        doc = fitz.open(stream=data, filetype="pdf")
    except Exception as e:
        return jsonify(ok=False, message=f"PDF ì—´ê¸° ì‹¤íŒ¨: {e}"), 400

    try:
        land_share_info = extract_land_share_table(data)
        land_ratios_info = extract_land_right_ratios(data)

        total, parts = compute_land_share_area(land_share_info, land_ratios_info)
        land_share_area = round(total, 2)
        joint_info = extract_joint_collateral_addresses_follow(data)
        mortgage_info = extract_mortgage_info(data)
        print(mortgage_info)
        gabu_info = extract_gabu_info(data)
    except Exception as e:
        import traceback;
        traceback.print_exc()
        return jsonify(ok=False, message=f"ë¶„ì„ ì‹¤íŒ¨: {e}"), 500

    return jsonify(
        ok=True,
        pageCount=len(doc),
        jointCollateralPages=joint_info["pages"],
        jointCollateralAddresses=joint_info["addresses"],
        jointCollateralCurrentAddress=joint_info["currentAddress"],
        mortgageInfo=mortgage_info["mortgages"],
        mortgageRiskFlags=mortgage_info["riskFlags"],  # ê°€ì••ë¥˜/ì••ë¥˜/ê°€ì²˜ë¶„
        gabuInfo=gabu_info,
        landShareArea=land_share_area
    ), 200


# ì¢Œí‘œë¥¼ í†µí•œ ë„¤ì´ë²„ ë¶€ë™ì‚° ì‹¤ì‹œê°„ í¬ë¡¤ë§
# --- Playwright ì„¤ì • ---
USER_DATA_DIR = "./.chrome-profile"  # ì¿ í‚¤/ì§€ë¬¸ ìœ ì§€
HEADLESS = False  # ë¨¼ì € ì°½ ë„ì›Œì„œ í™•ì¸ í›„ Trueë¡œ ì „í™˜ ê°€ëŠ¥
LIST_SEL_CANDS = [
    "div.item_list.item_list--article",
    "div.item_list--article",
    "div.article_list",  # ì˜ˆë¹„
]
API_SOFT_WAIT_MS = 800  # 800~1500 ê¶Œì¥ (ë„ˆë¬´ ëŠë¦¬ë©´ 900ë¡œ)
MAX_CLICK = 20  # í´ë¦­í•  ì¹´ë“œ ìˆ˜ ìƒí•œ


def _pick_scroll_box(page) -> str:
    # scrollHeight > clientHeight ì¸ ì²« ìš”ì†Œ ì„ íƒ
    page.wait_for_selector(", ".join(LIST_SEL_CANDS), timeout=8000)
    for sel in LIST_SEL_CANDS:
        try:
            ok = page.evaluate("""(s) => {
                const el = document.querySelector(s);
                if (!el) return false;
                return el.scrollHeight > el.clientHeight;
            }""", sel)
            if ok:
                return sel
        except:
            pass
    return LIST_SEL_CANDS[0]


def scroll_list_to_bottom(page, pause_ms=700, max_stall=2):
    sel = _pick_scroll_box(page)
    stall = 0
    last_h = -1

    # ì»¨í…Œì´ë„ˆì— í¬ì»¤ìŠ¤(í‚¤ë³´ë“œ ìŠ¤í¬ë¡¤ ë°±ì—…ìš©)
    try:
        page.locator(sel).click(position={"x": 10, "y": 10}, timeout=1000)
    except:
        pass

    for _ in range(60):  # ì¶©ë¶„í•œ íšŸìˆ˜ ì‹œë„
        try:
            h = page.evaluate("""(s) => {
                const el = document.querySelector(s);
                if (!el) return 0;
                el.scrollTop = el.scrollHeight;
                return el.scrollHeight;
            }""", sel)
        except:
            # ë¦¬ë Œë” ëì„ ë•Œ ë‹¤ì‹œ ê³ ë¥´ê¸°
            sel = _pick_scroll_box(page)
            h = page.evaluate("(s) => document.querySelector(s)?.scrollHeight || 0", sel)

        page.wait_for_timeout(pause_ms)

        if h == last_h:
            stall += 1
            if stall >= max_stall:
                break
        else:
            stall = 0
            last_h = h

    # ìŠ¤í¬ë¡¤ í›„ ì¹´ë“œ ìˆ˜ì§‘
    return page.query_selector_all("div.item, div.item_list--article div.item")


def get_env_proxy():
    for k in ("HTTPS_PROXY", "https_proxy", "HTTP_PROXY", "http_proxy"):
        v = os.environ.get(k)
        if v: return {"server": v}
    return None


def _launch_ctx(p):
    proxy_cfg = get_env_proxy()
    ctx = p.chromium.launch_persistent_context(
        user_data_dir=USER_DATA_DIR,
        channel="chrome",
        headless=HEADLESS,
        locale="ko-KR",
        viewport={"width": 1280, "height": 860},
        proxy=proxy_cfg,
        ignore_https_errors=True,
        args=[
            "--disable-blink-features=AutomationControlled",
            "--no-first-run", "--no-default-browser-check",
            "--disable-dev-shm-use", "--no-sandbox",
            "--disable-gpu", "--window-size=1280,860",
        ],
    )
    # ì•½ì‹ ìŠ¤í…”ìŠ¤
    ctx.add_init_script("""
      Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
      Object.defineProperty(navigator, 'language', {get: () => 'ko-KR'});
      Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR','ko','en-US','en']});
      window.chrome = window.chrome || { runtime: {} };
    """)
    ctx.set_extra_http_headers({"Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"})
    return ctx


def _goto_offices(page, lat: float, lng: float):
    root = "https://new.land.naver.com"
    path = f"/offices?ms={lat},{lng},19&a=SG&e=RETAIL"

    # ê¸°ë³¸ ì§„ì…: ë°”ë¡œ ëª©ì ì§€ë¡œ ì´ë™ (referer ìœ ì§€)
    page.goto(root + path, referer=root, wait_until="domcontentloaded", timeout=60000)

    try:
        # âœ… 'load' ëŒ€ì‹  'domcontentloaded'ë¡œ ëŒ€ê¸°
        page.wait_for_url("**/offices**", wait_until="domcontentloaded", timeout=60000)
    except PWTimeout:
        # ë³´ì •: SPA íŠ¹ì„±ìƒ URLì€ ë°”ë€Œì—ˆëŠ”ë° 'load'ê°€ ì•ˆ ì˜¤ëŠ” ì¼€ì´ìŠ¤ ë°©ì§€
        page.wait_for_load_state("domcontentloaded", timeout=30000)

    # âœ… URLë§Œ ë¯¿ì§€ ë§ê³ , ì‹¤ì œ ëª©ë¡ ì»¨í…Œì´ë„ˆê°€ ë¶™ì„ ë•Œê¹Œì§€ í•œ ë²ˆ ë” ëŒ€ê¸°
    page.wait_for_selector(", ".join(LIST_SEL_CANDS), state="attached", timeout=20000)


DETAIL_PATTERNS = [
    re.compile(r"https://new\.land\.naver\.com/api/.*/articles/\d+"),
    re.compile(r"https://new\.land\.naver\.com/api/.*/article/\d+"),
    re.compile(r"https://new\.land\.naver\.com/api/.*/detail"),
]


def _soft_wait_detail(page: Page, q: deque, prev_len: int, wait_ms: int) -> Optional[Dict[str, Any]]:
    deadline = time.time() + wait_ms / 1000.0
    while time.time() < deadline:
        if len(q) > prev_len:
            return q[-1]  # ê°€ì¥ ìµœê·¼ ë„ì°©ë¶„
        page.wait_for_timeout(40)  # ì§§ê²Œ ì–‘ë³´
    return None


def _is_api(url: str) -> bool:
    return "https://new.land.naver.com/api/" in url


def _is_detail(url: str) -> bool:
    return _is_api(url) and any(p.search(url) for p in DETAIL_PATTERNS)


def crawl_viewport(lat: float, lng: float,
                   radius_m: int = 800,
                   category: str = "offices",
                   filters: Optional[Dict[str, Any]] = None,
                   limit_detail_fetch: Optional[int] = 60) -> Dict[str, Any]:
    with sync_playwright() as p:
        ctx = _launch_ctx(p)
        page = ctx.new_page()

        detail_hits: List[Dict[str, Any]] = []

        # ë””í…Œì¼ ì‘ë‹µ í (on_respê°€ ì—¬ê¸°ë¡œ push)
        api_detail_queue: deque = deque()

        def on_resp(resp):
            url = resp.url
            if _is_detail(url):
                try:
                    data = resp.json()
                except Exception:
                    data = None
                api_detail_queue.append({"url": url, "status": resp.status, "data": data})

        page.on("response", on_resp)

        # 1) /offices ì§„ì… & ëª©ë¡ ë¡œë“œ
        _goto_offices(page, lat, lng)
        page.wait_for_timeout(800)
        scroll_list_to_bottom(page, pause_ms=700, max_stall=2)

        # 2) ì¹´ë“œ ëª©ë¡ í™•ë³´
        page.wait_for_selector(
            "div.item_list--article div.item, div.item_list.item_list--article div.item",
            timeout=6000
        )
        cards = page.query_selector_all(
            "div.item_list--article div.item, div.item_list.item_list--article div.item"
        )

        # 3) í´ë¦­ â†’ (ì§§ê²Œ) API ì†Œí”„íŠ¸ ëŒ€ê¸° â†’ DOM íŒŒì‹±(í•­ìƒ)
        for i, c in enumerate(cards):  # âœ… ëª¨ë“  ì¹´ë“œ ìˆœíšŒ
            if limit_detail_fetch and len(detail_hits) >= int(limit_detail_fetch):
                break

            try:
                before_len = len(api_detail_queue)
                c.scroll_into_view_if_needed()
                c.click()
            except Exception:
                continue

            # APIëŠ” ìµœëŒ€ 1.2ì´ˆë§Œ ì†Œí”„íŠ¸ ëŒ€ê¸° (ì—†ì–´ë„ í†µê³¼)
            hit = _soft_wait_detail(page, api_detail_queue, before_len, API_SOFT_WAIT_MS)

            # ë””í…Œì¼ íŒ¨ë„ DOM íŒŒì‹± (í•­ìƒ ì‹œë„)
            page.wait_for_timeout(10)  # íŒ¨ë„ ë Œë” ì—¬ìœ 
            try:
                dom_data = scrape_detail_panel(page)  # ë˜ëŠ” scrape_detail_panel_raw(page)
            except Exception:
                dom_data = {}

            detail_hits.append({
                "url": (hit["url"] if hit else None),
                "status": (hit["status"] if hit else None),
                "json": (hit["data"] if hit else None),
                "dom": dom_data,
            })

        # ì •ë¦¬
        ctx.close()

        # ëª¨ë“  í•­ëª© ë°˜í™˜ (limit_detail_fetchê°€ Noneì´ë©´ ì „ë¶€)
        limit = int(limit_detail_fetch or len(detail_hits))
        return {
            "ok": True,
            "meta": {
                "lat": lat, "lng": lng,
                "category": category,
                "limit_detail_fetch": limit,
                "count_detail": len(detail_hits),
                "source": "playwright_softwait(dom+api)",
            },
            "items": detail_hits[:limit],  # limit=Noneì´ë©´ ì „ë¶€
        }


def wait_for_element(page: Page, selector: str, timeout: int = 6000) -> Locator:
    loc = page.locator(selector)
    loc.wait_for(state="visible", timeout=timeout)
    return loc


def wait_for_elements(page: Page, selector: str, min_count: int = 1, timeout: int = 6000):
    page.wait_for_selector(selector, state="attached", timeout=timeout)
    loc = page.locator(selector)
    # ê°œìˆ˜ ë§Œì¡±ê¹Œì§€ í´ë§
    with page.expect_event("load", timeout=200) if False else nullcontext():  # no-op
        end = page.context._impl_obj._loop.time() + (timeout / 1000)
        while True:
            if loc.count() >= min_count:
                break
            if page.context._impl_obj._loop.time() > end:
                raise PWTimeout(f"Timeout waiting for {min_count}+ elements: {selector}")
    return [loc.nth(i) for i in range(loc.count())]


def wait_for_child_element(parent: Locator, selector: str, timeout: int = 6000) -> Locator:
    child = parent.locator(selector)
    child.wait_for(state="visible", timeout=timeout)
    return child


def text_or_none(loc: Locator, timeout: int = 3000) -> Optional[str]:
    try:
        return loc.inner_text(timeout=timeout).strip()
    except Exception:
        return None


def get_by_header(page: Page, header_text: str) -> Optional[str]:
    """
    <tr class="info_table_item">
      <th>ê´€ë¦¬ë¹„</th><td>â€¦</td>
    </tr>
    ê°™ì€ êµ¬ì¡°ì—ì„œ í—¤ë” í…ìŠ¤íŠ¸ë¡œ ê°’ì„ ë½‘ì•„ì˜´
    """
    row = page.locator(
        "tr.info_table_item",
        has=page.locator("th", has_text=header_text)
    ).first

    if row.count() == 0:
        return None

    return text_or_none(row.locator("td").first)  # text_or_none: Optional[str] ë°˜í™˜


def parse_korean_price(s: str) -> Optional[int]:
    """í•œêµ­ì‹ ê¸ˆì•¡ ë¬¸ìì—´(ì˜ˆ: 1ì–µ400, 2,000, 3500)ì„ ë§Œì› ë‹¨ìœ„ ì •ìˆ˜ë¡œ ë³€í™˜"""
    if not s:
        return None

    s = s.replace(",", "").strip()
    total = 0

    # ì–µ ë‹¨ìœ„ (1ì–µ = 10000ë§Œì›)
    match = re.search(r"(\d+)ì–µ", s)
    if match:
        total += int(match.group(1)) * 10000
        s = s[match.end():]

    # ë‚¨ì€ ìˆ«ì (ë§Œì› ë‹¨ìœ„)
    digits = re.findall(r"\d+", s)
    if digits:
        total += int("".join(digits))

    return total if total > 0 else None


def parse_deposit_monthly(price_value: Optional[str]) -> Tuple[Optional[int], Optional[int]]:
    """'ë³´ì¦ê¸ˆ/ì›”ì„¸' ë¬¸ìì—´ì„ (ë³´ì¦ê¸ˆ, ì›”ì„¸) íŠœí”Œë¡œ ë³€í™˜ (ë§Œì› ë‹¨ìœ„)"""
    if not price_value:
        return None, None

    if "/" not in price_value:  # ì „ì„¸ or ë§¤ë§¤ (ë‹¨ì¼ ê¸ˆì•¡)
        return parse_korean_price(price_value), None

    a, b = price_value.split("/", 1)
    return parse_korean_price(a), parse_korean_price(b)


def scrape_detail_panel(page: Page) -> dict:
    detail = {}

    # ê°€ê²© ë°•ìŠ¤
    price_box = wait_for_element(page, "div.info_article_price")
    detail["price_type"] = text_or_none(price_box.locator(".type"))  # ì˜ˆ: "ì›”ì„¸"
    detail["price_value"] = text_or_none(price_box.locator(".price"))  # ì˜ˆ: "4,000/230"

    dep, mon = parse_deposit_monthly(detail["price_value"])
    detail["deposit"] = dep
    detail["monthly"] = mon

    # ìƒì„¸ í…Œì´ë¸”(í—¤ë” ê¸°ë°˜ ì¶”ì¶œ)
    # í˜ì´ì§€ë§ˆë‹¤ ë¼ë²¨ì´ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆì–´, í•„ìš”í•œ ê²ƒë§Œ ê³¨ë¼ í˜¸ì¶œí•˜ë©´ ë¨
    candidates = {
        "ê´€ë¦¬ë¹„": ["ê´€ë¦¬ë¹„"],
        "ì „ìš©ë©´ì ": ["ì „ìš©ë©´ì "],
        "ì¸µ": ["ì¸µ", "ì¸µìˆ˜"],
        "ì…ì£¼ê°€ëŠ¥ì¼": ["ì…ì£¼ê°€ëŠ¥ì¼", "ì…ì£¼ì¼"],
        "ì£¼ì°¨": ["ì£¼ì°¨"],
        "ë‚œë°©": ["ë‚œë°©"],
        "ìš©ë„": ["ìš©ë„", "ì£¼ìš©ë„"],
        "ì‚¬ìš©ìŠ¹ì¸ì¼": ["ì‚¬ìš©ìŠ¹ì¸ì¼", "ì¤€ê³µì¼", "ì‚¬ìš©ìŠ¹ì¸"],
        "í™”ì¥ì‹¤ ìˆ˜": ["í™”ì¥ì‹¤ ìˆ˜"],
    }

    for key, labels in candidates.items():
        val = None
        for lbl in labels:
            val = get_by_header(page, lbl)
            if val: break
        detail[key] = val

    # ì¤‘ê°œì—…ì†Œ(ìˆì„ ê²½ìš°)
    broker_box = page.locator("div.broker_info, div.article_broker_info").first
    if broker_box.count() > 0:
        detail["broker_name"] = text_or_none(broker_box.locator(".name, .broker_name"))
        detail["broker_ceo"] = text_or_none(broker_box.locator(".ceo, .broker_ceo"))
        detail["broker_address"] = text_or_none(broker_box.locator(".addr, .broker_address"))
        detail["broker_phone"] = text_or_none(broker_box.locator(".tel, .broker_phone"))

    print(detail)

    return detail


# â”€â”€ ã¡ íŒŒì‹±: ë¬¸ìì—´ì—ì„œ "â€¦ã¡" ê°’ë“¤ë§Œ ë½‘ì•„ 'ê°€ì¥ ì‘ì€ ã¡'ë¥¼ ì „ìš©ë©´ì ìœ¼ë¡œ ê°„ì£¼ â”€â”€
# ì˜ˆ) "50ã¡/38.6ã¡(ì „ìš©ë¥ 80%)" â†’ [50.0, 38.6] â†’ 38.6
def _extract_area_sqm(val: Optional[str]) -> Optional[float]:
    if val is None:
        return None
    s = str(val)
    matches = re.findall(r"([\d]+(?:\.\d+)?)\s*(?:ã¡|mÂ²)", s, flags=re.IGNORECASE)
    if matches:
        nums = [float(x) for x in matches]
        return min(nums)  # ë³´í†µ ì „ìš©(ã¡)ì´ ë” ì‘ìŒ
    # "14í‰" ê°™ì€ ê²½ìš°ë¥¼ ëŒ€ë¹„(ìˆë‹¤ë©´)
    m_py = re.search(r"([\d]+(?:\.\d+)?)\s*í‰", s)
    if m_py:
        return round(float(m_py.group(1)) * 3.305785, 3)
    return None


# â”€â”€ í•µì‹¬: ì›”ì„¸ë§Œ í•„í„° â†’ ë©´ì  ë½‘ê¸° â†’ êµ¬ê°„ë³„ í‰ê·  ì›”ì„¸ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def avg_monthly_by_area(detail_hits: List[Dict[str, Any]]) -> Dict[str, float]:
    per_sqm_vals: List[float] = []  # ë§Œì›/ã¡
    per_pyeong_vals: List[float] = []  # ë§Œì›/í‰

    for it in detail_hits:
        d = it.get("dom") or {}
        if not d:
            continue

        # ì›”ì„¸ë§Œ
        price_type = str(d.get("price_type") or "")
        if "ì›”ì„¸" not in price_type:
            continue

        # ì›”ì„¸ ê¸ˆì•¡
        monthly: Optional[int] = d.get("monthly")

        # ë©´ì : ì „ìš©ë©´ì  ìš°ì„ , ì—†ìœ¼ë©´ ê³µê¸‰ë©´ì  ì‹œë„
        area_str = d.get("ì „ìš©ë©´ì ")
        area = _extract_area_sqm(area_str)

        if monthly is None or area is None:
            continue

        per_sqm = monthly / area  # ë§Œì›/ã¡
        per_pyeong = monthly / (area / 3.305785)  # ë§Œì›/í‰

        per_sqm_vals.append(per_sqm)
        per_pyeong_vals.append(per_pyeong)

        def _avg(vs: List[float]) -> Optional[float]:
            return round(sum(vs) / len(vs), 3) if vs else None

    return {
        "count": len(per_sqm_vals),
        "avg_per_sqm_manwon": _avg(per_sqm_vals),  # ë§Œì›/ã¡
        "avg_per_pyeong_manwon": _avg(per_pyeong_vals),  # ë§Œì›/í‰
    }


# --- Flask ë¼ìš°íŠ¸ ---
@app.route("/crawl", methods=["POST"])
def api_crawl():
    payload = request.get_json(silent=True) or {}
    lat = float(payload.get("lat"))
    lng = float(payload.get("lng"))
    radius_m = payload.get("radius_m") or 800
    category = payload.get("category") or "offices"
    limit_detail_fetch = int(payload.get("limit_detail_fetch") or 60)

    print(f"[CRAWL IN] lat={lat}, lng={lng}, r={radius_m}, cat={category}, limit={limit_detail_fetch}")
    try:
        # â˜… í‚¤ì›Œë“œ ì¸ìë¡œ í˜¸ì¶œ (í¬ì§€ì…”ë„ë¡œ ë„˜ê¸°ë©´ filters ìë¦¬ì— ë°•í˜€ì„œ ì—‰ë§ë¨)
        res = crawl_viewport(
            lat, lng,
            radius_m=radius_m,
            category=category,
            limit_detail_fetch=limit_detail_fetch
        )

        stats = avg_monthly_by_area(res["items"])
        print("[ë©´ì  êµ¬ê°„ë³„ í‰ê·  ì›”ì„¸]", stats)

        print(f"[CRAWL OUT] count={res['meta'].get('count_detail', len(res.get('items', [])))}")
        return jsonify(res), 200
    except Exception as e:
        print(f"[CRAWL ERR] {e}")
        return jsonify(ok=False, error="crawl_failed", message=str(e)), 500


@app.route("/get_base_price", methods=["POST"])
def get_base_price():
    data = request.json
    emd_name = data["emd_name"]
    bunji = data["bunji"]
    ho = data["ho"]
    target_floor = data["floor"]
    target_ho = data["target_ho"]
    target_sido = data["sidoNm"]
    target_sgg = data["sggNm"]

    with sync_playwright() as p:
        browser = p.chromium.launch_persistent_context(
            user_data_dir=USER_DATA_DIR,
            channel="chrome",
            headless=False,  # ì„œë²„ì—ì„œëŠ” True ê¶Œì¥
            locale="ko-KR",
            viewport={"width": 1280, "height": 860},
            ignore_https_errors=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-first-run", "--no-default-browser-check",
                "--disable-dev-shm-use", "--no-sandbox",
            ],
        )
        page = browser.new_page()

        # debugger ë¬´ë ¥í™” ìŠ¤í¬ë¦½íŠ¸ ì£¼ì…
        page.add_init_script("""
            Object.defineProperty(window, 'debugger', { get: () => undefined });
            setInterval(() => { try { delete window.debugger; } catch(e) {} }, 500);
        """)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™ˆíƒìŠ¤ ì ‘ì† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        page.goto(
            "https://hometax.go.kr/websquare/websquare.html?"
            "w2xPath=/ui/pp/index_pp.xml&tmIdx=47&tm2lIdx=4712090000&tm3lIdx=4712090300"
        )

        # ë²•ì •ë™ ë²„íŠ¼ í´ë¦­
        btn = wait_for_element(page, "#mf_txppWframe_btnLdCdPop")
        btn.click()
        time.sleep(0.5)

        # í–‰ì •ë™ ì…ë ¥
        emd_input_box = wait_for_element(page, "#mf_txppWframe_UTECMAAA08_wframe_inputSchNm")
        emd_input_box.fill(emd_name)

        # ì¡°íšŒ ë²„íŠ¼ í´ë¦­
        btn = wait_for_element(page, "#mf_txppWframe_UTECMAAA08_wframe_trigger6")
        btn.click()
        time.sleep(1)

        # í…Œì´ë¸” í–‰ë“¤ (locator ë°©ì‹)
        rows = page.locator("#mf_txppWframe_UTECMAAA08_wframe_ldCdAdmDVOList_body_table tbody tr")
        row_count = rows.count()

        for i in range(row_count):
            row = rows.nth(i)
            cols = row.locator("td")
            first_col = cols.nth(0).text_content().strip()
            second_col = cols.nth(1).text_content().strip()

            if target_sido in first_col and target_sgg in second_col:
                btn = cols.nth(8).locator("button[title='ì„ íƒ']")
                btn.click(force=True)
                break

        # ë²ˆì§€ ì…ë ¥
        bunji_input = wait_for_element(page, "#mf_txppWframe_txtBunj")
        bunji_input.fill(bunji)

        # í˜¸ ì…ë ¥
        ho_input = wait_for_element(page, "#mf_txppWframe_txtHo")
        ho_input.fill(ho)

        # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
        search_button = wait_for_element(page, "#mf_txppWframe_group1962")
        search_button.click()

        # ê±´ë¬¼ëª… ì„ íƒ
        build_name_button = wait_for_element(page, "a#txtItm0")
        build_name_button.click()

        # ë™ select
        dong_select_box = page.locator("#mf_txppWframe_selBldComp")
        dong_select_box.select_option(index=1)

        # ì¸µ select
        floor_select_box = page.locator("#mf_txppWframe_selBldFlor")
        floor_select_box.select_option(label=target_floor)

        # í˜¸ select
        ho_select_box = page.locator("#mf_txppWframe_selBldHo")
        ho_select_box.select_option(label=target_ho)

        # ìƒì„¸ ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
        detail_search_button = wait_for_element(page, "#mf_txppWframe_btnSchTsv")
        detail_search_button.click()

        # ê¸°ì¤€ì‹œê°€ í…Œì´ë¸”
        rows2 = page.locator("#mf_txppWframe_grdCmrcBldTsvList_body_tbody tr")
        first_row = rows2.nth(0)

        td2 = first_row.locator("td").nth(1).text_content().strip()
        td3 = first_row.locator("td").nth(2).text_content().strip()

        val2 = float(td2.replace(",", ""))
        val3 = float(td3)
        result = val2 * val3
        final_price = int(val2 // 10000 * 10000)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í† ì§€ ê¸°ì¤€ì‹œê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        page2 = browser.new_page()
        page2.goto("https://www.realtyprice.kr/notice/gsindividual/search.htm")

        page2.locator("#sido_list").select_option(label=target_sido)
        page2.locator("#sgg_list").select_option(label=target_sgg)
        page2.locator("#eub_list").select_option(label=emd_name)

        container = page2.locator("div.search-opt3")
        container.locator("input[name='bun1']").fill(bunji)
        container.locator("input[name='bun2']").fill(ho)

        page2.click(".search-bt input[type='image']")
        page2.wait_for_selector("#dataList tr", timeout=10000)

        first_row2 = page2.locator("#dataList tr").first
        price_text = first_row2.locator("td").nth(3).text_content().strip()
        land_price = int(re.sub(r"[^0-9]", "", price_text))

        browser.close()

    return jsonify({
        "emd_name": emd_name,
        "bunji": bunji,
        "ho": ho,
        "floor": target_floor,
        "target_ho": target_ho,
        "build_base_price": final_price,
        "land_base_price": land_price
    })


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True, use_reloader=False, threaded=False)
