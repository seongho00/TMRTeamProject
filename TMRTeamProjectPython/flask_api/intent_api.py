from collections import deque
from contextlib import nullcontext

from flask import Flask, request, jsonify, Response
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import pickle
import mecab_ko
import pymysql
import pymysql.cursors
import io, os, uuid, tempfile
import re, time, random, hashlib
from collections import deque
import io
import pdfplumber
import fitz  # â† PyMuPDF
from playwright.sync_api import Page, Locator, TimeoutError as PWTimeout
# HEIC ì§€ì› (ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ìë™ ë“±ë¡)
try:
    import pillow_heif

    pillow_heif.register_heif_opener()
except Exception:
    pass

# í¬ë¡¤ë§ ê´€ë ¨ import
from typing import Optional, Collection, List, Dict, Tuple, Any, Set
from playwright.sync_api import sync_playwright, TimeoutError as PwTimeout

# ì—…ì¢… ê²€ìƒ‰ ê´€ë ¨ import
import re
import unicodedata

try:
    from rapidfuzz import process, fuzz
    HAVE_RAPIDFUZZ = True
except Exception:
    HAVE_RAPIDFUZZ = False

# âœ… ì˜ˆì¸¡ í•¨ìˆ˜
def predict_intent(text, threshold=0.1):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=32)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=1)[0]
        confidence, predicted_class_id = torch.max(probs, dim=0)

    confidence = confidence.item()
    class_idx = int(predicted_class_id.item())

    if confidence < threshold:
        return "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.", confidence

    return class_idx, confidence


def extract_location(text):
    valid_city_map = {
        'ëŒ€ì „': ['ì„œêµ¬', 'ìœ ì„±êµ¬', 'ëŒ€ë•êµ¬', 'ë™êµ¬', 'ì¤‘êµ¬']
    }
    # ì‹œë„ + ì‹œêµ°êµ¬ + ìë©´ë™ê¹Œì§€ ì¶”ì¶œ
    pattern = r"(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)[\s]*(?:íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|ë„)?[\s]*" \
              r"([ê°€-í£]+[ì‹œêµ°êµ¬])?[\s]*([ê°€-í£]+[ë™ë©´ì])?"

    match = re.search(pattern, text)
    if match:
        sido, sigungu, eupmyeondong = match.groups()
        if sigungu and sigungu not in valid_city_map.get(sido, []):
            return "âŒ í–‰ì •êµ¬ì—­ í˜•ì‹ì— ë§ì§€ ì•ŠìŒ"  # âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì¡°í•©
        parts = [sido, sigungu, eupmyeondong]
        return " ".join(p for p in parts if p)

    return None


# âœ… ì˜ë„ + ì§€ì—­ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
def generate_response(user_input):
    intent, confidence = predict_intent(user_input)
    location = extract_location(user_input)

    if intent == 0:
        if location:
            return f"âœ… '{location}'ì˜ ë§¤ì¶œ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
        else:
            return "âš ï¸ ë§¤ì¶œ ì •ë³´ë¥¼ ì¡°íšŒí•˜ë ¤ë©´ ì§€ì—­ëª…ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    elif intent == 1:
        if location:
            return f"ğŸ“Š '{location}'ì˜ ì¸êµ¬ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
        else:
            return "âš ï¸ ì¸êµ¬ ì •ë³´ë¥¼ ì¡°íšŒí•˜ë ¤ë©´ ì§€ì—­ëª…ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    elif intent == 2:
        return "ğŸš¨ íì—… ìœ„í—˜ë„ ë†’ì€ ìƒê¶Œì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."

    else:
        return "â“ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."


# âœ… ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_nouns(text):
    parsed = tagger.parse(text)
    lines = parsed.split('\n')
    nouns = []

    for line in lines:
        if line == 'EOS' or line == '':
            continue
        word, feature = line.split('\t')
        pos = feature.split(',')[0]

        if pos in ['NNG', 'NNP']:  # ì¼ë°˜ëª…ì‚¬ or ê³ ìœ ëª…ì‚¬
            nouns.append(word)

    return nouns

# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ (DBì—ì„œ í–‰ì •ë™ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
def extract_emd_nm_list():
    conn = pymysql.connect(
        host='localhost',
        user='root',
        password='',
        db='TMRTeamProject',
        charset='utf8'
    )
    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT DISTINCT emd_nm FROM admin_dong")
            result = cursor.fetchall()
            if result:
                return [row[0] for row in result]
            else:
                return []  # âœ… ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë„ ë¦¬í„´
    except Exception as e:
        print("âŒ emd_nm ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨:", e)
        return []  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸
    finally:
        conn.close()


# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ (DBì—ì„œ ì—…ì¢… ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
def extract_upjong_code_map():
    conn = pymysql.connect(
        host='localhost',
        user='root',
        password='',
        db='TMRTeamProject',
        charset='utf8mb4',
        cursorclass=pymysql.cursors.DictCursor
    )
    try:
        with conn.cursor() as cursor:
            sql = """
                SELECT DISTINCT upjong_cd, upjong_nm
                FROM upjong_code
                WHERE upjong_cd IS NOT NULL AND upjong_nm IS NOT NULL
                ORDER BY upjong_nm
            """
            cursor.execute(sql)
            rows = cursor.fetchall()
            name_to_code = {r['upjong_nm']: r['upjong_cd'] for r in rows}

            print(name_to_code)

            return name_to_code
    except Exception as e:
        print("âŒ ì—…ì¢… ì½”ë“œ ë¡œë”© ì‹¤íŒ¨:", e)
        return {}
    finally:
        conn.close()

def _normalize(s: str) -> str:
    s = unicodedata.normalize("NFC", s)
    # í™˜ê²½ì— ë”°ë¼ ì•„ë˜ ë‘˜ ì¤‘ í•˜ë‚˜ ì‚¬ìš©
    s = re.sub(r'[^0-9A-Za-z\u3131-\u318E\uAC00-\uD7A3]+', '', s)  # ê¶Œì¥ ëŒ€ì•ˆ
    # s = re.sub(r'[\s\p{P}\p{S}]+', '', s)
    return s

def find_upjong_pre_morph_from_map(
        user_input: str,
        name_to_code: dict[str, str],   # {"ì¤‘êµ­ìŒì‹ì ":"CS1001", ...}
        fuzzy_threshold: int = 60,
        max_window_len: int = 12
):
    """
    ë°˜í™˜: (raw_phrase, mapped_name, mapped_code, score, method)
    ì‹¤íŒ¨: (None, None, None, None, None)
    """
    text_norm = _normalize(user_input)
    if not text_norm:
        return (None, None, None, None, None)

    # ì •ê·œí™” ì¸ë±ìŠ¤
    idx = { _normalize(nm): (nm, code) for nm, code in name_to_code.items() }

    # 1) ê³µë°±ë¬´ì‹œ ë¶€ë¶„ì¼ì¹˜(ê°€ì¥ ê¸´ ë§¤ì¹­ ìš°ì„ )
    best = None
    for nm_norm, (nm, cd) in idx.items():
        if nm_norm in text_norm:
            cand = (nm, nm, cd, 100, "substring")
            if not best or len(nm) > len(best[0]):
                best = cand
    if best:
        return best

    # 2) í¼ì§€ ë§¤ì¹­(ì˜µì…˜)
    if HAVE_RAPIDFUZZ:
        keys = list(idx.keys())
        BEST = (None, None, None, -1, "fuzzy")
        Lmax = min(max_window_len, max((len(k) for k in keys), default=0))
        for L in range(2, Lmax + 1):
            for i in range(0, max(0, len(text_norm) - L + 1)):
                sub = text_norm[i:i+L]
                m = process.extractOne(sub, keys, scorer=fuzz.ratio)
                if m and m[1] > BEST[3]:
                    nm, cd = idx[m[0]]
                    BEST = (sub, nm, cd, m[1], "fuzzy")
        if BEST[3] >= fuzzy_threshold:
            return BEST

    return (None, None, None, None, None)

# âœ… ì˜ë¯¸ ë¶„ì„ í•¨ìˆ˜
def analyze_input(user_input, intent, valid_emd_list):


    entities = {
        "sido": None,
        "sigungu": None,
        "emd_nm": None,
        "gender": None,
        "age_group": None,
        "upjong_cd": None,
        "raw_upjong": None,  # ë§¤ì¹­ ì•ˆ ë˜ë©´ ì›ë¬¸ ë³´ê´€
    }

    # ì—…ì¢… ì„ ë§¤í•‘ (ë™ì˜ì–´ ì—†ì´)
    raw, nm, cd, score, method = find_upjong_pre_morph_from_map(
        user_input, upjong_keywords, fuzzy_threshold=80
    )
    if cd:
        entities["raw_upjong"] = raw or nm
        entities["upjong_nm"]  = nm
        entities["upjong_cd"]  = cd

    tokens = extract_nouns_with_age_merge(user_input)
    print("ì¶”ì¶œëœ ëª…ì‚¬:", tokens)

    for t in tokens:
        # ì‹œ/ë„
        if t in valid_sido:
            entities["sido"] = t
        # ì‹œ/êµ°/êµ¬
        if t in valid_sigungu:
            entities["sigungu"] = t
        # í–‰ì •ë™
        if t in valid_emd_list:
            entities["emd_nm"] = t
        # ì„±ë³„
        if t in gender_keywords:
            entities["gender"] = gender_keywords[t]
        # ì—°ë ¹
        if t in age_keywords:
            entities["age_group"] = age_keywords[t]
        # ì—…ì¢…
        if entities["upjong_cd"] is None:
            if t in upjong_keywords:  # ê¸°ì¡´ í‚¤ì›Œë“œ ì‚¬ì „ (ê°„ë‹¨ ë§¤í•‘ìš©)
                entities["upjong_cd"] = upjong_keywords[t]
                entities["upjong_nm"] = t
                entities["raw_upjong"] = t
            else:
                # ì¼ìƒì–´ íŒíŠ¸ë§Œ ê¸°ë¡
                if t in ("ì¹´í˜", "í¸ì˜ì ", "ë¶„ì‹", "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ", "ì˜ë¥˜", "ì¤‘êµ­ì§‘", "ì¹˜í‚¨ì§‘"):
                    entities["raw_upjong"] = t

    # ì§€ì—­ ìµœì†Œ ë‹¨ìœ„ íŒì •(í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ OK)
    has_region = bool(entities["emd_nm"] or entities["sigungu"] or entities["sido"])

    # ëˆ„ë½ í•­ëª© ê³„ì‚°
    required = INTENT_REQUIRED.get(intent, {"need": [], "optional": []})
    missing = []

    if "sido_or_sigungu_or_emd" in required["need"] and not has_region:
        missing.append("region")  # ì§€ì—­(ì‹œ/êµ¬/ë™ ì¤‘ í•˜ë‚˜)

    # ë‹¤ë¥¸ ì˜ë¬´ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ì¡°ê±´ ì¶”ê°€(ì˜ˆ: intentë³„ í•„ìˆ˜ ì„±ë³„ ë“±)
    print(entities)
    return entities, missing


# ìˆ«ì í¬í•¨ ë¶„ì„
def extract_nouns_with_age_merge(text):
    parsed = tagger.parse(text)
    lines = parsed.split('\n')

    result = []
    i = 0
    while i < len(lines) - 1:  # ë§ˆì§€ë§‰ 'EOS' ì œì™¸
        line = lines[i]
        if line == '' or line == 'EOS':
            i += 1
            continue

        word, feature = line.split('\t')
        features = feature.split(',')

        # í˜„ì¬ í’ˆì‚¬
        current_tag = features[0]

        # ìˆ«ì + ëŒ€ ì¡°í•©ì´ë©´ ë³‘í•©
        if current_tag == 'SN' and i + 1 < len(lines):
            next_line = lines[i + 1]
            if '\t' not in next_line:
                i += 1
                continue
            next_word, next_feature = next_line.split('\t')
            next_tag = next_feature.split(',')[0]
            if next_tag == 'NNBC' and next_word == 'ëŒ€':
                result.append(word + next_word)  # ì˜ˆ: "20ëŒ€"
                i += 2
                continue

        # ëª…ì‚¬ë¥˜ë§Œ í•„í„°ë§ (ì§€ëª…/ì¼ë°˜ëª…ì‚¬ ë“±)
        if current_tag in ['NNG', 'NNP']:
            result.append(word)

        i += 1

    return result


# ì„œë²„ ì‹œì‘
tagger = mecab_ko.Tagger()

app = Flask(__name__)

# âœ… intent label ë³µì› (ë¼ë²¨ ì¸ì½”ë”ë¡œ)
with open("label_encoder.pickle", "rb") as f:
    label_encoder = pickle.load(f)
intent_labels = list(label_encoder.classes_)

# âœ… HuggingFace BERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_PATH = "./intent_bert_model"
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
model.eval()

# ì•± ì „ì²´ì—ì„œ ì‚¬ìš©í•  ì „ì—­ ë¦¬ìŠ¤íŠ¸
valid_sido = {'ì„œìš¸'}

# ì‹œêµ°êµ¬ ëª©ë¡
valid_sigungu = {
    'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ìš©ì‚°êµ¬', 'ì„±ë™êµ¬', 'ê´‘ì§„êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ì¤‘ë‘êµ¬',
    'ì„±ë¶êµ¬', 'ê°•ë¶êµ¬', 'ë„ë´‰êµ¬', 'ë…¸ì›êµ¬', 'ì€í‰êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ë§ˆí¬êµ¬',
    'ì–‘ì²œêµ¬', 'ê°•ì„œêµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'ë™ì‘êµ¬', 'ê´€ì•…êµ¬',
    'ì„œì´ˆêµ¬', 'ê°•ë‚¨êµ¬', 'ì†¡íŒŒêµ¬', 'ê°•ë™êµ¬'
}

gender_keywords = {
    "ë‚¨ì": "male", "ë‚¨ì„±": "male",
    "ì—¬ì": "female", "ì—¬ì„±": "female"
}

age_keywords = {
    "10ëŒ€": "age_10",
    "20ëŒ€": "age_20",
    "30ëŒ€": "age_30",
    "40ëŒ€": "age_40",
    "50ëŒ€": "age_50",
    "60ëŒ€": "age_60"
}

valid_emd_list = extract_emd_nm_list()
upjong_keywords = extract_upjong_code_map()
# ì˜ë„ë³„ ìš”êµ¬ íŒŒë¼ë¯¸í„° ì •ì˜
# intent: 0=ë§¤ì¶œ, 1=ìœ ë™ì¸êµ¬, 2=ìœ„í—˜ë„, 3=ì²­ì•½(ì˜ˆì‹œ)
INTENT_REQUIRED = {
    0: {"need": ["sido_or_sigungu_or_emd", "upjong_cd"], "optional": []},  # ë§¤ì¶œ ì¡°íšŒ: ì§€ì—­ì€ ìµœì†Œ 1ë‹¨ê³„, ì—…ì¢…ì€ ì„ íƒ
    1: {"need": ["sido_or_sigungu_or_emd"], "optional": ["gender", "age_group"]},  # ìœ ë™ì¸êµ¬: ì§€ì—­ í•„ìˆ˜, ì„±ë³„/ì—°ë ¹ ì„ íƒ
    2: {"need": ["sido_or_sigungu_or_emd"], "optional": ["upjong_cd"]},  # ìœ„í—˜ë„: ì§€ì—­ í•„ìˆ˜
    3: {"need": ["sido_or_sigungu_or_emd"], "optional": []},  # ì²­ì•½: ì§€ì—­ í•„ìˆ˜(í•„ìš”ì— ë§ê²Œ ìˆ˜ì •)
}



# âœ… API ë¼ìš°íŒ…
@app.route("/predict", methods=["GET"])
def predict():
    question = request.args.get("text", "").strip()

    if not question:
        return jsonify({"error": "text íŒŒë¼ë¯¸í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400

    # ì˜ˆì¸¡
    intent_id, confidence = predict_intent(question)

    # 2) íŒŒë¼ë¯¸í„° ë¶„ì„
    entities, missing = analyze_input(question, intent_id, valid_emd_list)

    # 3) ëˆ„ë½ ì•ˆë‚´
    if missing:
        parts = []
        if "region" in missing: parts.append("ì§€ì—­(ì‹œ/êµ¬/ë™)")
        if "upjong_cd" in missing:
            hint = f" (ì¸ì‹: {entities['raw_upjong']})" if entities.get("raw_upjong") else " (ì˜ˆ: ì¹´í˜, í¸ì˜ì )"
            parts.append("ì—…ì¢…" + hint)
        return jsonify({
            "intent": intent_id,
            "confidence": round(confidence, 4),
            "need_more": missing,
            "entities": entities,
            "message": " / ".join(parts) + " ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
        }), 200

    if intent_id == 0:  # ë§¤ì¶œ
        return jsonify({
            "intent": 0,
            "confidence": round(confidence, 4),
            "entities": entities,   # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200

    elif intent_id == 1:  # ìœ ë™ì¸êµ¬
        return jsonify({
            "intent": 1,
            "confidence": round(confidence, 4),
            "entities": entities,   # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200


    elif intent_id == 2:  # ìœ„í—˜ë„
        return jsonify({
            "intent": 2,
            "confidence": round(confidence, 4),
            "entities": entities,   # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200

    else:  # intent_id == 3 ë“±
        return jsonify({
            "intent": int(intent_id),
            "confidence": round(confidence, 4),
            "entities": entities,   # â† ì§€ì—­, ì—…ì¢…, ì„±ë³„, ì—°ë ¹ ë“± ì „ì²´ ì „ë‹¬
            "data": "...ë§¤ì¶œì¡°íšŒê²°ê³¼..."
        }), 200

_BRACKETS = re.compile(r"\[[^\]]+\]")
_CANCEL_RX = re.compile(r"(ê¸°?ë§ì†Œ|í•´ì§€)")
_HANGUL_OR_NUM = re.compile(r"[ê°€-í£0-9]")

def _norm_ws(s: str) -> str:
    return " ".join((s or "").replace("\u200b", "").split()).strip()

def _stable_unique(seq):
    seen, out = set(), []
    for x in seq:
        if x not in seen:
            seen.add(x); out.append(x)
    return out

_WS = re.compile(r"\s+")
_MONEY = re.compile(r"ê¸ˆ?\s*([0-9,]+)\s*ì›")

# 'ë§ì†Œ/í•´ì§€' í”Œë˜ê·¸ë§Œ í™•ì¸
_CANCEL_FLAG_RX   = re.compile(r"(ë§ì†Œ|í•´ì§€)")
# ë³´ì¡°: purpose í…ìŠ¤íŠ¸ì—ì„œ "1ë²ˆ ê·¼ì €ë‹¹ê¶Œ ì„¤ì •" ê°™ì€ í‘œê¸°ë¥¼ íŒŒì‹± (ìˆœìœ„ë²ˆí˜¸ ì¹¸ì´ ë¹„ì—ˆì„ ë•Œë§Œ ì‚¬ìš©)
_CANCEL_TARGET_RX = re.compile(r"(\d+)\s*ë²ˆ")
# ì·¨ì†Œ ëŒ€ìƒ ë²ˆí˜¸ë“¤: "ì œ1ë²ˆ", "1ë²ˆ", "1,2ë²ˆ", "1ë²ˆÂ·2ë²ˆ" ë“±ì—ì„œ ëª¨ë‘ ì¶”ì¶œ
_RANK_LIST_RX = re.compile(r"(?:ì œ?\s*)(\d+)\s*ë²ˆ")

_RISK_FLAG_RX = re.compile(r"(ê°€ì••ë¥˜|ê°€ì²˜ë¶„|ì••ë¥˜)")

def _one_line(s: str) -> str:
    return _WS.sub(" ", (s or "").strip())

def _parse_amount_num(text: str):
    m = _MONEY.search(text or "")
    if not m:
        return None
    try:
        return int(m.group(1).replace(",", ""))
    except:
        return None

def extract_mortgage_info(pdf_bytes: bytes):
    """
    ã€ì„êµ¬ã€‘ì—ì„œ ê·¼ì €ë‹¹ê¶Œ 'ì„¤ì •'ë§Œ ìˆ˜ì§‘í•˜ê³ ,
    'ë§ì†Œ/í•´ì§€' í–‰ì€ 'í•´ë‹¹ í–‰ì˜ ìˆœìœ„ë²ˆí˜¸'ë¥¼ ì·¨ì†Œ ëŒ€ìƒìœ¼ë¡œ ê°„ì£¼.
    ë°˜í™˜: [{"rankNo": int, "amountKRW": int|None, "status": "normal|cancelled"} ...]
    """
    title_rx = re.compile(r"ã€\s*ì„\s*êµ¬\s*ã€‘")

    mortgages_by_rank = {}   # rankNo -> {"rankNo", "amountKRW", "status"}
    cancel_ranks = set()     # ë§ì†Œ/í•´ì§€ë¡œ ì§€ëª©ëœ ìˆœìœ„ë²ˆí˜¸ë“¤
    risks = []

    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for page in pdf.pages:
            txt = page.extract_text() or ""
            if not title_rx.search(txt):
                continue

            tables = page.extract_tables() or []
            for tb in tables:
                if not tb:
                    continue

                # í—¤ë” ìœ„ì¹˜ ì¶”ì •
                col_rank = col_purpose = col_amount = None
                for row in tb[:5]:
                    cells = [(_one_line(c) if c else "") for c in (row or [])]
                    for i, c in enumerate(cells):
                        if col_rank is None    and (c in ("ìˆœìœ„ë²ˆí˜¸","ìˆœìœ„")):    col_rank = i
                        if col_purpose is None and ("ë“±ê¸°" in c and "ëª©ì " in c):  col_purpose = i
                        if col_amount is None  and ("ì±„ê¶Œìµœê³ ì•¡" in c):            col_amount = i
                if col_rank   is None: col_rank = 0
                if col_purpose is None: col_purpose = 1
                if col_amount is None:  col_amount = 4

                for row in tb:
                    if not row or len(row) <= max(col_rank, col_purpose, col_amount):
                        continue

                    rank_raw   = _one_line(row[col_rank])
                    purpose    = _one_line(row[col_purpose])
                    amount_txt = _one_line(row[col_amount])

                    # --- 1) ë§ì†Œ/í•´ì§€ í–‰: í…ìŠ¤íŠ¸ ì•ˆì˜ "â€¦ë²ˆ"ë“¤ë§Œ ì·¨ì†Œ ëŒ€ìƒìœ¼ë¡œ ìˆ˜ì§‘ ---
                    if _CANCEL_FLAG_RX.search(purpose):
                        nums = _RANK_LIST_RX.findall(purpose)  # ["1","2",...]
                        for n in nums:
                            cancel_ranks.add(int(n))
                        continue  # ë§ì†Œí–‰ ìì²´ëŠ” ìˆ˜ì§‘ ì•ˆ í•¨

                    # --- ê·¼ì €ë‹¹ê¶Œ 'ì„¤ì •'ë§Œ ìˆ˜ì§‘ ---
                    if ("ê·¼ì €ë‹¹ê¶Œ" in purpose) and ("ì„¤ì •" in purpose):
                        # ìˆœìœ„ë²ˆí˜¸ ì—´ì´ ìˆ«ìì—¬ì•¼ ì„¤ì •í–‰ìœ¼ë¡œ ì¸ì •
                        if rank_raw.isdigit():
                            row_rank = int(rank_raw)
                            mortgages_by_rank[row_rank] = {
                                "rankNo": row_rank,
                                "amountKRW": _parse_amount_num(amount_txt),
                                "status": "normal",
                            }

                    if _RISK_FLAG_RX.search(purpose):
                        risks.append({
                            "rankNo": int(rank_raw) if rank_raw.isdigit() else None,
                            "purpose": purpose
                        })

    # ë§ì†Œ ëŒ€ìƒ ìˆœìœ„ë¥¼ ì·¨ì†Œ ì²˜ë¦¬
    for r in cancel_ranks:
        if r in mortgages_by_rank:
            mortgages_by_rank[r]["status"] = "cancelled"

    # ì„¤ì •í–‰ë§Œ ì •ë ¬í•´ì„œ ë°˜í™˜
    return {
        "mortgages": sorted(mortgages_by_rank.values(), key=lambda x: x["rankNo"]),
        "riskFlags": risks
    }
def extract_joint_collateral_addresses_follow(
        pdf_bytes: bytes,
        must_have_bracket: bool = True,
        max_follow: int = 6,
):
    """
    ã€ê³µë™ë‹´ë³´ëª©ë¡ã€‘ì—ì„œ 'ë¶€ë™ì‚°ì— ê´€í•œ ê¶Œë¦¬ì˜ í‘œì‹œ' ì£¼ì†Œë§Œ ì¶”ì¶œ. + [ì§‘í•©ê±´ë¬¼] í˜„ì¬ì£¼ì†Œ ì¶”ê°€
    - ì²« í˜ì´ì§€: í—¤ë”(ë¶€ë™ì‚°/í‘œì‹œ) ë°˜ë“œì‹œ ì°¾ê³  ê·¸ ì—´ë§Œ ì‚¬ìš©
    - ì´ì–´ì§€ëŠ” í˜ì´ì§€: í—¤ë”ê°€ ì—†ì–´ë„ ì£¼ì†Œì—´ì„ 'í…ìŠ¤íŠ¸ëŸ‰/í•œê¸€+ìˆ«ìëŸ‰' ìŠ¤ì½”ì–´ë¡œ ì¶”ì •
    - ê°™ì€ í–‰ì— 'í•´ì§€/ë§ì†Œ/ê¸°ë§ì†Œ' í¬í•¨ë˜ë©´ ì œì™¸
    """

    # [ì§‘í•©ê±´ë¬¼] ë’¤ ì£¼ì†Œ ì¶”ì¶œ
    def _extract_current_address(pdf):
        for page in pdf.pages:
            txt = page.extract_text() or ""
            m = re.search(r"\[ì§‘í•©ê±´ë¬¼\]\s*(.+)", txt)
            if m:
                return _norm_ws(m.group(1))
        return None

    def _find_header_top(page):
        txt = page.extract_text() or ""
        if not re.search(r"ã€\s*ê³µë™ë‹´ë³´ëª©ë¡\s*ã€‘", txt):
            return False, 0.0
        words = page.extract_words(extra_attrs=["top","text"]) or []
        tops = [w["top"] for w in words if "ê³µë™ë‹´ë³´ëª©ë¡" in (w.get("text") or "")]
        return True, min(tops) if tops else 0.0

    def _extract_tables_on(page, crop_bbox=None):
        tgt = page.within_bbox(crop_bbox) if crop_bbox else page
        tables = []
        try:
            tables = tgt.extract_tables(table_settings={
                "vertical_strategy": "lines",
                "horizontal_strategy": "lines",
                "intersection_x_tolerance": 3,
                "intersection_y_tolerance": 3,
            }) or []
        except Exception:
            pass
        if not tables:
            try:
                tables = tgt.extract_tables() or []
            except Exception:
                tables = []
        return tables

    def _guess_addr_col(tb):
        """í—¤ë”ê°€ ì—†ì„ ë•Œ ì£¼ì†Œì—´ ì¶”ì •: (í•œê¸€+ìˆ«ì ê°œìˆ˜ ìŠ¤ì½”ì–´ + ì „ì²´ ê¸¸ì´ ìŠ¤ì½”ì–´) ìµœëŒ€"""
        if not tb or not tb[0]:
            return None
        cols = max(len(r or []) for r in tb)
        def col_score(i):
            kor_num = 0
            total = 0
            for r in tb[1:]:
                cell = (r[i] if i < len(r) else "") or ""
                t = _norm_ws(str(cell))
                kor_num += len(_HANGUL_OR_NUM.findall(t))
                total += len(t)
            # í•œê¸€/ìˆ«ì ë¹„ì¤‘ì„ ë” ê°€ì¤‘
            return kor_num * 2 + total
        scores = [col_score(i) for i in range(cols)]
        return max(range(cols), key=lambda i: scores[i])

    def _pick_addresses_from_tables(tables, require_header: bool, prev_last=None):
        hits = []

        for tb in tables:
            if not tb or not tb[0]:
                continue

            addr_idx = None
            header_row_idx = 0

            if require_header:
                for r_idx, row in enumerate(tb[:5]):
                    header = [(_ or "").strip() for _ in (row or [])]
                    header_txt = " ".join(header)
                    if "ë¶€ë™ì‚°" in header_txt and "í‘œì‹œ" in header_txt:
                        header_row_idx = r_idx
                        try:
                            addr_idx = next(i for i, h in enumerate(header) if ("ë¶€ë™ì‚°" in h and "í‘œì‹œ" in h))
                        except StopIteration:
                            addr_idx = None
                        break
                if addr_idx is None:
                    continue
            else:
                addr_idx = _guess_addr_col(tb)
                if addr_idx is None:
                    continue

            # í–‰ ìŠ¤ìº” (hitsë§Œ ì‚¬ìš©)
            for r in tb[header_row_idx+1:]:
                serial = (r[0] or "").strip() if r else ""
                row_text = " ".join((c or "") for c in r)

                cell = r[addr_idx] if addr_idx < len(r) else None
                if not cell or not str(cell).strip():
                    continue
                cleaned = _norm_ws(_BRACKETS.sub(" ", str(cell)))
                if not cleaned:
                    continue

                # ë¹ˆ ì¼ë ¨ë²ˆí˜¸: ì§ì „ í•­ëª©ì— merge
                if serial == "":
                    target = hits[-1] if hits else prev_last
                    if target is not None:
                        target["address"] += " " + cleaned
                        if _CANCEL_RX.search(row_text):
                            target["status"] = "cancelled"
                    # ë¶™ì¼ ëŒ€ìƒì´ ì „í˜€ ì—†ìœ¼ë©´ ê³ ì•„ ë¼ì¸ â†’ ìŠ¤í‚µ
                    continue

                if not serial.isdigit():
                    continue

                status = "cancelled" if _CANCEL_RX.search(row_text) else "normal"
                hits.append({"serial": serial, "address": cleaned, "status": status})

        return hits



    pages_hit, addresses = [], []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:

        # í˜„ì¬ì£¼ì†Œ ë¨¼ì € ë½‘ê¸°
        current_addr = _extract_current_address(pdf)

        i = 0
        while i < len(pdf.pages):
            page = pdf.pages[i]

            found, header_top = _find_header_top(page)
            if must_have_bracket and not found:
                i += 1
                continue

            crop = (0, header_top, page.width, page.height)

            # ì‹œì‘ í˜ì´ì§€
            prev_last = addresses[-1] if addresses else None
            addrs = _pick_addresses_from_tables(_extract_tables_on(page, crop), require_header=True, prev_last=prev_last)
            if addrs:
                pages_hit.append(i + 1)
                addresses.extend(addrs)

            # ì´ì–´ì§€ëŠ” í˜ì´ì§€
            follow, j = 0, i + 1
            while j < len(pdf.pages) and follow < max_follow:
                tables_j = _extract_tables_on(pdf.pages[j])
                if not tables_j:
                    break

                prev_last = addresses[-1] if addresses else None
                more = _pick_addresses_from_tables(tables_j, require_header=False, prev_last=prev_last)
                if more:
                    pages_hit.append(j + 1)
                    addresses.extend(more)

                follow += 1
                j += 1

            i += 1

    # ì—¬ê¸°ë¶€í„°ëŠ” addressesê°€ [ {serial, address, status}, ... ]
    return {"pages": sorted(set(pages_hit)), "addresses": addresses, "currentAddress": current_addr}

_OWNER_CHANGE_RX = re.compile(r"ì†Œìœ ê¶Œ.*ì´ì „")
_CO_OWNER_RX     = re.compile(r"ê³µìœ ")

# ê°‘êµ¬ ë¶„ì„
def extract_gabu_info(pdf_bytes: bytes):
    """
    ã€ê°‘êµ¬ã€‘ì—ì„œ ì†Œìœ ê¶Œ ë³€ê²½ ì´ë ¥, ê³µë™ì†Œìœ  ì—¬ë¶€ ì¶”ì¶œ
    ë°˜í™˜:
    {
        "ownerChangeCount": int,
        "ownerChangeDetails": [ ... ],
        "coOwners": bool
    }
    """
    title_rx = re.compile(r"ã€\s*ê°‘\s*êµ¬\s*ã€‘")
    owner_changes = []
    co_owners = False

    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for page in pdf.pages:
            txt = page.extract_text() or ""
            if not title_rx.search(txt):
                continue

            tables = page.extract_tables() or []
            for tb in tables:
                if not tb:
                    continue

                for row in tb:
                    cells = [(_one_line(c) if c else "") for c in (row or [])]
                    if not cells:
                        continue
                    purpose = cells[1] if len(cells) > 1 else ""   # ë“±ê¸°ëª©ì 
                    right_holder = cells[2] if len(cells) > 2 else ""  # ê¶Œë¦¬ì/ê¸°íƒ€ì‚¬í•­

                    # ì†Œìœ ê¶Œ ì´ì „ íƒì§€
                    if _OWNER_CHANGE_RX.search(purpose):
                        owner_changes.append(purpose)

                    # ê³µìœ ì ì—¬ë¶€ íƒì§€
                    if _CO_OWNER_RX.search(right_holder):
                        co_owners = True

    return {
        "ownerChangeCount": len(owner_changes),
        "ownerChangeDetails": owner_changes,
        "coOwners": co_owners
    }
# ============ Flask ë¼ìš°íŠ¸ ============
@app.route("/analyze", methods=["POST"])
def analyze():
    # íŒŒì¼ ì²˜ë¦¬
    uploads = []
    if "files" in request.files:
        uploads = request.files.getlist("files")
    elif "file" in request.files:
        uploads = [request.files["file"]]
    if not uploads:
        return jsonify(ok=False, message="PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 400
    if len(uploads) > 1:
        return jsonify(ok=False, message="PDFëŠ” 1ê°œë§Œ ì—…ë¡œë“œí•˜ì„¸ìš”."), 400

    up = uploads[0]
    data = up.read() or b""
    if b"%PDF-" not in data[:1024] and b"%PDF-" not in data:
        return jsonify(ok=False, message="PDFë§Œ í—ˆìš©í•©ë‹ˆë‹¤."), 415

    try:
        doc = fitz.open(stream=data, filetype="pdf")
    except Exception as e:
        return jsonify(ok=False, message=f"PDF ì—´ê¸° ì‹¤íŒ¨: {e}"), 400

    try:
        joint_info = extract_joint_collateral_addresses_follow(data)
        mortgage_info = extract_mortgage_info(data)
        gabu_info = extract_gabu_info(data)
    except Exception as e:
        import traceback;
        traceback.print_exc()
        return jsonify(ok=False, message=f"ë¶„ì„ ì‹¤íŒ¨: {e}"), 500


    return jsonify(
        ok=True,
        pageCount=len(doc),
        jointCollateralPages=joint_info["pages"],
        jointCollateralAddresses=joint_info["addresses"],
        jointCollateralCurrentAddress= joint_info["currentAddress"],
        mortgageInfo = mortgage_info["mortgages"],
        mortgageRiskFlags=mortgage_info["riskFlags"],  # ê°€ì••ë¥˜/ì••ë¥˜/ê°€ì²˜ë¶„
        gabuInfo=gabu_info
    ), 200


# ì¢Œí‘œë¥¼ í†µí•œ ë„¤ì´ë²„ ë¶€ë™ì‚° ì‹¤ì‹œê°„ í¬ë¡¤ë§
# --- Playwright ì„¤ì • ---
USER_DATA_DIR = "./.chrome-profile"  # ì¿ í‚¤/ì§€ë¬¸ ìœ ì§€
HEADLESS = False  # ë¨¼ì € ì°½ ë„ì›Œì„œ í™•ì¸ í›„ Trueë¡œ ì „í™˜ ê°€ëŠ¥
LIST_SEL_CANDS = [
    "div.item_list.item_list--article",
    "div.item_list--article",
    "div.article_list",  # ì˜ˆë¹„
]
API_SOFT_WAIT_MS = 800  # 800~1500 ê¶Œì¥ (ë„ˆë¬´ ëŠë¦¬ë©´ 900ë¡œ)
MAX_CLICK = 20  # í´ë¦­í•  ì¹´ë“œ ìˆ˜ ìƒí•œ


def _pick_scroll_box(page) -> str:
    # scrollHeight > clientHeight ì¸ ì²« ìš”ì†Œ ì„ íƒ
    page.wait_for_selector(", ".join(LIST_SEL_CANDS), timeout=8000)
    for sel in LIST_SEL_CANDS:
        try:
            ok = page.evaluate("""(s) => {
                const el = document.querySelector(s);
                if (!el) return false;
                return el.scrollHeight > el.clientHeight;
            }""", sel)
            if ok:
                return sel
        except:
            pass
    return LIST_SEL_CANDS[0]


def scroll_list_to_bottom(page, pause_ms=700, max_stall=2):
    sel = _pick_scroll_box(page)
    stall = 0
    last_h = -1

    # ì»¨í…Œì´ë„ˆì— í¬ì»¤ìŠ¤(í‚¤ë³´ë“œ ìŠ¤í¬ë¡¤ ë°±ì—…ìš©)
    try:
        page.locator(sel).click(position={"x": 10, "y": 10}, timeout=1000)
    except:
        pass

    for _ in range(60):  # ì¶©ë¶„í•œ íšŸìˆ˜ ì‹œë„
        try:
            h = page.evaluate("""(s) => {
                const el = document.querySelector(s);
                if (!el) return 0;
                el.scrollTop = el.scrollHeight;
                return el.scrollHeight;
            }""", sel)
        except:
            # ë¦¬ë Œë” ëì„ ë•Œ ë‹¤ì‹œ ê³ ë¥´ê¸°
            sel = _pick_scroll_box(page)
            h = page.evaluate("(s) => document.querySelector(s)?.scrollHeight || 0", sel)

        page.wait_for_timeout(pause_ms)

        if h == last_h:
            stall += 1
            if stall >= max_stall:
                break
        else:
            stall = 0
            last_h = h

    # ìŠ¤í¬ë¡¤ í›„ ì¹´ë“œ ìˆ˜ì§‘
    return page.query_selector_all("div.item, div.item_list--article div.item")


def get_env_proxy():
    for k in ("HTTPS_PROXY", "https_proxy", "HTTP_PROXY", "http_proxy"):
        v = os.environ.get(k)
        if v: return {"server": v}
    return None


def _launch_ctx(p):
    proxy_cfg = get_env_proxy()
    ctx = p.chromium.launch_persistent_context(
        user_data_dir=USER_DATA_DIR,
        channel="chrome",
        headless=HEADLESS,
        locale="ko-KR",
        viewport={"width": 1280, "height": 860},
        proxy=proxy_cfg,
        ignore_https_errors=True,
        args=[
            "--disable-blink-features=AutomationControlled",
            "--no-first-run", "--no-default-browser-check",
            "--disable-dev-shm-use", "--no-sandbox",
        ],
    )
    # ì•½ì‹ ìŠ¤í…”ìŠ¤
    ctx.add_init_script("""
      Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
      Object.defineProperty(navigator, 'language', {get: () => 'ko-KR'});
      Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR','ko','en-US','en']});
      window.chrome = window.chrome || { runtime: {} };
    """)
    ctx.set_extra_http_headers({"Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"})
    return ctx


def _goto_offices(page, lat: float, lng: float):
    root = "https://new.land.naver.com"
    path = f"/offices?ms={lat},{lng},19&a=SG&e=RETAIL"
    page.goto(root, wait_until="domcontentloaded", timeout=30000)
    try:
        page.evaluate("href => { location.href = href }", path)
        page.wait_for_url("**/offices**", timeout=20000)
        return
    except PwTimeout:
        pass
    page.goto(root + path, referer=root, wait_until="domcontentloaded", timeout=30000)
    page.wait_for_url("**/offices**", timeout=20000)


DETAIL_PATTERNS = [
    re.compile(r"https://new\.land\.naver\.com/api/.*/articles/\d+"),
    re.compile(r"https://new\.land\.naver\.com/api/.*/article/\d+"),
    re.compile(r"https://new\.land\.naver\.com/api/.*/detail"),
]


def _soft_wait_detail(page: Page, q: deque, prev_len: int, wait_ms: int) -> Optional[Dict[str, Any]]:
    deadline = time.time() + wait_ms / 1000.0
    while time.time() < deadline:
        if len(q) > prev_len:
            return q[-1]  # ê°€ì¥ ìµœê·¼ ë„ì°©ë¶„
        page.wait_for_timeout(40)  # ì§§ê²Œ ì–‘ë³´
    return None


def _is_api(url: str) -> bool:
    return "https://new.land.naver.com/api/" in url


def _is_detail(url: str) -> bool:
    return _is_api(url) and any(p.search(url) for p in DETAIL_PATTERNS)


def crawl_viewport(lat: float, lng: float,
                   radius_m: int = 800,
                   category: str = "offices",
                   filters: Optional[Dict[str, Any]] = None,
                   limit_detail_fetch: Optional[int] = 60) -> Dict[str, Any]:
    with sync_playwright() as p:
        ctx = _launch_ctx(p)
        page = ctx.new_page()

        detail_hits: List[Dict[str, Any]] = []

        # ë””í…Œì¼ ì‘ë‹µ í (on_respê°€ ì—¬ê¸°ë¡œ push)
        api_detail_queue: deque = deque()

        def on_resp(resp):
            url = resp.url
            if _is_detail(url):
                try:
                    data = resp.json()
                except Exception:
                    data = None
                api_detail_queue.append({"url": url, "status": resp.status, "data": data})

        page.on("response", on_resp)

        # 1) /offices ì§„ì… & ëª©ë¡ ë¡œë“œ
        _goto_offices(page, lat, lng)
        page.wait_for_timeout(800)
        scroll_list_to_bottom(page, pause_ms=700, max_stall=2)

        # 2) ì¹´ë“œ ëª©ë¡ í™•ë³´
        page.wait_for_selector(
            "div.item_list--article div.item, div.item_list.item_list--article div.item",
            timeout=6000
        )
        cards = page.query_selector_all(
            "div.item_list--article div.item, div.item_list.item_list--article div.item"
        )

        # 3) í´ë¦­ â†’ (ì§§ê²Œ) API ì†Œí”„íŠ¸ ëŒ€ê¸° â†’ DOM íŒŒì‹±(í•­ìƒ)
        for i, c in enumerate(cards):  # âœ… ëª¨ë“  ì¹´ë“œ ìˆœíšŒ
            if limit_detail_fetch and len(detail_hits) >= int(limit_detail_fetch):
                break

            try:
                before_len = len(api_detail_queue)
                c.scroll_into_view_if_needed()
                c.click()
            except Exception:
                continue

            # APIëŠ” ìµœëŒ€ 1.2ì´ˆë§Œ ì†Œí”„íŠ¸ ëŒ€ê¸° (ì—†ì–´ë„ í†µê³¼)
            hit = _soft_wait_detail(page, api_detail_queue, before_len, API_SOFT_WAIT_MS)

            # ë””í…Œì¼ íŒ¨ë„ DOM íŒŒì‹± (í•­ìƒ ì‹œë„)
            page.wait_for_timeout(10)  # íŒ¨ë„ ë Œë” ì—¬ìœ 
            try:
                dom_data = scrape_detail_panel(page)  # ë˜ëŠ” scrape_detail_panel_raw(page)
            except Exception:
                dom_data = {}

            detail_hits.append({
                "url": (hit["url"] if hit else None),
                "status": (hit["status"] if hit else None),
                "json": (hit["data"] if hit else None),
                "dom": dom_data,
            })


        # ì •ë¦¬
        ctx.close()

        # ëª¨ë“  í•­ëª© ë°˜í™˜ (limit_detail_fetchê°€ Noneì´ë©´ ì „ë¶€)
        limit = int(limit_detail_fetch or len(detail_hits))
        return {
            "ok": True,
            "meta": {
                "lat": lat, "lng": lng,
                "category": category,
                "limit_detail_fetch": limit,
                "count_detail": len(detail_hits),
                "source": "playwright_softwait(dom+api)",
            },
            "items": detail_hits[:limit],  # limit=Noneì´ë©´ ì „ë¶€
        }





def wait_for_element(page: Page, selector: str, timeout: int = 6000) -> Locator:
    loc = page.locator(selector)
    loc.wait_for(state="visible", timeout=timeout)
    return loc


def wait_for_elements(page: Page, selector: str, min_count: int = 1, timeout: int = 6000):
    page.wait_for_selector(selector, state="attached", timeout=timeout)
    loc = page.locator(selector)
    # ê°œìˆ˜ ë§Œì¡±ê¹Œì§€ í´ë§
    with page.expect_event("load", timeout=200) if False else nullcontext():  # no-op
        end = page.context._impl_obj._loop.time() + (timeout / 1000)
        while True:
            if loc.count() >= min_count:
                break
            if page.context._impl_obj._loop.time() > end:
                raise PWTimeout(f"Timeout waiting for {min_count}+ elements: {selector}")
    return [loc.nth(i) for i in range(loc.count())]


def wait_for_child_element(parent: Locator, selector: str, timeout: int = 6000) -> Locator:
    child = parent.locator(selector)
    child.wait_for(state="visible", timeout=timeout)
    return child


def text_or_none(loc: Locator, timeout: int = 3000) -> Optional[str]:
    try:
        return loc.inner_text(timeout=timeout).strip()
    except Exception:
        return None


def get_by_header(page: Page, header_text: str) -> Optional[str]:
    """
    <tr class="info_table_item">
      <th>ê´€ë¦¬ë¹„</th><td>â€¦</td>
    </tr>
    ê°™ì€ êµ¬ì¡°ì—ì„œ í—¤ë” í…ìŠ¤íŠ¸ë¡œ ê°’ì„ ë½‘ì•„ì˜´
    """
    row = page.locator(
        "tr.info_table_item",
        has=page.locator("th", has_text=header_text)
    ).first

    if row.count() == 0:
        return None

    return text_or_none(row.locator("td").first)  # text_or_none: Optional[str] ë°˜í™˜


def parse_korean_price(s: str) -> Optional[int]:
    """í•œêµ­ì‹ ê¸ˆì•¡ ë¬¸ìì—´(ì˜ˆ: 1ì–µ400, 2,000, 3500)ì„ ë§Œì› ë‹¨ìœ„ ì •ìˆ˜ë¡œ ë³€í™˜"""
    if not s:
        return None

    s = s.replace(",", "").strip()
    total = 0

    # ì–µ ë‹¨ìœ„ (1ì–µ = 10000ë§Œì›)
    match = re.search(r"(\d+)ì–µ", s)
    if match:
        total += int(match.group(1)) * 10000
        s = s[match.end():]

    # ë‚¨ì€ ìˆ«ì (ë§Œì› ë‹¨ìœ„)
    digits = re.findall(r"\d+", s)
    if digits:
        total += int("".join(digits))

    return total if total > 0 else None


def parse_deposit_monthly(price_value: Optional[str]) -> Tuple[Optional[int], Optional[int]]:
    """'ë³´ì¦ê¸ˆ/ì›”ì„¸' ë¬¸ìì—´ì„ (ë³´ì¦ê¸ˆ, ì›”ì„¸) íŠœí”Œë¡œ ë³€í™˜ (ë§Œì› ë‹¨ìœ„)"""
    if not price_value:
        return None, None

    if "/" not in price_value:  # ì „ì„¸ or ë§¤ë§¤ (ë‹¨ì¼ ê¸ˆì•¡)
        return parse_korean_price(price_value), None

    a, b = price_value.split("/", 1)
    return parse_korean_price(a), parse_korean_price(b)


def scrape_detail_panel(page: Page) -> dict:
    detail = {}

    # ê°€ê²© ë°•ìŠ¤
    price_box = wait_for_element(page, "div.info_article_price")
    detail["price_type"] = text_or_none(price_box.locator(".type"))  # ì˜ˆ: "ì›”ì„¸"
    detail["price_value"] = text_or_none(price_box.locator(".price"))  # ì˜ˆ: "4,000/230"

    dep, mon = parse_deposit_monthly(detail["price_value"])
    detail["deposit"] = dep
    detail["monthly"] = mon

    # ìƒì„¸ í…Œì´ë¸”(í—¤ë” ê¸°ë°˜ ì¶”ì¶œ)
    # í˜ì´ì§€ë§ˆë‹¤ ë¼ë²¨ì´ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆì–´, í•„ìš”í•œ ê²ƒë§Œ ê³¨ë¼ í˜¸ì¶œí•˜ë©´ ë¨
    candidates = {
        "ê´€ë¦¬ë¹„": ["ê´€ë¦¬ë¹„"],
        "ì „ìš©ë©´ì ": ["ì „ìš©ë©´ì "],
        "ì¸µ": ["ì¸µ", "ì¸µìˆ˜"],
        "ì…ì£¼ê°€ëŠ¥ì¼": ["ì…ì£¼ê°€ëŠ¥ì¼", "ì…ì£¼ì¼"],
        "ì£¼ì°¨": ["ì£¼ì°¨"],
        "ë‚œë°©": ["ë‚œë°©"],
        "ìš©ë„": ["ìš©ë„", "ì£¼ìš©ë„"],
        "ì‚¬ìš©ìŠ¹ì¸ì¼": ["ì‚¬ìš©ìŠ¹ì¸ì¼", "ì¤€ê³µì¼", "ì‚¬ìš©ìŠ¹ì¸"],
        "í™”ì¥ì‹¤ ìˆ˜": ["í™”ì¥ì‹¤ ìˆ˜"],
    }

    for key, labels in candidates.items():
        val = None
        for lbl in labels:
            val = get_by_header(page, lbl)
            if val: break
        detail[key] = val

    # ì¤‘ê°œì—…ì†Œ(ìˆì„ ê²½ìš°)
    broker_box = page.locator("div.broker_info, div.article_broker_info").first
    if broker_box.count() > 0:
        detail["broker_name"] = text_or_none(broker_box.locator(".name, .broker_name"))
        detail["broker_ceo"] = text_or_none(broker_box.locator(".ceo, .broker_ceo"))
        detail["broker_address"] = text_or_none(broker_box.locator(".addr, .broker_address"))
        detail["broker_phone"] = text_or_none(broker_box.locator(".tel, .broker_phone"))

    print(detail)

    return detail


# â”€â”€ ã¡ íŒŒì‹±: ë¬¸ìì—´ì—ì„œ "â€¦ã¡" ê°’ë“¤ë§Œ ë½‘ì•„ 'ê°€ì¥ ì‘ì€ ã¡'ë¥¼ ì „ìš©ë©´ì ìœ¼ë¡œ ê°„ì£¼ â”€â”€
# ì˜ˆ) "50ã¡/38.6ã¡(ì „ìš©ë¥ 80%)" â†’ [50.0, 38.6] â†’ 38.6
def _extract_area_sqm(val: Optional[str]) -> Optional[float]:
    if val is None:
        return None
    s = str(val)
    matches = re.findall(r"([\d]+(?:\.\d+)?)\s*(?:ã¡|mÂ²)", s, flags=re.IGNORECASE)
    if matches:
        nums = [float(x) for x in matches]
        return min(nums)  # ë³´í†µ ì „ìš©(ã¡)ì´ ë” ì‘ìŒ
    # "14í‰" ê°™ì€ ê²½ìš°ë¥¼ ëŒ€ë¹„(ìˆë‹¤ë©´)
    m_py = re.search(r"([\d]+(?:\.\d+)?)\s*í‰", s)
    if m_py:
        return round(float(m_py.group(1)) * 3.305785, 3)
    return None


# â”€â”€ í•µì‹¬: ì›”ì„¸ë§Œ í•„í„° â†’ ë©´ì  ë½‘ê¸° â†’ êµ¬ê°„ë³„ í‰ê·  ì›”ì„¸ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def avg_monthly_by_area(detail_hits: List[Dict[str, Any]]) -> Dict[str, float]:
    per_sqm_vals: List[float] = []  # ë§Œì›/ã¡
    per_pyeong_vals: List[float] = []  # ë§Œì›/í‰

    for it in detail_hits:
        d = it.get("dom") or {}
        if not d:
            continue

        # ì›”ì„¸ë§Œ
        price_type = str(d.get("price_type") or "")
        if "ì›”ì„¸" not in price_type:
            continue

        # ì›”ì„¸ ê¸ˆì•¡
        monthly: Optional[int] = d.get("monthly")

        # ë©´ì : ì „ìš©ë©´ì  ìš°ì„ , ì—†ìœ¼ë©´ ê³µê¸‰ë©´ì  ì‹œë„
        area_str = d.get("ì „ìš©ë©´ì ")
        area = _extract_area_sqm(area_str)

        if monthly is None or area is None:
            continue

        per_sqm = monthly / area  # ë§Œì›/ã¡
        per_pyeong = monthly / (area / 3.305785)  # ë§Œì›/í‰

        per_sqm_vals.append(per_sqm)
        per_pyeong_vals.append(per_pyeong)

        def _avg(vs: List[float]) -> Optional[float]:
            return round(sum(vs) / len(vs), 3) if vs else None

    return {
        "count": len(per_sqm_vals),
        "avg_per_sqm_manwon": _avg(per_sqm_vals),  # ë§Œì›/ã¡
        "avg_per_pyeong_manwon": _avg(per_pyeong_vals),  # ë§Œì›/í‰
    }


# --- Flask ë¼ìš°íŠ¸ ---
@app.route("/crawl", methods=["POST"])
def api_crawl():
    payload = request.get_json(silent=True) or {}
    lat = float(payload.get("lat"))
    lng = float(payload.get("lng"))
    radius_m = payload.get("radius_m") or 800
    category = payload.get("category") or "offices"
    limit_detail_fetch = int(payload.get("limit_detail_fetch") or 60)

    print(f"[CRAWL IN] lat={lat}, lng={lng}, r={radius_m}, cat={category}, limit={limit_detail_fetch}")
    try:
        # â˜… í‚¤ì›Œë“œ ì¸ìë¡œ í˜¸ì¶œ (í¬ì§€ì…”ë„ë¡œ ë„˜ê¸°ë©´ filters ìë¦¬ì— ë°•í˜€ì„œ ì—‰ë§ë¨)
        res = crawl_viewport(
            lat, lng,
            radius_m=radius_m,
            category=category,
            limit_detail_fetch=limit_detail_fetch
        )

        stats = avg_monthly_by_area(res["items"])
        print("[ë©´ì  êµ¬ê°„ë³„ í‰ê·  ì›”ì„¸]", stats)

        print(f"[CRAWL OUT] count={res['meta'].get('count_detail', len(res.get('items', [])))}")
        return jsonify(res), 200
    except Exception as e:
        print(f"[CRAWL ERR] {e}")
        return jsonify(ok=False, error="crawl_failed", message=str(e)), 500






if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True, use_reloader=False, threaded=False)
