from flask import Flask, request, jsonify, Response
import json
import torch
from transformers import BertTokenizer, BertForSequenceClassification
import pickle
import mecab_ko
import pymysql
import os, uuid, tempfile, hashlib
import cv2, numpy as np, pytesseract, re
from PIL import Image
from werkzeug.utils import secure_filename

# ì‚¬ì§„ ì—…ë¡œë“œ ì €ì¥ ë””ë ‰í† ë¦¬ (ìŠ¤í”„ë§ê³¼ ë™ì¼/ê³µìœ  ê²½ë¡œë©´ ë” ì¢‹ìŒ)
UPLOAD_DIR = os.path.join(tempfile.gettempdir(), "registry-uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)

# í—ˆìš© MIME (í•„ìš”ì‹œ application/pdf ì¶”ê°€)
ALLOWED = {"image/jpeg", "image/png", "image/webp", "image/heic"}


# âœ… ì˜ˆì¸¡ í•¨ìˆ˜
def predict_intent(text, threshold=0.1):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=32)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=1)[0]
        confidence, predicted_class_id = torch.max(probs, dim=0)

    confidence = confidence.item()
    class_idx = predicted_class_id.item()

    if confidence < threshold:
        return "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.", confidence

    predicted_label = label_encoder.inverse_transform([class_idx])[0]
    return predicted_label, confidence


def extract_location(text):
    valid_city_map = {
        'ëŒ€ì „': ['ì„œêµ¬', 'ìœ ì„±êµ¬', 'ëŒ€ë•êµ¬', 'ë™êµ¬', 'ì¤‘êµ¬']
    }
    # ì‹œë„ + ì‹œêµ°êµ¬ + ìë©´ë™ê¹Œì§€ ì¶”ì¶œ
    pattern = r"(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)[\s]*(?:íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|ë„)?[\s]*" \
              r"([ê°€-í£]+[ì‹œêµ°êµ¬])?[\s]*([ê°€-í£]+[ë™ë©´ì])?"

    match = re.search(pattern, text)
    if match:
        sido, sigungu, eupmyeondong = match.groups()
        if sigungu and sigungu not in valid_city_map.get(sido, []):
            return "âŒ í–‰ì •êµ¬ì—­ í˜•ì‹ì— ë§ì§€ ì•ŠìŒ"  # âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì¡°í•©
        parts = [sido, sigungu, eupmyeondong]
        return " ".join(p for p in parts if p)

    return None


# âœ… ì˜ë„ + ì§€ì—­ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
def generate_response(user_input):
    intent, confidence = predict_intent(user_input)
    location = extract_location(user_input)

    if intent == 0:
        if location:
            return f"âœ… '{location}'ì˜ ë§¤ì¶œ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
        else:
            return "âš ï¸ ë§¤ì¶œ ì •ë³´ë¥¼ ì¡°íšŒí•˜ë ¤ë©´ ì§€ì—­ëª…ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    elif intent == 1:
        if location:
            return f"ğŸ“Š '{location}'ì˜ ì¸êµ¬ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
        else:
            return "âš ï¸ ì¸êµ¬ ì •ë³´ë¥¼ ì¡°íšŒí•˜ë ¤ë©´ ì§€ì—­ëª…ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    elif intent == 2:
        return "ğŸš¨ íì—… ìœ„í—˜ë„ ë†’ì€ ìƒê¶Œì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."

    else:
        return "â“ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."


# âœ… ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_nouns(text):
    parsed = tagger.parse(text)
    lines = parsed.split('\n')
    nouns = []

    for line in lines:
        if line == 'EOS' or line == '':
            continue
        word, feature = line.split('\t')
        pos = feature.split(',')[0]

        if pos in ['NNG', 'NNP']:  # ì¼ë°˜ëª…ì‚¬ or ê³ ìœ ëª…ì‚¬
            nouns.append(word)

    return nouns


# âœ… ì˜ë¯¸ ë¶„ì„ í•¨ìˆ˜
def analyze_input(user_input, valid_emd_list):
    # ì‹œë„ ëª©ë¡ (ë‹¨ì¼ ë‹¨ì–´ ê¸°ì¤€)
    valid_sido = {'ëŒ€ì „'}

    # ì‹œêµ°êµ¬ ëª©ë¡
    valid_sigungu = {'ì„œêµ¬', 'ìœ ì„±êµ¬', 'ëŒ€ë•êµ¬', 'ë™êµ¬', 'ì¤‘êµ¬'}

    gender_keywords = {
        "ë‚¨ì": "male", "ë‚¨ì„±": "male",
        "ì—¬ì": "female", "ì—¬ì„±": "female"
    }

    age_keywords = {
        "10ëŒ€": "age_10",
        "20ëŒ€": "age_20",
        "30ëŒ€": "age_30",
        "40ëŒ€": "age_40",
        "50ëŒ€": "age_50",
        "60ëŒ€": "age_60"
    }

    nouns = extract_nouns_with_age_merge(user_input)
    print("ì¶”ì¶œëœ ëª…ì‚¬:", nouns)
    gender = None
    age_group = None
    sido = None
    sigungu = None
    emd_nm = None

    for token in nouns:
        # ì‹œë„ ì„¤ì •
        if token in valid_sido:
            sido = token

        # ì‹œêµ°êµ¬ ì„¤ì •
        if token in valid_sigungu:
            sigungu = token

        # ì„±ë³„ ì„¤ì •
        if token in gender_keywords:
            gender = gender_keywords[token]

        # ë‚˜ì´ëŒ€ ì„¤ì •
        if token in age_keywords:
            age_group = age_keywords[token]

        # í–‰ì •ë™ ì„¤ì •
        if token in valid_emd_list:
            emd_nm = token  # âœ… í–‰ì •ë™ ì´ë¦„ ì €ì¥

    return gender, age_group, sido, sigungu, emd_nm


# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ (DBì—ì„œ í–‰ì •ë™ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)
def extract_emd_nm_list():
    conn = pymysql.connect(
        host='localhost',
        user='root',
        password='',
        db='TMRTeamProject',
        charset='utf8'
    )
    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT DISTINCT emd_nm FROM admin_dong")
            result = cursor.fetchall()
            if result:
                return [row[0] for row in result]
            else:
                return []  # âœ… ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë„ ë¦¬í„´
    except Exception as e:
        print("âŒ emd_nm ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨:", e)
        return []  # âœ… ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸
    finally:
        conn.close()


# ìˆ«ì í¬í•¨ ë¶„ì„
def extract_nouns_with_age_merge(text):
    parsed = tagger.parse(text)
    lines = parsed.split('\n')

    result = []
    i = 0
    while i < len(lines) - 1:  # ë§ˆì§€ë§‰ 'EOS' ì œì™¸
        line = lines[i]
        if line == '' or line == 'EOS':
            i += 1
            continue

        word, feature = line.split('\t')
        features = feature.split(',')

        # í˜„ì¬ í’ˆì‚¬
        current_tag = features[0]

        # ìˆ«ì + ëŒ€ ì¡°í•©ì´ë©´ ë³‘í•©
        if current_tag == 'SN' and i + 1 < len(lines):
            next_line = lines[i + 1]
            if '\t' not in next_line:
                i += 1
                continue
            next_word, next_feature = next_line.split('\t')
            next_tag = next_feature.split(',')[0]
            if next_tag == 'NNBC' and next_word == 'ëŒ€':
                result.append(word + next_word)  # ì˜ˆ: "20ëŒ€"
                i += 2
                continue

        # ëª…ì‚¬ë¥˜ë§Œ í•„í„°ë§ (ì§€ëª…/ì¼ë°˜ëª…ì‚¬ ë“±)
        if current_tag in ['NNG', 'NNP']:
            result.append(word)

        i += 1

    return result


# ì„œë²„ ì‹œì‘
tagger = mecab_ko.Tagger()

app = Flask(__name__)

# âœ… intent label ë³µì› (ë¼ë²¨ ì¸ì½”ë”ë¡œ)
with open("label_encoder.pickle", "rb") as f:
    label_encoder = pickle.load(f)
intent_labels = list(label_encoder.classes_)

# âœ… HuggingFace BERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_PATH = "./intent_bert_model"
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
model = BertForSequenceClassification.from_pretrained(MODEL_PATH)
model.eval()

# ì•± ì „ì²´ì—ì„œ ì‚¬ìš©í•  ì „ì—­ ë¦¬ìŠ¤íŠ¸
valid_emd_list = extract_emd_nm_list()


# âœ… API ë¼ìš°íŒ…
@app.route("/predict", methods=["GET"])
def predict():
    question = request.args.get("text", "").strip()

    if not question:
        return jsonify({"error": "text íŒŒë¼ë¯¸í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400

    # ì˜ˆì¸¡
    intent, confidence = predict_intent(question)

    # ê¸°ë³¸ ì‘ë‹µ
    response_data = {
        "intent": str(intent),
        "confidence": float(round(confidence, 4)),
        "message": generate_response(question)
    }

    # intentê°€ 1(ìœ ë™ì¸êµ¬ ì¡°íšŒ)ì¼ ë•Œë§Œ ì˜ë¯¸ ë¶„ì„ ì‹¤í–‰
    if intent == 1:
        gender, age_group, sido, sigungu, emd_nm = analyze_input(question, valid_emd_list)
        response_data.update({
            "sido": str(sido),
            "sigungu": str(sigungu),
            "emd_nm": str(emd_nm),
            "gender": str(gender),
            "age_group": str(age_group)
        })

    return Response(
        json.dumps(response_data, ensure_ascii=False),
        content_type="application/json; charset=utf-8"
    )


# ì‚¬ì§„ ì „ì²˜ë¦¬ ë° ë¶„ì„ ì½”ë“œ
def _sha1(path, limit=1024 * 128):
    """íŒŒì¼ ì•ë¶€ë¶„ë§Œ ì½ì–´ ë¹ ë¥¸ ì²´í¬ì„¬(ì„ íƒ)"""
    h = hashlib.sha1()
    with open(path, "rb") as f:
        chunk = f.read(limit)
        h.update(chunk)
    return h.hexdigest()


def preprocess_for_ocr(img_bgr: np.ndarray) -> np.ndarray:
    """ê¸°ë³¸ ì „ì²˜ë¦¬: ë¦¬ì‚¬ì´ì¦ˆ â†’ ê·¸ë ˆì´ â†’ ì¡ìŒì œê±° â†’ ëŒ€ë¹„ë³´ì • â†’ ì´ì§„í™”"""
    h, w = img_bgr.shape[:2]
    scale = 1400 / max(h, w)
    if scale < 1:
        img_bgr = cv2.resize(img_bgr, (int(w * scale), int(h * scale)))
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    gray = cv2.bilateralFilter(gray, 9, 75, 75)  # ì¡ìŒ ì¤„ì´ë©´ì„œ ì—£ì§€ ì‚´ë¦¬ê¸°
    gray = cv2.equalizeHist(gray)  # ëŒ€ë¹„ í–¥ìƒ
    bw = cv2.adaptiveThreshold(gray, 255,
                               cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                               cv2.THRESH_BINARY, 31, 15)
    return bw


def ocr_image(path: str) -> str:
    """ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ â†’ ì „ì²˜ë¦¬ â†’ Tesseract OCR â†’ í…ìŠ¤íŠ¸"""
    pil = Image.open(path)  # HEICë„ pillow-heif ë“±ë¡í•˜ë©´ ìë™ ì²˜ë¦¬
    pil.load()
    bgr = cv2.cvtColor(np.array(pil.convert("RGB")), cv2.COLOR_RGB2BGR)
    pre = preprocess_for_ocr(bgr)
    pil_pre = Image.fromarray(pre)
    text = pytesseract.image_to_string(pil_pre, config=TESS_CONF)
    # ì•½ê°„ì˜ í´ë¦°ì—…
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text).strip()
    return text


# ì„¹ì…˜ ë‚˜ëˆ„ê¸°(í‘œì œë¶€/ê°‘êµ¬/ì„êµ¬ í‚¤ì›Œë“œ ê¸°ë°˜ì˜ ë§¤ìš° ë‹¨ìˆœí•œ ë¶„ë¦¬)
SEC_RE = {
    "í‘œì œë¶€": re.compile(r"(í‘œì œë¶€.*?)(?=ê°‘êµ¬|ì„êµ¬|$)", re.S),
    "ê°‘êµ¬": re.compile(r"(ê°‘êµ¬.*?)(?=í‘œì œë¶€|ì„êµ¬|$)", re.S),
    "ì„êµ¬": re.compile(r"(ì„êµ¬.*?)(?=í‘œì œë¶€|ê°‘êµ¬|$)", re.S),
}


def split_sections(text: str) -> dict:
    secs = {}
    for k, rx in SEC_RE.items():
        m = rx.search(text)
        secs[k] = m.group(1) if m else ""
    return secs


# ì„êµ¬ì—ì„œ ê¶Œë¦¬/ê¸ˆì•¡/ë‚ ì§œ ì•„ì£¼ ê¸°ì´ˆ ì¶”ì¶œ(ì •ê·œì‹ ê¸°ë°˜ â€“ ì„œì‹ ì°¨ì´ì— ì•½í•¨)
DATE_RE = r"(\d{4}[./-]\d{1,2}[./-]\d{1,2})"
MONEY_RE = r"([0-9,]+)\s*ì›"


def parse_eulgu(eul_text: str) -> list:
    """
    ì˜ˆ: ìˆœìœ„ë²ˆí˜¸ 1 ê·¼ì €ë‹¹ê¶Œ ... ì ‘ìˆ˜ì¼ 2023-01-01 ì±„ê¶Œìµœê³ ì•¡ 100,000,000ì›
    ì„œì‹ì´ ì œê°ê°ì´ë¼ 100% ì •í™•í•˜ì§„ ì•Šì§€ë§Œ, ê¸°ë³¸ì ì¸ íŒ¨í„´ë§Œ ë½‘ìŒ
    """
    entries = []
    pat = re.compile(
        rf"ìˆœìœ„ë²ˆí˜¸\s*(\d+).*?"
        rf"(ê·¼ì €ë‹¹ê¶Œ|ì €ë‹¹ê¶Œ|ì „ì„¸ê¶Œ|ì„ì°¨ê¶Œ|ê°€ë“±ê¸°|ì‹ íƒë“±ê¸°).*?"
        rf"(ì ‘ìˆ˜|ì ‘ìˆ˜ì¼|ì›ì¸ì¼ì).*?{DATE_RE}.*?"
        rf"(ì±„ê¶Œìµœê³ ì•¡|ì „ì„¸ê¸ˆ|ì±„ê¶Œì•¡)?\s*{MONEY_RE}?",
        re.S
    )
    for m in pat.finditer(eul_text):
        entries.append({
            "ìˆœìœ„": int(m.group(1)),
            "ê¶Œë¦¬": m.group(2),
            "ì ‘ìˆ˜ì¼": re.sub(r"[./]", "-", m.group(4)),
            "ê¸ˆì•¡ëª…": m.group(5) or "",
            "ê¸ˆì•¡": int((m.group(6) or "0").replace(",", "")) if m.group(6) else None
        })
    return entries


@app.route("/analyze", methods=["POST"])
def analyze():
    if "files" not in request.files:
        return jsonify(ok=False, message="files í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤."), 400

    files = request.files.getlist("files")
    if not files:
        return jsonify(ok=False, message="ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 400

    results_meta = []
    texts = []

    for f in files:
        if not f or f.filename == "":
            continue

        ctype = (f.mimetype or "").lower()
        if ctype not in ALLOWED:
            return jsonify(ok=False, message=f"í—ˆìš©ë˜ì§€ ì•Šì€ í˜•ì‹: {ctype}"), 415

        # ë©”íƒ€ë°ì´í„°ë§Œ ë³´ì¡´(ì €ì¥ì€ ì•ˆ í•¨)
        base, ext = os.path.splitext(secure_filename(f.filename))
        ext = ext or ".bin"
        fname = f"{uuid.uuid4()}{ext}"

        # ë°”ì´íŠ¸ë¥¼ ë©”ëª¨ë¦¬ë¡œ ì½ìŒ
        data = f.read()
        if not data:
            continue

        try:
            if ctype == "application/pdf":
                txt = ocr_pdf_bytes(data)
            else:
                txt = ocr_image_bytes(data, ctype)
        except Exception as e:
            txt = f"[OCR ì‹¤íŒ¨: {e}]"

        results_meta.append({
            "originalName": f.filename,
            "contentType": ctype,
            "fileName": fname,     # ì‹¤ì œ ì €ì¥ì€ ì•ˆ í•˜ì§€ë§Œ ì¶”ì ìš©ìœ¼ë¡œ ì‘ë‹µ
            "size": len(data),
        })
        texts.append(txt)

    if not results_meta:
        return jsonify(ok=False, message="ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."), 400

    full_text = "\n\n---PAGEBREAK---\n\n".join(texts)
    secs = split_sections(full_text)
    eul_entries = parse_eulgu(secs.get("ì„êµ¬", ""))

    return jsonify(
        ok=True,
        count=len(results_meta),
        files=results_meta,                 # storedPath ì œê±°ë¨ (ë””ìŠ¤í¬ ì €ì¥ ì•ˆ í•¨)
        sections_detected=[k for k, v in secs.items() if v],
        textPreview=full_text[:2000],
        parsed={"ì„êµ¬_entries": eul_entries}
    ), 200


if __name__ == "__main__":
    app.run(port=5000, debug=True)
